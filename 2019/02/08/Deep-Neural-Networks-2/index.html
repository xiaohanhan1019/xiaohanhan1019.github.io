<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8" name="google-site-verification" content="4r262MlSHG2usfX6yLIYST-qBIOPGbPPczLRsjsatqY">
<title>Deep Neural Networks-2 - xiaohanhan&#39;s blog</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



    <meta name="description" content="本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第四周深层神经网络的相关课程及作业。 Building blocks of deep neural networks本节，我们将用网络块来深入理解正向传播和反向传播的过程，由于之前部分已经详细解释了正向传播和反向传播，这里不再详述，本节只是起到补充说明加深理解的作用。">
<meta name="keywords" content="Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Neural Networks-2">
<meta property="og:url" content="https://xiaohanhan1019.github.io/2019/02/08/Deep-Neural-Networks-2/index.html">
<meta property="og:site_name" content="xiaohanhan&#39;s blog">
<meta property="og:description" content="本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第四周深层神经网络的相关课程及作业。 Building blocks of deep neural networks本节，我们将用网络块来深入理解正向传播和反向传播的过程，由于之前部分已经详细解释了正向传播和反向传播，这里不再详述，本节只是起到补充说明加深理解的作用。">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://xiaohanhan1019.github.io/2019/02/08/Deep-Neural-Networks-2/1.png">
<meta property="og:updated_time" content="2019-02-10T08:36:58.920Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Deep Neural Networks-2">
<meta name="twitter:description" content="本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第四周深层神经网络的相关课程及作业。 Building blocks of deep neural networks本节，我们将用网络块来深入理解正向传播和反向传播的过程，由于之前部分已经详细解释了正向传播和反向传播，这里不再详述，本节只是起到补充说明加深理解的作用。">
<meta name="twitter:image" content="https://xiaohanhan1019.github.io/2019/02/08/Deep-Neural-Networks-2/1.png">





<link rel="icon" href="/images/avatar.png">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/xcode.css">


    
    
    
    <style>body>.footer,body>.navbar,body>.section{opacity:0}</style>
    

    
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">
    

    
    

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">


    
    
    
    

<link rel="stylesheet" href="/css/back-to-top.css">


    
    

    
    
    
    

    
    
<link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

    
    
    

    


<link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body class="is-2-column">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                xiaohanhan
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item" href="/">Home</a>
                
                <a class="navbar-item" href="/archives">Archives</a>
                
                <a class="navbar-item" href="/categories">Categories</a>
                
                <a class="navbar-item" href="/tags">Tags</a>
                
                <a class="navbar-item" href="/about">About</a>
                
            </div>
            
            <div class="navbar-end">
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="Search" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-9-tablet is-9-desktop is-9-widescreen has-order-2 column-main"><div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-02-08T09:11:34.000Z">2019-02-08</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/deeplearning-ai/">deeplearning.ai</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/deeplearning-ai/Deep-learning-NN/">Deep learning & NN</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    15 minutes read (About 2186 words)
                </span>
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                Deep Neural Networks-2
            
        </h1>
        <div class="content">
            <p>本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第四周深层神经网络的相关课程及作业。</p>
<h3 id="Building-blocks-of-deep-neural-networks"><a href="#Building-blocks-of-deep-neural-networks" class="headerlink" title="Building blocks of deep neural networks"></a><strong>Building blocks of deep neural networks</strong></h3><p>本节，我们将用网络块来深入理解正向传播和反向传播的过程，由于之前部分已经详细解释了正向传播和反向传播，这里不再详述，本节只是起到补充说明加深理解的作用。</p>
<a id="more"></a>
<p><img src="/2019/02/08/Deep-Neural-Networks-2/1.png" alt=""></p>
<p>上图展示了神经网络正向传播和反向传播中数据的传递的整个过程。顺便提一下，在正向传播中，我们需要缓存 $z^{[l]}$ ，因为在反向传播中会用到。</p>
<h3 id="Forward-and-backward-propagation"><a href="#Forward-and-backward-propagation" class="headerlink" title="Forward and backward propagation"></a><strong>Forward and backward propagation</strong></h3><p>总结一下，如何在深层神经网络中实现前向传播和反向传播</p>
<h5 id="Forward-propagation-for-layer-l"><a href="#Forward-propagation-for-layer-l" class="headerlink" title="Forward propagation for layer $l$"></a>Forward propagation for layer $l$</h5><blockquote>
<p>Input $a^{[l-1]}$</p>
<p>Output $a^{[l]}$, cache $(z^{[l]})$</p>
<p>Step：</p>
<script type="math/tex; mode=display">
z^{[l]} = W^{[l]} \cdot a^{[l-1]}+b^{[l]}</script><script type="math/tex; mode=display">
a^{[l]} = g^{[l]}(z^{[l]})</script><p>After vectorization：</p>
<script type="math/tex; mode=display">
Z^{[l]} = W^{[l]}A^{[l-1]}+b^{[l]}</script><script type="math/tex; mode=display">
A^{[l]} = g^{[l]}(Z^{[l]})</script></blockquote>
<h5 id="Backward-propagation-for-layer-l"><a href="#Backward-propagation-for-layer-l" class="headerlink" title="Backward propagation for layer $l$"></a>Backward propagation for layer $l$</h5><blockquote>
<p>Input $da^{[l]}$</p>
<p>Output $da^{[l-1]}, dW^{[l]}, db^{[l]}$</p>
<p>Step：</p>
<script type="math/tex; mode=display">
dz^{[l]} = da^{[l]} * g^{[l]'}(z^{[l]})</script><script type="math/tex; mode=display">
dW^{[l]} = dz^{[l]} \cdot a^{[l-1]}</script><script type="math/tex; mode=display">
db^{[l]} = dz^{[l]}</script><script type="math/tex; mode=display">
da^{[l-1]} = W^{[l]T} \cdot dz^{[l]}</script><p>After vectorization：</p>
<script type="math/tex; mode=display">
dZ^{[l]} = dA^{[l]} * g^{[l]'}(Z^{[l]})</script><script type="math/tex; mode=display">
dW^{[l]} = \frac{1}{m}dZ^{[l]}A^{[l-1]T}</script><script type="math/tex; mode=display">
db^{[l]} = \frac{1}{m}np.sum(dZ^{[l]},axis=1,keepdims=True)</script><script type="math/tex; mode=display">
dA^{[l-1]} = W^{[l]T} \cdot dZ^{[l]}</script></blockquote>
<h3 id="Parameters-vs-Hyperparameters"><a href="#Parameters-vs-Hyperparameters" class="headerlink" title="Parameters vs. Hyperparameters"></a><strong>Parameters vs. Hyperparameters</strong></h3><p>在神经网络中，参数有：$W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]} …​$ ，超参数有：学习率 (learning rate) $\alpha​$ ，梯度下降迭代次数 (iteration) ，隐藏层 (hidden layer) 个数， 隐藏层神经元个数 $n^{[1]}, n^{[2]} …​$ ，激活函数的选择等等。这些超参数控制了最后参数的变化，因此称为超参数，在接下来的课程会深入探讨。</p>
<p>选择最优的超参数常常是困难的，它需要靠我们的经验，以及一次次的尝试，从而获得更好的参数来训练出更优的模型。</p>
<h3 id="Homework-Building-your-Deep-Neural-Network-step-by-step"><a href="#Homework-Building-your-Deep-Neural-Network-step-by-step" class="headerlink" title="Homework-Building your Deep Neural Network step by step"></a><strong>Homework-Building your Deep Neural Network step by step</strong></h3><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> h5py</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">from</span> testCases_v3 <span class="hljs-keyword">import</span> *</span><br><span class="line"><span class="hljs-keyword">from</span> dnn_utils_v2 <span class="hljs-keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># initialize parameters 初始化一个 2-layer 神经网络</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize_parameters</span><span class="hljs-params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Argument:</span></span><br><span class="line"><span class="hljs-string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="hljs-string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="hljs-string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="hljs-string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="hljs-string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="hljs-string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="hljs-string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="hljs-number">1</span>)</span><br><span class="line"></span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * <span class="hljs-number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="hljs-number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * <span class="hljs-number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="hljs-number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">assert</span> (W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="hljs-keyword">assert</span> (b1.shape == (n_h, <span class="hljs-number">1</span>))</span><br><span class="line">    <span class="hljs-keyword">assert</span> (W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="hljs-keyword">assert</span> (b2.shape == (n_y, <span class="hljs-number">1</span>))</span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="hljs-string">"W1"</span>: W1,</span><br><span class="line">                  <span class="hljs-string">"b1"</span>: b1,</span><br><span class="line">                  <span class="hljs-string">"W2"</span>: W2,</span><br><span class="line">                  <span class="hljs-string">"b2"</span>: b2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># initialize parameters 初始化一个 l-layer 神经网络</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize_parameters_deep</span><span class="hljs-params">(layer_dims)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    layer_dims -- python array (list) containing the dimensions of each layer in our network</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="hljs-string">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="hljs-string">                    bl -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="hljs-number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layer_dims)  <span class="hljs-comment"># number of layers in the network</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, L):</span><br><span class="line">        parameters[<span class="hljs-string">'W'</span> + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - <span class="hljs-number">1</span>]) * <span class="hljs-number">0.01</span></span><br><span class="line">        parameters[<span class="hljs-string">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class="hljs-number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">assert</span> (parameters[<span class="hljs-string">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l - <span class="hljs-number">1</span>]))</span><br><span class="line">        <span class="hljs-keyword">assert</span> (parameters[<span class="hljs-string">'b'</span> + str(l)].shape == (layer_dims[l], <span class="hljs-number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 前向传播，仅仅计算 Z</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">linear_forward</span><span class="hljs-params">(A, W, b)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implement the linear part of a layer's forward propagation.</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    A -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="hljs-string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="hljs-string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    Z -- the input of the activation function, also called pre-activation parameter</span></span><br><span class="line"><span class="hljs-string">    cache -- a python dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    Z = np.dot(W, A) + b</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">assert</span> (Z.shape == (W.shape[<span class="hljs-number">0</span>], A.shape[<span class="hljs-number">1</span>]))</span><br><span class="line">    cache = (A, W, b)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> Z, cache</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 对于某一层前向传播的整个过程</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">linear_activation_forward</span><span class="hljs-params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="hljs-string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="hljs-string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="hljs-string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    A -- the output of the activation function, also called the post-activation value</span></span><br><span class="line"><span class="hljs-string">    cache -- a python dictionary containing "linear_cache" and "activation_cache";</span></span><br><span class="line"><span class="hljs-string">             stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">if</span> activation == <span class="hljs-string">"sigmoid"</span>:</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">elif</span> activation == <span class="hljs-string">"relu"</span>:</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">assert</span> (A.shape == (W.shape[<span class="hljs-number">0</span>], A_prev.shape[<span class="hljs-number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> A, cache</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 整个模型的前向传播整个过程，对于 l-layer 神经网络</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">L_model_forward</span><span class="hljs-params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    X -- data, numpy array of shape (input size, number of examples)</span></span><br><span class="line"><span class="hljs-string">    parameters -- output of initialize_parameters_deep()</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    AL -- last post-activation value</span></span><br><span class="line"><span class="hljs-string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="hljs-string">                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</span></span><br><span class="line"><span class="hljs-string">                the cache of linear_sigmoid_forward() (there is one, indexed L-1)</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    A = X</span><br><span class="line">    L = len(parameters) // <span class="hljs-number">2</span>  <span class="hljs-comment"># number of layers in the neural network 因为同时有w,b两个参数，所以除2</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 循环调用，从第1层到第L-1层</span></span><br><span class="line">    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, L):</span><br><span class="line">        A_prev = A</span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="hljs-string">'W'</span> + str(l)], parameters[<span class="hljs-string">'b'</span> + str(l)],</span><br><span class="line">                                             activation=<span class="hljs-string">"relu"</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 第L层使用sigmoid函数</span></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="hljs-string">'W'</span> + str(L)], parameters[<span class="hljs-string">'b'</span> + str(L)], activation=<span class="hljs-string">"sigmoid"</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">assert</span> (AL.shape == (<span class="hljs-number">1</span>, X.shape[<span class="hljs-number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> AL, caches</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 计算成本函数</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_cost</span><span class="hljs-params">(AL, Y)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implement the cost function defined by equation (7).</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span><br><span class="line"><span class="hljs-string">    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    cost -- cross-entropy cost</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    m = Y.shape[<span class="hljs-number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 注意这里np.log(AL).T需要转置,因为Y和AL都是列向量</span></span><br><span class="line">    cost = (<span class="hljs-number">-1.</span> / m) * (np.dot(Y, np.log(AL).T) - np.dot(<span class="hljs-number">1</span> - Y, np.log(<span class="hljs-number">1</span> - AL).T))</span><br><span class="line"></span><br><span class="line">    cost = np.squeeze(cost)  <span class="hljs-comment"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span></span><br><span class="line">    <span class="hljs-keyword">assert</span> (cost.shape == ())</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 反向传播，仅仅是对于线性函数z = w.T * x + b</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">linear_backward</span><span class="hljs-params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implement the linear portion of backward propagation for a single layer (layer l)</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span><br><span class="line"><span class="hljs-string">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="hljs-string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="hljs-string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="hljs-number">1</span>]</span><br><span class="line"></span><br><span class="line">    dW = <span class="hljs-number">1.</span> / m * np.dot(dZ, A_prev.T)</span><br><span class="line">    db = <span class="hljs-number">1.</span> / m * np.sum(dZ, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-keyword">True</span>)</span><br><span class="line">    dA_prev = np.dot(W.T, dZ)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="hljs-keyword">assert</span> (dW.shape == W.shape)</span><br><span class="line">    <span class="hljs-keyword">assert</span> (db.shape == b.shape)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> dA_prev, dW, db</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 对于每一层整个反向传播过程，其中对于dZ = dA * g'(Z)的计算已经有函数relu_backward,sigmoid_backward帮你实现了</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">linear_activation_backward</span><span class="hljs-params">(dA, cache, activation)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    dA -- post-activation gradient for current layer l</span></span><br><span class="line"><span class="hljs-string">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="hljs-string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="hljs-string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="hljs-string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">if</span> activation == <span class="hljs-string">"relu"</span>:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">elif</span> activation == <span class="hljs-string">"sigmoid"</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> dA_prev, dW, db</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 整个模型的反向传播过程，对于 l-layer 神经网络</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">L_model_backward</span><span class="hljs-params">(AL, Y, caches)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    AL -- probability vector, output of the forward propagation (L_model_forward())</span></span><br><span class="line"><span class="hljs-string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat)</span></span><br><span class="line"><span class="hljs-string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="hljs-string">                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)</span></span><br><span class="line"><span class="hljs-string">                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    grads -- A dictionary with the gradients</span></span><br><span class="line"><span class="hljs-string">             grads["dA" + str(l)] = ...</span></span><br><span class="line"><span class="hljs-string">             grads["dW" + str(l)] = ...</span></span><br><span class="line"><span class="hljs-string">             grads["db" + str(l)] = ...</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = len(caches)  <span class="hljs-comment"># the number of layers</span></span><br><span class="line">    m = AL.shape[<span class="hljs-number">1</span>]</span><br><span class="line">    Y = Y.reshape(AL.shape)  <span class="hljs-comment"># after this line, Y is the same shape as AL</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Initializing the backpropagation 对于dAL的计算公式已经给出了</span></span><br><span class="line">    dAL = - (np.divide(Y, AL) - np.divide(<span class="hljs-number">1</span> - Y, <span class="hljs-number">1</span> - AL))</span><br><span class="line"></span><br><span class="line">    current_cache = caches[L - <span class="hljs-number">1</span>]</span><br><span class="line">    grads[<span class="hljs-string">"dA"</span> + str(L)], grads[<span class="hljs-string">"dW"</span> + str(L)], grads[<span class="hljs-string">"db"</span> + str(L)] = linear_activation_backward(dAL, current_cache,</span><br><span class="line">                                                                                                  activation=<span class="hljs-string">"sigmoid"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> reversed(range(L - <span class="hljs-number">1</span>)):</span><br><span class="line">        current_cache = caches[l]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="hljs-string">"dA"</span> + str(l + <span class="hljs-number">2</span>)], current_cache,</span><br><span class="line">                                                                    activation=<span class="hljs-string">"relu"</span>)</span><br><span class="line">        grads[<span class="hljs-string">"dA"</span> + str(l + <span class="hljs-number">1</span>)] = dA_prev_temp</span><br><span class="line">        grads[<span class="hljs-string">"dW"</span> + str(l + <span class="hljs-number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="hljs-string">"db"</span> + str(l + <span class="hljs-number">1</span>)] = db_temp</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> grads</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 更新参数</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_parameters</span><span class="hljs-params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Update parameters using gradient descent</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    parameters -- python dictionary containing your parameters</span></span><br><span class="line"><span class="hljs-string">    grads -- python dictionary containing your gradients, output of L_model_backward</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    parameters -- python dictionary containing your updated parameters</span></span><br><span class="line"><span class="hljs-string">                  parameters["W" + str(l)] = ...</span></span><br><span class="line"><span class="hljs-string">                  parameters["b" + str(l)] = ...</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="hljs-number">2</span>  <span class="hljs-comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Update rule for each parameter. Use a for loop.</span></span><br><span class="line">    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> range(L):</span><br><span class="line">        parameters[<span class="hljs-string">"W"</span> + str(l + <span class="hljs-number">1</span>)] = parameters[<span class="hljs-string">"W"</span> + str(l + <span class="hljs-number">1</span>)] - learning_rate * grads[<span class="hljs-string">"dW"</span> + str(l + <span class="hljs-number">1</span>)]</span><br><span class="line">        parameters[<span class="hljs-string">"b"</span> + str(l + <span class="hljs-number">1</span>)] = parameters[<span class="hljs-string">"b"</span> + str(l + <span class="hljs-number">1</span>)] - learning_rate * grads[<span class="hljs-string">"db"</span> + str(l + <span class="hljs-number">1</span>)]</span><br><span class="line">    <span class="hljs-keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span></span><br><span class="line">    plt.rcParams[<span class="hljs-string">'figure.figsize'</span>] = (<span class="hljs-number">5.0</span>, <span class="hljs-number">4.0</span>) <span class="hljs-comment"># set default size of plots</span></span><br><span class="line">    plt.rcParams[<span class="hljs-string">'image.interpolation'</span>] = <span class="hljs-string">'nearest'</span></span><br><span class="line">    plt.rcParams[<span class="hljs-string">'image.cmap'</span>] = <span class="hljs-string">'gray'</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="hljs-number">1</span>)</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

        </div>
        
        <div class="level is-size-7 is-uppercase">
            <div class="level-start">
                <div class="level-item">
                    <span class="is-size-6 has-text-grey has-mr-7">#</span>
                    <a class="has-link-grey -link" href="/tags/Deep-Learning/">Deep Learning</a>
                </div>
            </div>
        </div>
        
        
        
    </div>
</div>





<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2019/02/08/Deep-Neural-Networks-1/">
                <span class="level-item">Deep Neural Networks-1</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>


</div>
                




<div class="column is-3-tablet is-3-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered">
                <div>
                    <img class="image is-128x128 has-mb-6" src="/images/avatar.png" alt="Zihan Song">
                    
                    <p class="is-size-4 is-block">
                        Zihan Song
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        Developer
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>Shanghai, China</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level is-mobile">
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Posts
                    </p>
                    <p class="title has-text-weight-normal">
                        8
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Categories
                    </p>
                    <p class="title has-text-weight-normal">
                        4
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Tags
                    </p>
                    <p class="title has-text-weight-normal">
                        3
                    </p>
                </div>
            </div>
        </nav>
        <div class="level">
            <a class="level-item button is-link is-rounded" href="http://github.com/xiaohanhan1019">
                Follow</a>
        </div>
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank" title="Github" href="http://github.com/xiaohanhan1019">
                
                <i class="fab fa-github"></i>
                
            </a>
            
        </div>
        
    </div>
</div>
    
        
<div class="card widget" id="toc">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Catalogue
            </h3>
            <ul class="menu-list"><li>
        <a class="is-flex" href="#Building-blocks-of-deep-neural-networks">
        <span class="has-mr-6">1</span>
        <span>Building blocks of deep neural networks</span>
        </a></li><li>
        <a class="is-flex" href="#Forward-and-backward-propagation">
        <span class="has-mr-6">2</span>
        <span>Forward and backward propagation</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Forward-propagation-for-layer-l">
        <span class="has-mr-6">2.1</span>
        <span>Forward propagation for layer $l$</span>
        </a></li><li>
        <a class="is-flex" href="#Backward-propagation-for-layer-l">
        <span class="has-mr-6">2.2</span>
        <span>Backward propagation for layer $l$</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Parameters-vs-Hyperparameters">
        <span class="has-mr-6">3</span>
        <span>Parameters vs. Hyperparameters</span>
        </a></li><li>
        <a class="is-flex" href="#Homework-Building-your-Deep-Neural-Network-step-by-step">
        <span class="has-mr-6">4</span>
        <span>Homework-Building your Deep Neural Network step by step</span>
        </a></li></ul>
        </div>
    </div>
</div>

    
        


    
        
<div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Categories
            </h3>
            <ul class="menu-list">
            <li>
        <a class="level is-marginless" href="/categories/Java/">
            <span class="level-start">
                <span class="level-item">Java</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/deeplearning-ai/">
            <span class="level-start">
                <span class="level-item">deeplearning.ai</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">6</span>
            </span>
        </a><ul><li>
        <a class="level is-marginless" href="/categories/deeplearning-ai/Deep-learning-NN/">
            <span class="level-start">
                <span class="level-item">Deep learning & NN</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">6</span>
            </span>
        </a></li></ul></li><li>
        <a class="level is-marginless" href="/categories/vim/">
            <span class="level-start">
                <span class="level-item">vim</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li>
            </ul>
        </div>
    </div>
</div>
    
        
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            Tag Cloud
        </h3>
        <a href="/tags/Deep-Learning/" style="font-size: 20px;">Deep Learning</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/vim/" style="font-size: 10px;">vim</a>
    </div>
</div>

    
    
        <!-- 主页三栏，Post页两栏-->
        <!-- <div class="column-right-shadow is-hidden-widescreen "> -->
        <div class="column-right-shadow  ">
        
            
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            Recent
        </h3>
        
        <article class="media">
            
            <a href="/2019/02/08/Deep-Neural-Networks-2/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="Deep Neural Networks-2">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-02-08T09:11:34.000Z">2019-02-08</time></div>
                    <a href="/2019/02/08/Deep-Neural-Networks-2/" class="has-link-black-ter is-size-6">Deep Neural Networks-2</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/deeplearning-ai/">deeplearning.ai</a> / <a class="has-link-grey -link" href="/categories/deeplearning-ai/Deep-learning-NN/">Deep learning & NN</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/02/08/Deep-Neural-Networks-1/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="Deep Neural Networks-1">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-02-08T05:51:44.000Z">2019-02-08</time></div>
                    <a href="/2019/02/08/Deep-Neural-Networks-1/" class="has-link-black-ter is-size-6">Deep Neural Networks-1</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/deeplearning-ai/">deeplearning.ai</a> / <a class="has-link-grey -link" href="/categories/deeplearning-ai/Deep-learning-NN/">Deep learning & NN</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/02/01/Shallow-Neural-Networks-2/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="Shallow Neural Networks-2">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-02-01T10:42:57.000Z">2019-02-01</time></div>
                    <a href="/2019/02/01/Shallow-Neural-Networks-2/" class="has-link-black-ter is-size-6">Shallow Neural Networks-2</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/deeplearning-ai/">deeplearning.ai</a> / <a class="has-link-grey -link" href="/categories/deeplearning-ai/Deep-learning-NN/">Deep learning & NN</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/02/01/Shallow-Neural-Networks-1/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="Shallow Neural Networks-1">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-02-01T04:11:24.000Z">2019-02-01</time></div>
                    <a href="/2019/02/01/Shallow-Neural-Networks-1/" class="has-link-black-ter is-size-6">Shallow Neural Networks-1</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/deeplearning-ai/">deeplearning.ai</a> / <a class="has-link-grey -link" href="/categories/deeplearning-ai/Deep-learning-NN/">Deep learning & NN</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/01/26/Basic-of Neural-Networks-2/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="Basic of Neural Networks-2">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-01-26T04:42:51.000Z">2019-01-26</time></div>
                    <a href="/2019/01/26/Basic-of Neural-Networks-2/" class="has-link-black-ter is-size-6">Basic of Neural Networks-2</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/deeplearning-ai/">deeplearning.ai</a> / <a class="has-link-grey -link" href="/categories/deeplearning-ai/Deep-learning-NN/">Deep learning & NN</a>
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>

        
            <div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            Archives
        </h3>
        <ul class="menu-list">
        
        <li>
            <a class="level is-marginless" href="/archives/2019/02/">
                <span class="level-start">
                    <span class="level-item">February 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">4</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2019/01/">
                <span class="level-start">
                    <span class="level-item">January 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">4</span>
                </span>
            </a>
        </li>
        
        </ul>
        </div>
    </div>
</div>
        
            <div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Tags
            </h3>
            <ul class="menu-list">
                
                <li>
                    <a class="level is-marginless" href="/tags/Deep-Learning/">
                        <span class="level-start">
                            <span class="level-item">Deep Learning</span>
                        </span>
                        <span class="level-end">
                            <span class="level-item tag">6</span>
                        </span>
                    </a>
                </li>
                
                <li>
                    <a class="level is-marginless" href="/tags/Java/">
                        <span class="level-start">
                            <span class="level-item">Java</span>
                        </span>
                        <span class="level-end">
                            <span class="level-item tag">1</span>
                        </span>
                    </a>
                </li>
                
                <li>
                    <a class="level is-marginless" href="/tags/vim/">
                        <span class="level-start">
                            <span class="level-item">vim</span>
                        </span>
                        <span class="level-end">
                            <span class="level-item tag">1</span>
                        </span>
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>
        
        </div>
    
</div>

                <!-- 




<div class="column is-3-tablet is-3-desktop is-3-widescreen  has-order-3 column-right ">
    
        
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            Recent
        </h3>
        
        <article class="media">
            
            <a href="/2019/02/08/Deep-Neural-Networks-2/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="Deep Neural Networks-2">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-02-08T09:11:34.000Z">2019-02-08</time></div>
                    <a href="/2019/02/08/Deep-Neural-Networks-2/" class="has-link-black-ter is-size-6">Deep Neural Networks-2</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/deeplearning-ai/">deeplearning.ai</a> / <a class="has-link-grey -link" href="/categories/deeplearning-ai/Deep-learning-NN/">Deep learning & NN</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/02/08/Deep-Neural-Networks-1/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="Deep Neural Networks-1">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-02-08T05:51:44.000Z">2019-02-08</time></div>
                    <a href="/2019/02/08/Deep-Neural-Networks-1/" class="has-link-black-ter is-size-6">Deep Neural Networks-1</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/deeplearning-ai/">deeplearning.ai</a> / <a class="has-link-grey -link" href="/categories/deeplearning-ai/Deep-learning-NN/">Deep learning & NN</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/02/01/Shallow-Neural-Networks-2/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="Shallow Neural Networks-2">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-02-01T10:42:57.000Z">2019-02-01</time></div>
                    <a href="/2019/02/01/Shallow-Neural-Networks-2/" class="has-link-black-ter is-size-6">Shallow Neural Networks-2</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/deeplearning-ai/">deeplearning.ai</a> / <a class="has-link-grey -link" href="/categories/deeplearning-ai/Deep-learning-NN/">Deep learning & NN</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/02/01/Shallow-Neural-Networks-1/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="Shallow Neural Networks-1">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-02-01T04:11:24.000Z">2019-02-01</time></div>
                    <a href="/2019/02/01/Shallow-Neural-Networks-1/" class="has-link-black-ter is-size-6">Shallow Neural Networks-1</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/deeplearning-ai/">deeplearning.ai</a> / <a class="has-link-grey -link" href="/categories/deeplearning-ai/Deep-learning-NN/">Deep learning & NN</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/01/26/Basic-of Neural-Networks-2/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="Basic of Neural Networks-2">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-01-26T04:42:51.000Z">2019-01-26</time></div>
                    <a href="/2019/01/26/Basic-of Neural-Networks-2/" class="has-link-black-ter is-size-6">Basic of Neural Networks-2</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/deeplearning-ai/">deeplearning.ai</a> / <a class="has-link-grey -link" href="/categories/deeplearning-ai/Deep-learning-NN/">Deep learning & NN</a>
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>

    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            Archives
        </h3>
        <ul class="menu-list">
        
        <li>
            <a class="level is-marginless" href="/archives/2019/02/">
                <span class="level-start">
                    <span class="level-item">February 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">4</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2019/01/">
                <span class="level-start">
                    <span class="level-item">January 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">4</span>
                </span>
            </a>
        </li>
        
        </ul>
        </div>
    </div>
</div>
    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Tags
            </h3>
            <ul class="menu-list">
                
                <li>
                    <a class="level is-marginless" href="/tags/Deep-Learning/">
                        <span class="level-start">
                            <span class="level-item">Deep Learning</span>
                        </span>
                        <span class="level-end">
                            <span class="level-item tag">6</span>
                        </span>
                    </a>
                </li>
                
                <li>
                    <a class="level is-marginless" href="/tags/Java/">
                        <span class="level-start">
                            <span class="level-item">Java</span>
                        </span>
                        <span class="level-end">
                            <span class="level-item tag">1</span>
                        </span>
                    </a>
                </li>
                
                <li>
                    <a class="level is-marginless" href="/tags/vim/">
                        <span class="level-start">
                            <span class="level-item">vim</span>
                        </span>
                        <span class="level-end">
                            <span class="level-item tag">1</span>
                        </span>
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>
    
    
</div>
 -->
                <!-- 主页三栏，post页两栏 -->
                
            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    xiaohanhan
                
                </a>
                <p class="is-size-7">
                &copy; 2019 xiaohanhan&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a href="http://github.com/ppoffice/hexo-theme-icarus">Icarus</a>
                </p>
            </div>
            <div class="level-end">
            
            </div>
        </div>
    </div>
</footer>


  <script src="https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js"></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({theme: 'default'});
    }
  </script>


    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("en");</script>


    
    
    
    <script src="/js/animation.js"></script>
    

    
    
    
    <script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
    <script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
    <script src="/js/gallery.js" defer></script>
    

    
    

<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


    
    
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {matchFontHeight: false},
        SVG: {matchFontHeight: false},
        CommonHTML: {matchFontHeight: false}
    });
});
</script>

    
    

<a id="back-to-top" title="Back to Top" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>


    
    

    
    
    
    

    
    
    
    
    
    <script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>
    <script src="/js/clipboard.js" defer></script>
    

    


<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something...">
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>