<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Shallow Neural Networks-2]]></title>
    <url>%2F2019%2F02%2F01%2FShallow-Neural-Networks-2%2F</url>
    <content type="text"><![CDATA[本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第三周浅层次神经网络的相关课程及相关作业。 Why do you need non-linear activation function为什么神经网络需要非线性的激活函数？不能使用线性的激活函数，比如 $g(z) = z$ 吗？ 假设我们使用线性的激活函数 $g(z) = z$ ，那么有： a^{[1]} = z^{[1]} = W^{[1]}x + b^{[1]} a^{[2]} = z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}把 $a^{[1]}$ 带入，则有 a^{[2]} = W^{[2]}(W^{[1]}x + b^{[1]}) + b^{[2]} = (W^{[2]}W{[1]})x + (W^{[2]}b^{[1]}+b{[2]}) = W'x+b'我们可以得到，$a^{[2]}$ 仍是输入 $x$ 的线性组合。也就是说，使用线性函数的神经网络仅仅只是把输入 $x$ 线性组合再输出。即便是包含许多隐藏层的神经网络，如果使用的是线性的激活函数，不管多少层，得到的输出依然是 $x$ 的线性组合，也就意味着隐藏层根本没有什么作用。所以，隐藏层激活函数必须是非线性的，否则将失去意义。 只有一个地方你可能会使用线性激活函数，在机器学习的回归问题中，$y$ 是一个实数，比如你像预测房地产的价格，那么 $y$ 是一个实数，而不是像二分类问题那样要么 $0$ 要么 $1$ ，这种情况下，在输出层你可能会使用线性激活函数，但隐藏层不会使用线性激活函数。 Derivatives of activation functions在反向传播的过程中，我们需要计算激活函数的导数，那么我们来看一下上述这些激活函数的导数。 Sigmoid 函数 g(z) = \frac{1}{1+e^{-z}} g'(z) = g(z)(1-g(z)) = a(1-a) tanh 函数 g(z) = \frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} g'(z) = 1-(g(z))^{2} = 1-a^2 ReLU 函数 g(z) = max(0,z) g'(z) = \begin{cases} 0,\quad z < 0 \\ 1,\quad z \geq 0 \end{cases} Leaky ReLU 函数 g(z) = max(0.01z,z) g'(z) = \begin{cases} 0.01,\quad z < 0 \\\ 1,\quad \quad z \geq 0 \end{cases}Gradient descent for neural networks好了，有了以上的铺垫，我们终于可以实现在单隐藏层神经网络上的梯度下降算法了。 由于是单隐藏层神经网络，那么我们有参数 $W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}​$ 。我们一般用 $n_x = n^{[0]}​$ 来表示输入层特征的个数，用 $n^{[1]}​$ 表示隐藏层节点个数，用 $n^{[2]}=1​$ 表示输出层节点个数。 其中，$W^{[1]}​$ 的维度为 $(n^{[1]}, n^{[0]})​$ ，$b^{[1]}​$ 的维度为 $(n^{[1]}, 1)​$ ，$W^{[2]}​$ 的维度为 $(n^{[2]}, n^{[1]})​$ ，$b^{[2]}​$ 的维度为 ​$(n^{[2]}, 1)​$ 。 假设我们在做二元分类，那么 Cost function 为： J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \frac{1}{m}\sum_{i=1}^{m}L(\hat{y}, y)整个训练神经网络的过程为： Repeat{ initialize parameters $W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]} $ compute predicts $\hat{y}^{(i)}, i \in [1,m]​$ compute $dW^{[1]}, db^{[1]}, W^{[2]}, b^{[2]}$ update $W^{[1]} = W^{[1]} - \alpha dW^{[1]}, b^{[1]} = b^{[1]} - \alpha db^{[1]} …​$ } 在训练神经网络时，随机初始化参数很重要，并不是单纯的全部初始化为 $0​$ ，我们将在后续详细讨论。 在之前，我们讨论了如果计算 predict (预测值) ，以及如何向量化实现整个过程，所以现在的关键在于，如何计算这些偏导项 $dW^{[1]}, db^{[1]}​$ … 神经网络正向传播的过程为： Z^{[1]} = W^{[1]}X + b^{[1]} A^{[1]} = g(Z^{[1]}) Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} A^{[2]} = g(Z^{[2]}) = \sigma(Z^{[2]})反向传播的过程为： dZ^{[2]} = A^{[2]}-Y dW^{[2]} = \frac{1}{m}dZ^{[2]}A^{[1]T} db^{[2]} = \frac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True) 这个过程其实和之前的 Logistic Regression 很相似。需要注意的是，这里$db^{[2]}$ 的计算直接用了 python 语句，np.sum函数的参数 axis 代表在哪个维度上求和，keepdims为了保持 $db^{[2]}$ 的形状为 $(n^{[2]},1)$ 而不是奇怪的 $(n^{[2]}, )$ dZ^{[2]} = W^{[2]T}dZ^{[2]} * g^{[1]'}(Z^{[1]}) dW^{[1]} = \frac{1}{m}dZ^{[1]}X^{T} db^{[1]}=\frac{1}{m}np.sum(dZ^{[1]},axis = 1,keepdims = True) 需要注意的是 $dZ^{[2]} = W^{[2]T}dZ^{[2]} *g^{[1]’}(Z^{[1]})$ 这里是对应元素相乘，$W^{[2]T}dZ^{[2]}$ 的形状为 $(n^{[1]}, m)$ ，$g’^{[1]}(Z^{[1]})$ 的形状也为 $(n^{[1]}, m)$ 。 关于详细的推导过程，会在接下来的部分详细说明。 Backpropagation intuition先回顾一下之前我们在实现 Logistic Regression 时是如何推导的，可以参考之前的博客 Logistic Regression Gradient Descent. 由于现在多了一层隐藏层，整个反向传播过程会更复杂一点。 首先，先考虑单个样本的情况，先画出计算图，如下图： 根据导数链式法则，可以计算出： dz^{[2]} = \frac{\partial L}{\partial z^{[2]}} = \frac{\partial L}{\partial a^{[2]}} \cdot \frac{\partial a^{[2]}}{\partial z^{[2]}}= a^{[2]}-y dW^{[2]}= \frac{\partial L}{\partial W^{[2]}} = \frac{\partial L}{\partial z^{[2]}} \cdot \frac{\partial z^{[2]}}{\partial W^{[2]}} = dz^{[2]} \cdot \frac{\partial z^{[2]}}{\partial W^{[2]}} = dz^{[2]}a^{[1]T} 注意，这里 $dW^{[2]}$ 的形状为 $(n^{[2]}, n^{[1]})$ ， $n^{[1]}$ 为隐藏层节点个数，而 $a^{[1]}$ 形状为 $(n^{[1]}, n^{[2]})$ ，故这里需要 $a^{[1]}$ 转置，即 $a^{[1]T}$ 。 db^{[2]}= \frac{\partial L}{\partial b^{[2]}} = \frac{\partial L}{\partial z^{[2]}} \cdot \frac{\partial z^{[2]}}{\partial b^{[2]}} = dz^{[2]} \cdot \frac{\partial z^{[2]}}{\partial b^{[2]}} = dz^{[2]} dz^{[1]} = \frac{\partial L}{\partial z^{[1]}} = \frac{\partial L}{\partial a^{[1]}} \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}} = \frac{\partial L}{\partial z^{[2]}} \cdot \frac{\partial z^{[2]}}{\partial a^{[1]}} \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}} = dz^{[2]} \cdot \frac{\partial z^{[2]}}{\partial a^{[1]}} \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}}即， dz^{[1]} = W^{[2]T}dz^{[2]} * g^{[1]'}(z^{[1]}) 同样，这里的 $W^{[2]T}$ 也需要转置，具体为什么，我也不是很明白，但可以从矩阵乘法的规则来判断其形状是否正确。需要注意的是，这里的乘法 $*$ 为对应元素相乘，而不是一般意义上的矩阵乘法。在实现过程中，你必须要确保矩阵的形状相匹配，这里 $W^{[2]T}$ 形状为 $(n^{[1]}, n^{[2]})$ ，$dz^{[2]}$ 的形状为 $(n^{[2]}, 1)$ ，$z^{[1]}$ 的形状为 $(n^{[1]}, 1)$ dW^{[1]} = dz^{[1]} \cdot \frac{\partial z^{[1]}}{\partial W^{[1]}} = dz^{[1]}x^{T} = dz^{[1]}a^{[0]T} db^{[1]} = dz^{[1]} \cdot \frac{\partial z^{[1]}}{\partial b^{[1]}} = dz^{[1]}到这里为止，我们就推导完了反向传播的6个公式。接下来，我们需要将其推广到 $m$ 个训练样本的向量化实现上，得到结果如下，： Random Initialization神经网络中的参数 $W$ 是不能和 Logistic Regression 那样全部初始化为 $0$ 的，我们来分析一下原因。 假设我们有这样一个神经网络，如下图所示： 假设，我们将 $W^{[1]}, W^{[2]}$ 都初始化为零矩阵，那么经过正向传播以后，我们会得到 $a^{[1]}_1 = a^{[1]}_{2}$ ，那么根据对称性，在反向传播后会有 $dz^{[1]}_{1} = dz^{[1]}_{2}$ ， $dW^{[1]}_{1}=dW^{[1]}_{2}$ ，无论你执行多少次梯度下降算法，隐藏层的每个节点都在做相同的操作。这样的话，最后我们获得的 $W^{[1]}, W^{[2]}$ 每行元素都相同，也就是说所有隐藏层中的节点都可以用一个节点来代替，多余的节点没有任何意义，这不是我们想要的。 另外，参数 $b$ 可以全部初始化为 $0$ ，不会发生上面提到的问题。 所以，我们必须随机初始化所有的参数。python 语句如下： 1234W1 = np.random.randn((2,2))*0.01b1 = np.zero((2,1))W1 = np.random.randn((1,2))*0.01b1 = 0 你可能为由疑问，为什么这里要 $*0.01$ ，为什么是 $0.01$ 而不是其他的数字？事实上，我们倾向于把矩阵初始化为非常非常小的随机值。因为如果你用 tanh 函数或者 sigmoid 函数作为激活函数， $W$ 比较小，那正向传播后得到的 $z$ 也会比较小，经过激活函数后所得到的 $a$ 也会接近于 $0$ ，而在 $0$ 附近，激活函数的斜率比较大，能大大地提高梯度下降算法的更新速度，即学习的速度。如果你使用的激活函数为 ReLU 或是 Leaky ReLU 则没有这个问题。 有时候，会有比 $0.01$ 更好用的常数，但如果你只是训练一个单隐层神经网络，或是一个相对较浅的神经网络，没有太多隐藏层，使用 $0.01$ 没有太大问题。但是当你训练一个很深的神经网络时，你可能需要尝试一下别的常数，关于常数的详细内容会在后续部分提到。 Homework123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309# Package importsimport numpy as npimport matplotlib.pyplot as pltfrom testCases_v2 import *import sklearnimport sklearn.datasetsimport sklearn.linear_modelfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasetsdef layer_sizes(X, Y): """ Arguments: X -- input dataset of shape (input size, number of examples) Y -- labels of shape (output size, number of examples) Returns: n_x -- the size of the input layer n_h -- the size of the hidden layer n_y -- the size of the output layer """ n_x = X.shape[0] # size of input layer n_h = 4 n_y = Y.shape[0] # size of output layer return (n_x, n_h, n_y)def initialize_parameters(n_x, n_h, n_y): """ Argument: n_x -- size of the input layer n_h -- size of the hidden layer n_y -- size of the output layer Returns: params -- python dictionary containing your parameters: W1 -- weight matrix of shape (n_h, n_x) b1 -- bias vector of shape (n_h, 1) W2 -- weight matrix of shape (n_y, n_h) b2 -- bias vector of shape (n_y, 1) """ np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random. W1 = np.random.randn(n_h, n_x) * 0.01 b1 = np.zeros((n_h, 1)) W2 = np.random.randn(n_y, n_h) * 0.01 b2 = np.zeros((n_y, 1)) assert (W1.shape == (n_h, n_x)) assert (b1.shape == (n_h, 1)) assert (W2.shape == (n_y, n_h)) assert (b2.shape == (n_y, 1)) parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125; return parametersdef forward_propagation(X, parameters): """ Argument: X -- input data of size (n_x, m) parameters -- python dictionary containing your parameters (output of initialization function) Returns: A2 -- The sigmoid output of the second activation cache -- a dictionary containing "Z1", "A1", "Z2" and "A2" """ # Retrieve each parameter from the dictionary "parameters" W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] # Implement Forward Propagation to calculate A2 (probabilities) Z1 = np.dot(W1, X) + b1 A1 = np.tanh(Z1) Z2 = np.dot(W2, A1) + b2 A2 = sigmoid(Z2) assert (A2.shape == (1, X.shape[1])) cache = &#123;"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2&#125; return A2, cachedef compute_cost(A2, Y, parameters): """ Computes the cross-entropy cost given in equation (13) Arguments: A2 -- The sigmoid output of the second activation, of shape (1, number of examples) Y -- "true" labels vector of shape (1, number of examples) parameters -- python dictionary containing your parameters W1, b1, W2 and b2 Returns: cost -- cross-entropy cost given equation (13) """ m = Y.shape[1] # number of example # Compute the cross-entropy cost logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y) cost = -np.sum(logprobs) / m cost = np.squeeze(cost) # makes sure cost is the dimension we expect. # E.g., turns [[17]] into 17 assert (isinstance(cost, float)) return costdef backward_propagation(parameters, cache, X, Y): """ Implement the backward propagation using the instructions above. Arguments: parameters -- python dictionary containing our parameters cache -- a dictionary containing "Z1", "A1", "Z2" and "A2". X -- input data of shape (2, number of examples) Y -- "true" labels vector of shape (1, number of examples) Returns: grads -- python dictionary containing your gradients with respect to different parameters """ m = X.shape[1] # First, retrieve W1 and W2 from the dictionary "parameters". W1 = parameters["W1"] W2 = parameters["W2"] # Retrieve also A1 and A2 from dictionary "cache". A1 = cache["A1"] A2 = cache["A2"] # Backward propagation: calculate dW1, db1, dW2, db2. dZ2 = A2 - Y dW2 = np.dot(dZ2, A1.T) / m db2 = np.sum(dZ2, axis=1, keepdims=True) / m dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2)) dW1 = np.dot(dZ1, X.T) / m db1 = np.sum(dZ1, axis=1, keepdims=True) / m grads = &#123;"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2&#125; return gradsdef update_parameters(parameters, grads, learning_rate=1.2): """ Updates parameters using the gradient descent update rule given above Arguments: parameters -- python dictionary containing your parameters grads -- python dictionary containing your gradients Returns: parameters -- python dictionary containing your updated parameters """ # Retrieve each parameter from the dictionary "parameters" W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] # Retrieve each gradient from the dictionary "grads" dW1 = grads["dW1"] db1 = grads["db1"] dW2 = grads["dW2"] db2 = grads["db2"] # Update rule for each parameter W1 = W1 - learning_rate * dW1 b1 = b1 - learning_rate * db1 W2 = W2 - learning_rate * dW2 b2 = b2 - learning_rate * db2 parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125; return parametersdef nn_model(X, Y, n_h, num_iterations=10000, print_cost=False): """ Arguments: X -- dataset of shape (2, number of examples) Y -- labels of shape (1, number of examples) n_h -- size of the hidden layer num_iterations -- Number of iterations in gradient descent loop print_cost -- if True, print the cost every 1000 iterations Returns: parameters -- parameters learnt by the model. They can then be used to predict. """ np.random.seed(3) n_x = layer_sizes(X, Y)[0] n_y = layer_sizes(X, Y)[2] # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: "n_x, n_h, n_y". Outputs = "W1, b1, W2, b2, parameters". parameters = initialize_parameters(n_x, n_h, n_y) W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache". A2, cache = forward_propagation(X, parameters) # Cost function. Inputs: "A2, Y, parameters". Outputs: "cost". cost = compute_cost(A2, Y, parameters) # Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads". grads = backward_propagation(parameters, cache, X, Y) # Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters". parameters = update_parameters(parameters, grads) # Print the cost every 1000 iterations if print_cost and i % 1000 == 0: print("Cost after iteration %i: %f" % (i, cost)) return parametersdef predict(parameters, X): """ Using the learned parameters, predicts a class for each example in X Arguments: parameters -- python dictionary containing your parameters X -- input data of size (n_x, m) Returns predictions -- vector of predictions of our model (red: 0 / blue: 1) """ # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold. A2, cache = forward_propagation(X, parameters) predictions = (A2 &gt; 0.5) return predictionsdef main(): # random seed np.random.seed(1) # set a seed so that the results are consistent # Load data X, Y = load_planar_dataset() # Visualize the data # 原来的代码会报错，同样planar_utils.py 21行也需要修改 plt.scatter(X[0, :], X[1, :], c=Y.reshape(X[0, :].shape), cmap=plt.cm.Spectral) # plt.show() # shape of dataset shape_X = X.shape shape_Y = Y.shape m = X.shape[1] # training set size # Build a model with a n_h-dimensional hidden layer parameters = nn_model(X, Y, n_h=4, num_iterations=10000, print_cost=True) # Plot the decision boundary plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y) plt.title("Decision Boundary for hidden layer size " + str(4)) plt.show() # Print accuracy predictions = predict(parameters, X) print('Accuracy: %d' % float( (np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%') # # Train the logistic regression classifier # clf = sklearn.linear_model.LogisticRegressionCV() # clf.fit(X.T, Y.T) # # # Plot the decision boundary for logistic regression # plot_decision_boundary(lambda x: clf.predict(x), X, Y) # plt.title("Logistic Regression") # plt.show() # # # Print accuracy # LR_predictions = clf.predict(X.T) # print('Accuracy of logistic regression: %d ' % float( # (np.dot(Y, LR_predictions) + np.dot(1 - Y, 1 - LR_predictions)) / float(Y.size) * 100) + # '% ' + "(percentage of correctly labelled datapoints)")main()]]></content>
      <categories>
        <category>deeplearning.ai</category>
        <category>Deep learning &amp; NN</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shallow Neural Networks-1]]></title>
    <url>%2F2019%2F02%2F01%2FShallow-Neural-Networks-1%2F</url>
    <content type="text"><![CDATA[本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第三周浅层次神经网络的相关课程。 Neural Network Overview本周，你将学会如何实现神经网络。上周，我们讨论了对数几率回归 (logistic regression) ，并且使用计算图 (computation graph) 的方式了解了梯度下降算法的正向传播和反向传播的两个过程，如下图所示 : 而神经网络 (Neural Network) 是这个样子，如下图 : 我们可以把很多 sigmoid 单元堆叠起来来构成一个神经网络。在之前所学的对数几率回归中，每一个节点对应着两个计算步骤：首先计算 $z=w^{T}x + b​$ ，然后计算 $a=\sigma(z)​$ 。在这个神经网络中，三个竖排堆叠的节点就对应着这两部分的计算，那个单独的节点也对应着另一个类似的 $z, a​$ 的计算。 在神经网络中，所用的符号也会有些不一样。我们还是用 $x$ 来表示输入特征，用 $W^{[1]}, b^{[1]}$ 来表示参数，这样你就可以计算出 $z^{[1]} = W^{[1]}x + b^{[1]}$ 。这里右上角的 $[1]$ 代表着节点所属的层，你可以认为层数从 $0$ 开始算起，如上图中的 $x_1, x_2, x_3$ 就代表着第 $0$ 层（也称为输入层），三个竖排的节点就属于第 $1$ 层（也称为隐藏层），单独的那个节点属于第 $2$ 层（也称为输出层）。需要注意的是，这与之前用来标注第 $i$ 个训练样本 $(x^{(i)}, y^{(i)})$ 不同，这里用的是方括号。 那么，在这个神经网络模型中，正向传播就分为两层。 从输入层到隐藏层：在使用类似对数几率回归的方法计算了 $z^{[1]}$ 之后，再计算 $a^{[1]}=\sigma(z^{[1]})​$ 。 从隐藏层到输出层：使用相同的方法计算 $z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$ 。当然，这里的参数 $W^{[2]}, b^{[2]}$ 与 $W^{[1]}, b^{[1]}$ 不同，且第 $1$ 层的输出 $a^{[1]}$ 作为第 $2$ 层的输入 $x$ ，接着同样计算 $a^{[2]}=\sigma(z^{[2]})$ ，得到的 $a^{[2]}​$ 就是整个神经网络的输出。 同样，还需要通过反向传播计算 $da^{[2]}, dz^{[2]}$ 等等，这些将会在后面详细讨论。 One hidden layer Neural Network下图是一张单隐藏层的神经网络，也称为双层神经网络 (2 layer NN) 。我们把最左边的 $x1, x2, x3$ 称为输入层 （Input Layer) ，中间称为隐藏层 (Hidden Layer) ，最右边只有一个节点的称为输出层 (Output Layer) ，负责输出预测值 $\hat{y}$ 。在计算神经网络的层数时，不算入输入层。 由于在训练过程中，我们看不到这些中间节点的真正数值，不像输入，输出层那样，所以称为隐藏层。 之前，我们用 $x$ 来表示输入，其实它还有一种表示方式 $a^{[0]}$ ，这个 $a$ 有 activation (激活) 的意思，意味这它把不同层的值传递给下一层，起到了激活的作用。用上标 $[i]$ 表示在第 $i$ 层，用下标 $j$ 表示这层中第 $j$ 个节点，如 $a^{[1]}_{2}$ 即表示第 $1$ 层的第 $2$ 个节点。那么上图中隐藏层的4个节点可以写成矩阵的形式： a^{[1]} = \left( \begin{array}{c} a^{[1]}_{1} \\\ a^{[1]}_{2} \\\ a^{[1]}_{3} \\\ a^{[1]}_{4} \end{array} \right)Computing a Neural Network’s Output接下来，我们来看神经网络的输出是如何计算出来的。我们可以把神经网络的计算看作对数几率回归的多次重复计算。 我们先来回顾一下对数几率回归的计算过程，如下图： 这里的圆圈代表了，对数几率回归的两个步骤。我们先隐去其他节点，如右图，那么它就和对数几率回归非常相似，我们可以计算出 $z^{[1]}_{1} = w^{[1]T}_{1}x+b^{[1]}_{1}$ ， $a^{[1]}_{1} = \sigma(z^{[1]}_{1})$ ，上标代表层数，下标表示这层上的第几个节点。 以此类推，我们可以写出： 回想起之前所讲的向量化，如果我们想让程序高效的运行，就必须将其向量化。 我们首先先将 $w$ 向量化，由于有4个对数几率回归单元，而每一个回归单元都有其对应的参数向量 $w$ ，且每一个回归单元都有输入 $x_1, x_2, x_3​$ ，所以我们可以得到： W^{[1]} = \left( \begin{array}{c} w^{[1]T}_{1} \\\ w^{[1]T}_{2} \\\ w^{[1]T}_{3} \\\ w^{[1]T}_{4} \end{array} \right)_{(4 \times 3)}那么，我们可以得到如下式子： Z^{[1]} = \left( \begin{array}{c} w^{[1]T}_{1} \\\ w^{[1]T}_{2} \\\ w^{[1]T}_{3} \\\ w^{[1]T}_{4} \end{array} \right) \cdot \left( \begin{array}{c} x_1 \\\ x_2 \\\ x_3 \end{array} \right) + \left( \begin{array}{c} b^{[1]}_{1} \\\ b^{[1]}_{2} \\\ b^{[1]}_{3} \\\ b^{[1]}_{4} \end{array} \right) = \left( \begin{array}{c} w^{[1]T}_{1}x+b^{[1]}_{1} \\\ w^{[1]T}_{2}x+b^{[1]}_{2} \\\ w^{[1]T}_{3}x+b^{[1]}_{3} \\\ w^{[1]T}_{4}x+b^{[1]}_{4} \end{array} \right) = \left( \begin{array}{c} z^{[1]}_{1} \\\ z^{[1]}_{2} \\\ z^{[1]}_{3} \\\ z^{[1]}_{4} \end{array} \right) a^{[1]} = \left( \begin{array}{c} a^{[1]}_{1} \\\ a^{[1]}_{2} \\\ a^{[1]}_{3} \\\ a^{[1]}_{4} \end{array} \right) = \sigma(Z^{[1]}) =\sigma(W^{[1]}x + b^{[1]})在本神经网络中，你就应该计算 (为了更好理解，右下角标注了矩阵的形状) ： z^{[1]}_{(4 \times 1)} = W^{[1]}_{(4 \times 3)}x_{(3 \times 1)}+ b^{[1]}_{(4 \times 1)} a^{[1]}_{4 \times 1} = \sigma(z^{[1]}_{(4 \times 1)})记得我们之前说过，可以用 $a^{[0]}​$表示 $x​$ ，所以 $z^{[1]}​$ 有可以写成 $z^{[1]} = W^{[1]}a^{[0]}+ b^{[1]}​$ ，那么可以用同样的方法推导出： z^{[2]}_{(1 \times 1)} = W^{[2]}_{(1 \times 4)}a^{[1]}_{(4 \times 1)}+ b^{[2]}_{(1 \times 1)} a^{[2]}_{1 \times 1} = \sigma(z^{[2]}_{(1 \times 1)})所以在代码中，我们只需实现上述的4行代码。然而这是对单个样本的，即对于输入的特征向量 $x$ ，可以计算出 $\hat{y} = a^{[2]}​$ ，对于整个训练集的向量化将在接下来的部分介绍。 Vectorizing across multiple examples接下来，我们实现对整个训练集的向量化。假设你有 $m​$ 个训练样本，理解了上述理论，那么我们可以通过输入 $x^{(i)}​$ 计算得出 $ \hat{y}^{(i)} = a^{[2](i)}​$ 其中 $i \in [1,m]​$ 。 还记得我们之前把输入的特征向量 $x^{(1)}, x^{(2)}, …, x^{(m)}​$ 横向堆叠起来，得到了一个 $(n_x \times m)​$ 的矩阵 $X​$ ，即 \mathbf{X} =\left( \begin{array}{c}\vdots & \vdots & \ldots & \vdots \\\\x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\\vdots & \vdots & \ldots & \vdots\end{array} \right)_{(n_x \times m)}同样的，我们把 $x^{(1)}, x^{(2)}, …, x^{(m)}​$ 横向堆叠起来，那么我们需要计算的式子变为： Z^{[1]} = W^{[1]}X + b^{[1]} A^{[1]} = \sigma(Z^{[1]}) Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} A^{[2]} = \sigma(Z^{[2]})其中，$Z^{[1]}​$ 的形状为 $(4 \times m)​$ ，你可以这样理解，每一个样本对应着矩阵的一列，所以有 $m​$ 列，隐藏层中的每一个节点对应着矩阵的一行，所以 $Z^{[1]}​$ 有 $4​$ 行，规律同样适用于 $X, A​$。 Activation functions到目前为止，我们一直选择 sigmoid 函数作为 activation function (激活函数) ，但有时使用其他函数效果要好得多，它们各自有不同的特点，下面我们来介绍几个不同的激活函数 $g(x)$： tanh (双曲正切函数)，实际上是 sigmoid 函数向下平移后再经过拉伸得到的。对于隐藏单元，如果你选择 tanh 作为激活函数，它的表现几乎总是比 sigmoid 函数要好，因为 tanh 函数的输出介于 $(-1,1)$ 之间，激活函数的平均值更接近于 $0$ ，而不是 $0.5$ ，这让下一层的学习更方便一点。所以之后我们几乎不再使用 sigmoid 作为激活函数了，但有一个例外，即选择输出层的激活函数的时候，因为二分类问题的输出为 $\{0,1\}$ ，你更希望 $\hat{y}$ 介于 $0,1$ 之间，所以一般会选择 sigmoid 函数。 所以之前所举的例子中，你可以使用 tanh 函数作为隐藏层的激活函数，而选择 sigmoid 函数作为输出层的激活函数。同样的，可以使用上标来表示每一层的激活函数，如：$g^{[1]}(x) = tanh(z), g^{[2]} = \sigma(z)​$ 然而，不管是 tanh 还是 sigmoid 函数，都有一个缺点，如果 $z​$ 特别大或者特别小，那么在这一点的函数导数会很小，因此会拖慢梯度下降算法。为了弥补这个缺点，就出现了 ReLU (rectified linear unit) 函数，该函数有如下特点：当 $z​$ 为正，导数为 $1​$，当 $z​$ 为负，导数为 $0​$ ，当 $z​$ 为 $0​$ 时，导数不存在，但在实际使用中， $z​$ 几乎不会等于 $0​$ ，当然你可以在程序中直接把在 $z=0​$ 点的导数赋为 $0​$ 或 $1​$ 。 但 ReLU 也有一个缺点，当 $z$ 为负时，导数恒等于 $0$ 。所以就有了 Leaky ReLU 函数，通常表现比 ReLU 函数更好，但实际使用中频率没那么高。 ReLU 和 Leaky ReLU 的优势在于，对于 $z$ 的许多取值，激活函数的导数和 $0$ 差的很远，这也就意味着，在实践中你的神经网络的学习速度会快很多。 总结一下，在我们一般选择 sigmoid 作为输出层的激活函数，而选择 ReLU 作为其他层的激活函数，ReLU 如今被人们广泛使用。]]></content>
      <categories>
        <category>deeplearning.ai</category>
        <category>Deep learning &amp; NN</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Basic of Neural Networks-2]]></title>
    <url>%2F2019%2F01%2F26%2FBasic-of%20Neural-Networks-2%2F</url>
    <content type="text"><![CDATA[本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第二周神经网络基础相关课程及第二周作业。 VectorizationVectorization (向量化) 的意义在于：消除代码中显式的调用for循环。在深度学习领域中，你常常需要训练大数据集，所以程序运行的效率非常重要，否则需要等待很长时间才能得到结果。 在对数几率回归中，你需要计算 $z = w^{T}x + b$ ，其中 $w, x$ 都是 $n_x$ 维向量 如果你是用非向量化的实现，即传统的矩阵相乘算法伪代码如下：$ z = 0 $$ for \quad i \quad in \quad range(n_x) : $$ \quad z+= w[i] * x[i] $$z+=b ​$ 若使用向量化的实现，Python代码如下：1z = np.dot(w,x) + b 即清晰又高效 可以测试一下这两个代码在效率方面的差距，大约差了300倍。可以试想一下，如果你的代码1分钟出结果，和5个小时才出结果，那可差太远了。 为了加快深度学习的运算速度，可以使用GPU (Graphic Processing Unit) 。事实上，GPU 和 CPU 都有并行指令 (Parallelization Instructions) ，同时也叫作 SIMD (Single Instruction Multiple Data)，即单指令流多数据流，是一种采用一个控制器来控制多个处理器，同时对一组数据 (又称“数据向量”) 中的每一个分别执行相同的操作从而实现空间上的并行性的技术。numpy 是 Python 数据分析及科学计算的基础库，它有许多内置 (Built-in) 函数，主要用于数组的计算，充分利用了并行化，使得运算速度大大提高。在深度学习的领域，一般来说，能不用显式的调用for循环就不用。 这样，我们就可以使用 Vectorization 来优化梯度下降算法，先去掉内层对 feature (特征 $w1, w2 …$) 的循环 ： $J=0; db=0; dw = np.zeros(n_x,1)$$for \quad i = 1 \quad to \quad m $$\quad z^{(i)} = w^{T}x^{(i)}+b$$\quad a^{(i)} = \sigma(z^{(i)})$$\quad J += -(y^{(i)} \log a^{(i)} + (1-y^{(i)}) \log(1-a^{(i)}))$$\quad dz^{(i)} = a^{(i)}-y^{(i)}$$\quad dw+=x^{(i)}dz^{(i)} \quad \quad $ //vectorization$\quad db += dz^{(i)}$$J /= m$$dw /= m$$db /= m​$ 然后，我们再去掉对 $m$ 个训练样本的外层循环，分别从正向传播和反向传播两方面来分析： 正向传播回顾一下对数几率回归的正向传播步骤，如果你有 $m$ 个训练样本 那么对第一个样本进行预测，你需要计算$ \quad z^{(1)} = w^{T}x^{(1)} + b$$ \quad a^{(1)} = \sigma(z^{(1)})​$ 然后继续对第二个样本进行预测$ \quad z^{(2)} = w^{T}x^{(2)} + b$$ \quad a^{(2)} = \sigma(z^{(2)})$ 然后继续对第三个，第四个，…，直到第 $m$ 个 回忆一下之前在二分分类部分所讲到的用更紧凑的符号 $X​$ 表示整个训练集，即大小为 $(n_x,m)​$ 的矩阵 : \mathbf{X} = \left( \begin{array}{c} \vdots & \vdots & \ldots & \vdots \\\\ x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\ \vdots & \vdots & \ldots & \vdots \end{array} \right)那么计算 $z^{(1)}, z^{(2)}, … , z^{(m)}$ 的步骤如下 :首先先构造一个 $(1,m)$ 的矩阵 $[z^{(1)}, z^{(2)}, … , z^{(m)}]$ ，则 Z = [z^{(1)}, z^{(2)}, ... , z^{(m)}] = w^{T}X + [b, b , ... , b] = [w^{T}x^{(1)} + b, w^{T}x^{(2)} + b] , ... , w^{T}x^{(m)} + b]在 Python 中一句代码即可完成上述过程1Z = np.dot(w.T,X) + b 你可能会有疑问，明明这里的 $b$ 只是一个实数 (或者说是一个 $b_{(1,1)}$ 的矩阵) ，为什么可以和矩阵 $Z_{(1,m)}$ 相加？事实上，当做 $Z+b$ 这个操作时，Python 会自动把矩阵 $b_{(1,1)}$ 自动扩展为 $b_{(1,m)}$ 这样的一个行向量，在 Python 中这称为 Broadcasting (广播) ，现在你只要看得懂就好，接下来会更详细地说明它。 同理我们可以得到 A = [a^{(1)}, a^{(2)}, ... , a^{(m)}] = \sigma (Z)同样也只需一句 Python 代码 1A = sigmoid(Z) 反向传播接着，我们来看如何用向量化优化反向传播，计算梯度 同样，你需要计算$dz^{(1)} = a^{(1)} - y^{(1)}​$$dz^{(2)} = a^{(2)} - y^{(2)}​$$…​$一直到第 $m​$ 个 即计算 $dZ = [dz^{(1)} , dz^{(2)} , … , dz^{(m)}]$ 之前我们已经得到 $A = [a^{(1)}, a^{(2)}, … , a^{(m)}] = \sigma (Z)​$我们再定义输出标签 $Y = [y^{(1)}, y^{(2)}, … , y^{(m)}]​$ 那么， dZ = A-Y = [a^{(1)}-y^{(1)} , a^{(2)}-y^{(2)} , ... , a^{(m)}-y^{(m)}]有了 $dZ$ 我们就可以计算 $dw, db​$根据之前的公式，有 db = \frac{1}{m} \sum_{i=1}^{m}dz^{(i)} dw = \frac{1}{m}X \cdot dZ^{T} = \frac{1}{m} \left( \begin{array}{c} \vdots & \vdots & \ldots & \vdots \\\\ x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\ \vdots & \vdots & \ldots & \vdots \end{array} \right) \left( \begin{array}{c} dz^{(1)} \\\\ \vdots \\\\ dz^{(m)} \end{array} \right) = \frac{1}{m}[x^{(1)}dz^{(1)} + ...+ x^{(m)}dz^{(m)}]对应的 Python 代码即为12db = np.sum(dZ)/mdw = np.dot(x,dZ.T)/m 最后，我们可以得到向量化后的梯度下降算法1234567import numpy as npZ = np.dot(w.T,X) + bA = sigmoid(Z)dw = np.dot(x,dZ.T)/mdb = np.sum(dZ)/mw = w - alpha * dwb = b - alpha * db 你可能会有疑问，为什么这里不需要再计算 Cost function (代价函数) $J$ 了，笔者认为 $J$ 只是对数几率回归模型所需要的损失函数，借助它我们才能计算出 $dw, db$ ，从而进行迭代。在后续的作业中，计算 $J​$ 可以帮助我们对模型进一步分析。这样，我们就完成了对数几率回归的梯度下降的一次迭代，但如果你需要多次执行迭代的操作，只能显式的调用for循环。 BroadcastingBroadcasting (广播) 机制主要是为了方便不同 shape 的 array (可以理解为不同形状的矩阵) 进行数学运算 当我们将向量和一个常量做加减乘除操作时，比如对数几率回归中的 $w^{T}x+b$ ，会对向量中的每一格元素都和常量做一次操作，或者你可以理解为把这个数复制 $m \times n$ 次，使其变为一个形状相同的 $(m,n)$ 矩阵，如 : \begin{bmatrix}1 \\ 2 \\ 3 \\ 4\end{bmatrix} + 100 => \begin{bmatrix}1 \\ 2 \\ 3 \\ 4\end{bmatrix} + \begin{bmatrix}100 \\ 100 \\ 100 \\ 100\end{bmatrix} = \begin{bmatrix}101 \\ 102 \\ 103 \\ 104\end{bmatrix} 一个 $(m,n)$ 矩阵和一个 $(1,n)$ 矩阵相加 (减乘除)，会将这个 $(1,n)$ 矩阵复制 $m$ 次，使其变为 $(m,n)$ 矩阵然后相加，如 : \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix} 100 & 200 & 300\end{bmatrix} => \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix} 100 & 200 & 300 \\ 100 & 200 & 300\end{bmatrix} = \begin{bmatrix}101 & 202 & 303 \\ 104 & 205 & 306\end{bmatrix} 同样地，一个 $(m,n)$ 矩阵和一个 $(m,1)$ 矩阵相加 (减乘除)，会将这个 $(m,1)$ 矩阵复制 $n$ 次，使其变为 $(m,n)$ 矩阵然后相加 \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix}100 \\ 200\end{bmatrix} => \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix} 100 & 100 & 100 \\ 200 & 200 & 200\end{bmatrix} = \begin{bmatrix}101 & 102 & 103 \\ 204 & 205 & 206\end{bmatrix}通俗的讲，numpy 会通过复制的方法，使两个不同形状的矩阵变得一致，再执行相关操作。值得一提的是，为了保证运算按照我们的想法进行，使用 reshape() 函数是一个较好的习惯。 Homework附上所有代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276# LogisticRegression.pyimport numpy as npimport matplotlib.pyplot as pltimport h5pyimport scipyfrom PIL import Imagefrom scipy import ndimagefrom lr_utils import load_dataset# Sigmoid 函数def sigmoid(z): """ Compute the sigmoid of z Arguments: z -- A scalar or numpy array of any size. Return: s -- sigmoid(z) """ s = 1 / (1 + np.exp(-z)) return s# 初始化 w,bdef initialize_with_zeros(dim): """ This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0. Argument: dim -- size of the w vector we want (or number of parameters in this case) Returns: w -- initialized vector of shape (dim, 1) b -- initialized scalar (corresponds to the bias) """ w = np.zeros((dim, 1)) # (dim, 1) 是shape参数，代表初始化一个dim*1的矩阵 b = 0 assert (w.shape == (dim, 1)) assert (isinstance(b, float) or isinstance(b, int)) return w, b# propagate 正向与反向传播def propagate(w, b, X, Y): """ Implement the cost function and its gradient for the propagation explained above Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples) Return: cost -- negative log-likelihood cost for logistic regression dw -- gradient of the loss with respect to w, thus same shape as w db -- gradient of the loss with respect to b, thus same shape as b Tips: - Write your code step by step for the propagation. np.log(), np.dot() """ m = X.shape[1] # FORWARD PROPAGATION (FROM X TO COST) A = sigmoid(np.dot(w.T, X) + b) # compute activation cost = -1 / m * (np.dot(Y,np.log(A).T) + np.dot(1 - Y,np.log(1 - A).T)) # compute cost # BACKWARD PROPAGATION (TO FIND GRAD) dw = 1 / m * np.dot(X, (A - Y).T) db = 1 / m * np.sum(A - Y) assert (dw.shape == w.shape) assert (db.dtype == float) cost = np.squeeze(cost) # 删除shape为1的维度，比如cost=[[1]]，则经过np.squeeze处理后cost=[1] assert (cost.shape == ()) grads = &#123;"dw": dw, "db": db&#125; return grads, cost# 梯度下降def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False): """ This function optimizes w and b by running a gradient descent algorithm Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of shape (num_px * num_px * 3, number of examples) Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples) num_iterations -- number of iterations of the optimization loop learning_rate -- learning rate of the gradient descent update rule print_cost -- True to print the loss every 100 steps Returns: params -- dictionary containing the weights w and bias b grads -- dictionary containing the gradients of the weights and bias with respect to the cost function costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve. Tips: You basically need to write down two steps and iterate through them: 1) Calculate the cost and the gradient for the current parameters. Use propagate(). 2) Update the parameters using gradient descent rule for w and b. """ costs = [] for i in range(num_iterations): # Cost and gradient calculation grads, cost = propagate(w, b, X, Y) # Retrieve derivatives from grads dw = grads["dw"] db = grads["db"] # update rule w = w - dw * learning_rate b = b - db * learning_rate # Record the costs if i % 100 == 0: costs.append(cost) # Print the cost every 100 training examples if print_cost and i % 100 == 0: print("Cost after iteration %i: %f" % (i, cost)) params = &#123;"w": w, "b": b&#125; grads = &#123;"dw": dw, "db": db&#125; return params, grads, costs# 利用logistic regression判断Y的标签值def predict(w, b, X): ''' Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b) Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Returns: Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X ''' m = X.shape[1] Y_prediction = np.zeros((1, m)) w = w.reshape(X.shape[0], 1) # Compute vector "A" predicting the probabilities of a cat being present in the picture A = sigmoid(np.dot(w.T, X) + b) # A.shape = (1,m) for i in range(A.shape[1]): # Convert probabilities A[0,i] to actual predictions p[0,i] if A[0, i] &gt; 0.5: Y_prediction[0, i] = 1 else: Y_prediction[0, i] = 0 assert (Y_prediction.shape == (1, m)) return Y_prediction# 构建整个模型def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False): """ Builds the logistic regression model by calling the function you've implemented previously Arguments: X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train) Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train) X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test) Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test) num_iterations -- hyperparameter representing the number of iterations to optimize the parameters learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize() print_cost -- Set to true to print the cost every 100 iterations Returns: d -- dictionary containing information about the model. """ # initialize parameters with zeros w, b = initialize_with_zeros(X_train.shape[0]) # Gradient descent parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Retrieve parameters w and b from dictionary "parameters" w = parameters["w"] b = parameters["b"] # Predict test/train set examples Y_prediction_test = predict(w, b, X_test) Y_prediction_train = predict(w, b, X_train) # Print train/test Errors print("train accuracy: &#123;&#125; %".format( 100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100)) print("test accuracy: &#123;&#125; %".format( 100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100)) d = &#123; "costs": costs, "Y_prediction_test": Y_prediction_test, "Y_prediction_train": Y_prediction_train, "w": w, "b": b, "learning_rate": learning_rate, "num_iterations": num_iterations &#125; return ddef main(): train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset( ) m_train = train_set_x_orig.shape[0] m_test = test_set_x_orig.shape[0] num_px = train_set_x_orig.shape[2] train_set_x_flatten = train_set_x_orig.reshape(m_train, -1).T test_set_x_flatten = test_set_x_orig.reshape(m_test, -1).T train_set_x = train_set_x_flatten / 255. test_set_x = test_set_x_flatten / 255. # train model d = model( train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True) # 判断单张图片是否有猫 my_image = "a.jpg" # change this to the name of your image file # We preprocess the image to fit your algorithm. fname = "images/" + my_image image = np.array(ndimage.imread(fname, flatten=False)) my_image = scipy.misc.imresize(image, size=(num_px, num_px)).reshape((1, num_px * num_px * 3)).T my_predicted_image = predict(d["w"], d["b"], my_image) print("y = " + str(np.squeeze(my_predicted_image)) + ", your algorithm predicts a \"" + classes[ int(np.squeeze(my_predicted_image)),].decode("utf-8") + "\" picture.") plt.imshow(image) plt.show()main() lr_utils.py1234567891011121314151617181920# lr_utils.pyimport numpy as npimport h5py def load_dataset(): train_dataset = h5py.File('datasets/train_catvnoncat.h5', "r") train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # your train set features train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # your train set labels test_dataset = h5py.File('datasets/test_catvnoncat.h5', "r") test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # your test set features test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # your test set labels classes = np.array(test_dataset["list_classes"][:]) # the list of classes train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0])) test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0])) return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes]]></content>
      <categories>
        <category>deeplearning.ai</category>
        <category>Deep learning &amp; NN</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Basic of Neural Networks-1]]></title>
    <url>%2F2019%2F01%2F21%2FBasic-of-Neural-Networks-1%2F</url>
    <content type="text"><![CDATA[本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第二周神经网络基础的相关课程。 Binary Classification在二分分类问题中，目标是训练出一个分类器，以特征向量x (feature vector)为输入，以y (output label)为输出，y一般只有 ${0,1}​$ 两个离散值。以图像识别问题为例，判断图片中是否由猫存在，0代表noncat，1代表cat 通常，我们用 $(x,y)​$ 来表示一个单独的样本，其中x(feature vector)是$n_x​$维的向量 ( $n_x​$ 为样本特征个数，即决定输出的因素) ，y(output label)为输出，取值为 $y\in\{0,1\}​$则m个训练样本 (training example) 可表示为 \{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\}用$ m=m_{train} $表示训练样本的个数 最后，我们可以用更紧凑的符号 $X$ 表示整个训练集，$X$ 由训练集中的 $x^{(1)}$，$x^{(2)}$，…，$x^{(m)}$ 作为列向量组成，$X\in{\Bbb R}^{n_x*m}$，即 X.shape = $(n_x,m)$ \mathbf{X} = \left( \begin{array}{c} \vdots & \vdots & \ldots & \vdots \\\\ x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\ \vdots & \vdots & \ldots & \vdots \end{array} \right)同时，把y也放入列中，用 $Y$ 来表示，$Y\in{\Bbb R}^{1*m}$，即 Y.shape = $(1,m)​$ \mathbf{Y} = \left( \begin{array}{c} y^{(1)} & y^{(2)} & \ldots & y^{(m)} \ \end{array} \right)Logistic Regression参照了周志华的西瓜书，把 Logisitic Regression 翻译为对数纪律回归，简称为对率回归。对数几率回归是一种解决二分分类问题的机器学习方法，用于预测某种实物的可能性。 Given x, you want $\hat{y} = P(y=1 \mid x)​$. In other words, if x is a picture, as talked about above, you want $\hat{y}​$ to tell you the chance that there is a cat in the picture. 根据输入 $x$ 和参数 $w, b$，计算出 $\hat{y}$ ，下面介绍了两种方法 : Parameter : $w\in{\Bbb R}^{n_x}, b\in{\Bbb R}​$Output : $\hat{y}​$ One way : $\hat{y} = w^{T}x + b​$ (Linear regression) Not good for binary classification Because you want $\hat{y}​$ to be the chance that $y​$ equals to one. In this situation $\hat{y}​$ can be much bigger than 1 or negative. The other way : $\hat{y} = \sigma(w^{T}x + b)$ (Logistic Regression) $\sigma(z) = \frac{1}{1+e^{-z}} ​$ 通过$\sigma(z)$函数，可以将输出限定在$[0,1]$之间 Logistic Regression Cost Function给出$\{(x^{(1)},y^{(1)})…,(x^{(m)},y^{(m)})\}​$，希望通过训练集，找到参数 $w, b​$ 使得 $\hat{y}^{(i)} \approx y^{(i)}​$ 。所以，我们需要定义一个loss function，通过这个loss function来衡量你的预测输出值 $\hat{y}​$ 与 $y​$ 的实际值由多接近 对于m个训练样本，我们通常用上标 $(i)​$ 来指明数据与第 $i​$ 个样本有关。 通常，我们这样定义Loss function (损失函数) : L(\hat{y},y) = \frac{1}{2}(\hat{y} - y)^2但在对数几率回归中一般不使用，因为它是non-convex (非凸的) ，将来使用梯度下降算法 (Gradient Descent)时无法找到全局最优值 在对数几率回归中，我们使用的损失函数为 : L(\hat{y},y) = -(y \log\hat{y} + (1-y) \log(1-\hat{y})) If y = 1 : $L(\hat{y},y) = -\log(\hat{y})​$, you want $\hat{y}​$ to be large if y = 0 : $L(\hat{y},y) = -\log(1-\hat{y})​$, you want $\hat{y}​$ to be small 所以，这个损失函数和 $L(\hat{y},y) = \frac{1}{2}(\hat{y} - y)^2$ 类似，都希望 $L$ 越小越好 上述的Loss function衡量了单个训练样本的表现，对于m个样本，我们定义Cost function (代价函数) ，它衡量了全体训练样本的表现 J(w,b) = \frac{1}{m} \sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)}) = -\frac{1}{m} \sum_{i=1}^{m}y^{(i)} \log\hat{y}^{(i)} + (1-y^{(i)}) \log(1-\hat{y}^{(i)})Loss function只适用于单个训练样本，Cost function是基于参数的总代价。所以，在训练对数几率回归模型时，我们要找到合适的参数 $w, b$ 使得Cost function尽可能的小 Gradient Descent我们将使用梯度下降 (Gradient Descent) 算法来找出合适的参数 $w,b$，使得Cost function 即 $J(w,b)$ 最小 最上方的小红点为初始点，对于对数几率回归，一般使用0来初始化，随机初始化也有效，但通常不这么做 梯度下降过程： 从初始点开始，朝最陡的下坡方向走一步 重复上述过程，不断修正 $w, b​$ 使得 $J(w,b)​$ 接近全局最优值 (global opitmal) 代码表述为： Repeat {$w := w - \alpha \frac{\partial J(w,b)}{\partial w}​$ &nbsp; &nbsp;&nbsp;&nbsp;在代码中 $\frac{\partial J(w,b)}{\partial w}​$ 记作”dw”$b := b - \alpha \frac{\partial J(w,b)}{\partial b}​$ &nbsp; &nbsp;&nbsp;&nbsp;在代码中 $\frac{\partial J(w,b)}{\partial b}​$ 记作”db”} Computation Graph神经网络的训练包含了两个过程： 正向传播 (Forward Propagation)，从输入经过一层层神经网络，最后得到 $\hat{y}$ ，从而计算代价函数 $J$ 反向传播 (Back Propagation)，根据损失函数 $L(\hat{y},y)$ 来反方向的计算每一层参数的偏导数，从而更新参数 下面我们用计算图 (Computation Graph) 来理解这个过程 从左向右，可以计算出 $J​$ 的值，对应着神经网络中输入经过计算得到代价函数 $J(w,b)​$ 值的过程 从右向左，根据求导的链式法则，可以得到： \frac{\partial J}{\partial a} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial a} = 3 \cdot 1 = 3 \frac{\partial J}{\partial b} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial u} \cdot \frac{\partial u}{\partial b} = 3 \cdot 1 \cdot c = 6 \frac{\partial J}{\partial c} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial u} \cdot \frac{\partial u}{\partial c} = 3 \cdot 1 \cdot b = 9在反向传播中，一般我们只关心最终输出值 (在这个例子中是 $J$ ) ，需要计算 $J$ 对于某个变量 (记作var) 的导数，即 $\frac {dJ}{dvar}​$，在Python代码中简写为dvar Logistic Regression Gradient Descent现在，我们来实现对数几率回归梯度下降算法，只考虑单个样本的情况 : $z = w^{T} + b$ $\hat{y} = a = \sigma({z})​$ $L(\hat{y},y) = -(y \log\hat{y} + (1-y) \log(1-\hat{y}))​$ 假设样本只有两个，分别为 $x1, x2$，则计算图如下 : 在对数几率回归中，我们需要做的是，改变参数 $w, b$ 的值，来最小化损失函数，即需要计算出 $dw, dz$ 向后传播计算损失函数 $L​$ 的偏导数步骤如下： $da = \frac {\partial L(a,y)}{\partial a} = -\frac {y}{a} + \frac{1-y}{1-a}​$ $dz = \frac {\partial L}{\partial z} = \frac {\partial L}{\partial a} \cdot \frac {da}{dz}= (-\frac {y}{a} + \frac{1-y}{1-a}) \cdot a \cdot (1-a) = a - y​$ $dw_1 = \frac {\partial L}{\partial w_1} = \frac {\partial L}{\partial z} \cdot \frac { \partial z}{\partial w_1} = x_1 \cdot dz ​$ $dw_2 = \frac {\partial L}{\partial w_2} = \frac {\partial L}{\partial z} \cdot \frac { \partial z}{\partial w_2} = x_2 \cdot dz ​$ $db = \frac {\partial L}{\partial b} = \frac {\partial L}{\partial z} \cdot \frac { \partial z}{\partial b} = dz​$ 所以，在对数几率回归梯度下降算法中你需要做的是 $ dz = a - y$ $dw_1 = x_1 \cdot dz ​$ $dw_2 = x_2 \cdot dz ​$ $db = dz​$ 更新$w_1​$, $w_1 = w_1 - \alpha dw_1​$ 更新$w_2$, $w_2 = w_2 - \alpha dw_2$ 更新$b​$, $b = b - \alpha db​$ Gradient descent on $m$ examples之前只实现了单个样本的梯度下降算法，现在我们将梯度下降算法应用到整个训练集 $J(w,b) = \frac{1}{m} \sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)}) $ $a^{(i)} = \hat{y}^{(i)} = \sigma(z^{(i)}) = \sigma(w^{T}x^{(i)}+b)​$ $\frac {\partial}{\partial w_1}J(w,b) = \frac {1}{m} \sum_{i=1}^{m} \frac {\partial}{\partial w_1}L(a^{(i)},y^{(i)}) = \frac {1}{m} \sum_{i=1}^{m}dw_1^{(i)}$ $dw_1^{(i)}​$按照之前单个样本的情况计算 伪代码如下 : $J=0; dw_1=0; dw_2=0; db=0;$$for \quad i = 1 \quad to \quad m $$\quad z^{(i)} = w^{T}x^{(i)}+b$$\quad a^{(i)} = \sigma(z^{(i)})$$\quad J += -(y^{(i)} \log a^{(i)} + (1-y^{(i)}) \log(1-a^{(i)}))$$\quad dz^{(i)} = a^{(i)}-y^{(i)}$$\quad dw_1 += x_1^{(i)}dz^{(i)}$$\quad dw_2 += x_2^{(i)}dz^{(i)} \qquad$$\quad…\quad\quad\quad\quad\quad\quad$ //这里应该是一个循环，这里 $n_x = 2$$\quad db += dz^{(i)}$​$J /= m$$dw_1 /= m$$dw_2 /= m$$db /= m$ $w_1 = w_1 - \alpha dw_1​$$w_2 = w_2 - \alpha dw_2​$$b = b - \alpha db​$ 但这种方法，有两个循环，一个是最外层的循环，循环 $m$ 个训练样本，另一个是 $dw_1, dw_2$ (feature) 的循环，在这个例子中 $n_x = 2$。随着训练集越来越大，应该尽量避免使用for循环，而使用向量化技术 (vectorization)]]></content>
      <categories>
        <category>deeplearning.ai</category>
        <category>Deep learning &amp; NN</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java常量池]]></title>
    <url>%2F2019%2F01%2F17%2FJava-Constant-Pool%2F</url>
    <content type="text"><![CDATA[常量池 相同的值只存储一份，节省内存，共享访问，提高运行效率 基本类型的包装类 Boolean Byte Short Integer Long Character Float Double 八种基本类型的包装类 常量值范围 Boolean：true, false Byte Character : \u0000 - \u007f Short Integer Long : -128 - 127 Float Double : 无常量池 ==与equals() 对于基本数据类型，==比较他们的数值 对于对象，==比较两个对象在内存中的存放地址，可以通过重写equals()来比较两个对象的内容是否相等 字符串常量 Java为常量字符串建立了常量池缓存机制123456String s1 = "abc";String s2 = "ab" + "c";String s3 = "a" + "b" + "c"; //都是常量，是确定的，编译器将优化System.out.println(s1==s2); //trueSystem.out.println(s1==s3); //trueSystem.out.println(s2==s3); //true 基本类型的包装类和字符串的两种创建方式 字面值赋值，放在栈内存（将被常量化） Integer a = 1; String b = &quot;abc&quot;; new对象进行创建，放在堆内存（不会常量化） Integer c = new Integer(1); String d = new String(&quot;abc&quot;); 栈内存读取速度快，容量小 堆内存读取速度慢，容量大，可以通俗的理解为Java认为new出来的对象所占内存较大（不确定，而字面值是确定的），所以需要放在堆内存 Integer常量池的例子12345678910111213141516171819int i1 = 10;Integer i2 = 10; //自动装箱，10本来只是int，是基本类型，而我们需要把它变成一个对象，相当于包装了一层System.out.println(i1==i2) //true//自动拆箱 基本类型和包装类进行比较，包装类自动拆箱 Integer i3 = new Integer(10);System.out.println(i1==i3) //true 同理，包装类自动拆箱System.out.println(i2==i3) //false i2,i3都是对象，而i2是常量，在常量池，i3是new出来的对象，在堆内存中 Integer i4 = new Integer(5);Integer i5 = new Integer(5);System.out.println(i1 == (i4+i5)); //trueSystem.out.println(i1 == (i4+i5)); //trueSystem.out.println(i1 == (i4+i5)); //true//i4+i5的操作将会使i4,i5自动拆箱为基本类型并运算得到10，而根据之前所提到的，基本类型和包装类进行比较，包装类自动拆箱，所以都为trueInteger i6 = i4 + i5;System.out.println(i1==i6); //true，同理i4+i5的操作使i4,i5自动拆箱，得到10，相当于Integer i6 = 10;System.out.println(i3==i6); //false String常量池的例子字符串常量池存在于方法区，方法区包含的都是在整个程序中唯一的元素，如static变量 一个简单的例子 1234567String s1 = "abc";String s2 = "abc";String s3 = new String("abc");String s4 = new String("abc");System.out.println(s1==s2); //true 都是常量池System.out.println(s1==s3); //false 一个是栈内存，一个是堆内存System.out.println(s3==s4); //false 都是堆内存，但是不同对象 图解：(&quot;由&#39;代替) graph LR; subgraph 方法区 s['abc'] end subgraph 堆 A["s3 = new String('abc')"] B["s4 = new String('abc')"] end subgraph 栈 s1 s2 s3 s4 end s1-->s s2-->s A-->s B-->s s3-->A s4-->B 更为复杂的例子123456789101112String s5 = "abcdef";String s6 = s1 + "def"; //涉及到变量（不确定的），编译器不会优化String s7 = "abc" + "def"; //都是常量，编译器会优化成abcdefString s8 = "abc" + new String("def"); //涉及到new对象，编译器不优化System.out.println(s6==s7); //falseSystem.out.println(s6==s8); //falseSystem.out.println(s7==s8); //falseSystem.out.println(s5==s7); //trueString s9 = s3 + "def"; //由于s3是new的，涉及到new对象，编译器不优化System.out.println(s7==s9); //false//对于s5~s9，只有s5,s7是在常量池中，其余都在堆内存上，且地址互不相同]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim Tutorial]]></title>
    <url>%2F2019%2F01%2F14%2FVim-Tutorial%2F</url>
    <content type="text"><![CDATA[This tutorial includes some basic vim commands and I hope that it will be helpful. Moving the cursor h : left j : down k : up l : right It takes time to get used to it. Navigation w: move the cursor one word forward (to the first letter) b : one word backward (also to the first letter) e : one word forward (to the last letter) fx : forward to the letter x ( : to the start of the sentence ) : start of the sentence 0 : start of line $ : end of line { : start of paragraph } : end of paragraph G : end of file ctrl+G : to see the cursor location and file status gg : start of file xG : to the number x line of file typing a number before a motion repeats it that many times! Delete x: delete the character at the cursor dw: delete all the characters between the cursor and the first letter of the next word e.g. Please delete the word. (Assume the cursor is at l) After you press dw, the sentence becomes Please dethe word delete de: delete all the characters between the cursor and the next space e.g. Please delete the word. (Assume the cursor is at l) After you press de, the sentence becomes Please de the word delete d$ : delete to end of line dd : delete whole line p : After you delete something, press p to paste things you delete wherever you like. Insert a : insert after the cursor A : insert after the end of line i : insert before the cursor I : insert before the start of line o : insert in the next line O : insert in the previous line Search /yourSearchString + &lt;Enter&gt; : search for yourSearchString n : to search for the same string again (press &lt;Enter&gt; to exit) N : to search for the same string again, but in opposite direction ctrl+o : to go back to where you came from ctrl+i : to go forward set option :set ic : ignore case :set noic : disable ignore case :set hls : highlight the matches :set nohls : disable highlight matches :set is : increase search :set nois: disable increase search % : move the cursor to the other matching parenthesis Replace rx : replace the character at cursor with x ce : almost the same as de, but this time will place you in Insert Mode s/old/new : replace the first occurrence of ‘old’ with ‘new’ s/old/new/g : replace all occurrence of ‘old’ with ‘new’ in one line #,#/old/new/g : #,# are the line numbers of the range of lines where the replace should be done %s/old/new/g : replace all occurrence of ‘old’ with ‘new’ in the whole file %s/old/new/g : replace all occurrence of ‘old’ with ‘new’ in the whole file, with a prompt whether to replace or not Undo &amp; Redo u : undo the last command U : undo the command excuting on the while line ctrl+R : redo the command Copy &amp; Paste y : to copy p : to paste e.g. Start Visual Mode with v and move the cursor to chose whatever you want, type y to copy the highlighted text and type p to paste the text. Others . : repeat the last command &lt;start position&gt;&lt;command&gt;&lt;end position&gt; : many commands follow this pattern e.g. 0y$ means copy the whole line 0 move the cursor to the start of line y copy $ move the cursor to the end of line ctrl+n : auto complete]]></content>
      <categories>
        <category>vim</category>
      </categories>
      <tags>
        <tag>vim</tag>
      </tags>
  </entry>
</search>
