<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Objected-Oriented Analysis and Design-2]]></title>
    <url>%2F2019%2F02%2F17%2FObject-Oriented-2%2F</url>
    <content type="text"><![CDATA[本文为华东师范大学开设的面向对象分析与设计慕课课程第二周笔记，主要介绍了UML统一建模语言。 UML (Unified Modeling Language) 统一建模语言由于客户和编程人员对需求理解的差异，且完整地理解一个复杂的系统很困难，所以需要建模。建模是为了能够更好地理解正在开发的系统。 建模 所谓建模就是把不太理解的东西和一些已经较为理解、且十分类似的东西做比较，从而对这些不太理解的东西产生更深刻的理解 模型 建模产生的结果就是模型，模型是对现实的简化、对事物的一种抽象 模型可以帮助人们更好地了解事物的本质，抓住问题的要害 在模型中，人们总是剔除那些与问题无关的、非本质的东西，从而使模型与真实的实体相比更加简单、易于把握 建模的目的 帮助我们按照需要对系统进行可视化 允许我们详细说明系统的结构和行为 给出了一个指导我们构造系统的模板 对我们所做出的决策进行文档化 建模的四项基本原理 选择要创建什么模型，不同的模型会有不同的启发 每一种模型可以在不同的精度级别上表示 A model is an abstraction of the real world. Good models are still connected with reality. 最好的模型都是与现实相关联的 模型都是对现实的简化，但是简化不能掩盖掉任何重要的细节 单个模型是不充分的, 对每一个重要的系统最好用一组几乎独立的模型去处理。 UML统一建模语言 “事实上的”工业标准，相当于软件工程师的工具包 UML的构造块 (记住它们的常用符号) 事物（结构事物，行为事物，分组事物，注释事物） 关系（依赖，关联，泛化，实现） 图（类图，对象图，顺序图，通信图，构建图，活动图，包图，用例图，状态图，部署图） UML公有机制 详述 修饰 通用划分 扩展机制 构造型 (用于自定义)，标记值，约束 用例图用例图的建模元素有边界，参与者，用例，关系 Actor 参与者 代表位于系统之外并和系统进行交互的一类事物（人、物、其他软件子系统等） 通过它，可以对软件系统与外界发生的交互进行分析和描述 通过它，可以了解客户希望软件系统提供哪些功能 如何确定参与者 谁使用系统？Who or what uses the system 谁安装系统、维护系统？Who installs the system? Who maintains the system 谁启动系统、关闭系统？Who starts and stops the system 谁从系统中获取信息，谁提供信息给系统？Who gets and provides information to the system 在系统交互中，谁扮演了什么角色？What roles do they play in the interaction 系统会与哪些其他系统相关联？What other systems interact with this system 内/外部定时器 Does anything happen at a fixed time? UseCase 用例 系统为响应参与者引发的一个事件而执行的一系列的处理/动作，而这些处理应该为参与者产生一种有价值的结果，这些动作不但应包含正常情况的各种动作序列，而且应包含对非正常情况时软件系统的动作序列的描述。用例名称一般用短小精悍的“动名词” 寻找用例 参与者希望系统提供什么功能 Start with actors, then identify what they want to do What functions will the actor want from the system ? 系统是否存储和检索信息 当系统改变状态时，是否通知参与者 Are any actors notified when the system changes ? 是否存在影响系统的外部事件，是哪个参与者通知系统这些外部事件 Are there external events that notify the system ? 哪个参与者触发了活动？Which actors trigger activity ? 用例图中的关系 参与者与用例之间 关联关系 参与者与参与者之间 泛化关系 用例之间的关系 泛化关系 如下订单和网上下订单 包含关系 include 如取钱用例的输入密码，因为你做任何事情都需要输入密码 扩展关系 extend 如取钱用例的打印单据，你可以选择在取钱以后是否打印单据 UseCase description用例描述一般由一个主事件流和多个异常事件流描述 主事件流：一切正常时的动作序列 异常事件流或可选事件流流：主事件流的每一步都有可能出现异常，此处描述异常情况的处理 活动图活动图描述了在一个过程中，顺序的/并行的活动及其之间的关系，一般用于业务过程，复杂算法建模。 活动图是顶点和弧的集合，包括活动节点，动作，流，对象值，注解和约束等 开始、结束、对象 活动节点 一个活动是一个过程中进行的非原子的执行单元 活动的执行最终延伸为一些独立动作 (Action) 的执行 分支 一个分支可以有一个进入流和多个离去流 在每个离去流上必须设置一个监护条件 条件放在方括号里 条件不能重叠，以免二义性 可以有 [else] 分支 两个控制路径可以重新合并，无需监护条件 Forking and Joining 分岔和汇合 分岔表示把一个单独的控制流分成两个或多个并发的控制流 汇合表示两个或多个并发控制流的同步发生，一个汇合可以有两个或多个进入转移和一个输出转移在UML中，用同步棒来说明并行控制流的分岔和汇合 Swimlanes 泳道 将一个活动图中的活动分组，每一组表示一个特定的类别、人或部门，他们负责完成组内的活动 每个组被称为一个泳道 每个活动严格地属于一个用到，同步棒可以跨越泳道 活动图与用例模型互为补充，主要用于需求分析阶段 类图Class 类 具有相同属性、操作、方法、关系或者行为的一组对象的描述符 类是真实世界事物的抽象 问题领域的类：在对系统建模时，将会涉及到如何识别业务系统中的事物，这些事物构成了整个业务系统。在UML中，把所有的这些事物都建模为类 (class) Object 对象 当这些事物存在于真实世界中时，它们是类的实例，并被称为对象 同一个类的各对象具有相同的属性，但属性的取值可以不同，提供相同的操作、有相同的语义 类图中的UML元素 类之间的关系：关联关系，依赖关系，泛化关系，实现关系 关联的修饰 名称及其方向 关联关系的多重性 角色 类：内容包括名称，属性，操作，职责 关联类，Association class is an association that is also a class, and consists of the class, association and the dashed line 顺序图一种详细描述对象之间以及对象与参与者之间交互的图，它由一组相互协作的对象或参与者实例以及它们之间发送的消息组成，强调消息之间的顺序，可用于动态验证模型的可行性。顺序图验证的某一功能，属于某个用例描述的功能中的一部分 (称为用例实现)。 顺序图的构成参与者，对象生命线，执行规约，消息 对象生命线 表示对象在一段时间的存在 执行规约 (或称控制焦点) 执行规约 (execution specification) 是一个对象执行一个操作的时期 消息 对象间的协作与交流表现为一个对象以某种方式启动另一个对象的活动，这种交流在UML里被定义为消息 同步消息：如有对象A与对象B，A给B发送同步消息，要求对象B处理好消息并且返回结果，对象A才会继续往下 异步消息：如有对象A与对象B，A给B发送同步消息，这时候A是将消息发送到B的队列里面，接着A继续做自己的事情，等到有空的时候，就去看看自己的队列中有没有消息，如果有就把这个请求拿出来处理，如果没有就继续做自己的事情。B也是如此，当B闲下来后发现A发来了消息，那么它就会处理并返回给A结果，A在它空闲时查看队列就会得到返回结果，过程如下图： 结构化控制在顺序图中，除了按顺序排列消息外，还应表示对消息进行选择、循环和并行处理 通信图顺序图与通信图本质上是一样的，在语义上是等价的。但建模的角度不同，前者反映了对象之间协作的时间顺序，后者反映了对象之间协作的结构关系，强调了对象之间的交流。 通信图的构成对象，链接，在链接上的消息 状态图描述了单个对象在其生命周期内响应事件所经历的状态序列以及动态行为 State machine状态机 是一种行为，说明对象在它的生命期中, 响应事件所经历的状态序列以及它们对每个事件的响应 State Diagram 状态图 状态机可以用状态图来可视化，状态图显示了一个状态机，它强调从状态到状态的控制流 State 状态 是对象的生命期中的一个条件或状况，在此期间，对象可以响应事件、执行某活动等 状态由以下几个部分组成 名称 进入/退出动作 (entry/exit action) 内部迁移 (internal transition) 子状态 延迟事件 (deferred event) Event 事件 是对一个在时间和空间上占有一定位置的有意义的事情的描述 在状态机的语境中，一个事件是一个激励的发生，它能够触发一个状态迁移 UML对四种事件进行建模 change event 参量变化 signal 信号 (异步) call 调用 (同步) 时间事件 Transition 迁移 在状态A，发生事件并满足一定条件，转到状态B。一个迁移由5部分组成： source state 源状态 event trigger 事件触发器 guard condition 触发条件 effect 效应 (或称迁移动作) 目标状态 特殊的迁移 self transition 自身迁移 internal transition 内部迁移 状态图建模注意事项 不允许孤立的状态存在 不允许只进不出的状态迁移 不允许只出不进的状态迁移 不允许没有事件发生的迁移]]></content>
      <categories>
        <category>Software Engineering</category>
        <category>Object-Oriented</category>
      </categories>
      <tags>
        <tag>Object-Oriented</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Objected-Oriented Analysis and Design-1]]></title>
    <url>%2F2019%2F02%2F16%2FObject-Oriented-1%2F</url>
    <content type="text"><![CDATA[本文为华东师范大学开设的面向对象分析与设计慕课课程第一周笔记，主要讲了面向对象方法学，面向对象的起源以及面向对象的相关概念与特征 面向对象分析与设计面向对象之父——艾伦.C.凯 启发来源: 生物学上，细胞信息的传递方式 传送数据时，将数据以及对数据的处理方法一起打包 类与对象Class 类A class is a description (描述符) of a set of objects that share the same attributes, operations, relationships, and semantics 类是共享相同属性、操作、方法、关系或行为的一组对象的描述符 Object 对象An object is an instance created from a class 一个对象是根据一个类创建的一个实例 An instance’s behaviour and information structure is defined in the class.Its current state (values of instance variables) is determined by operations performed on it. Message 消息对象之间的一种交流手段 All objects of a paricular type can receive the same messages. 面向对象的思考方式面向对象适合解决不确定的事件、创新性的事件 面向过程处理已知的事实、重要的条件都已知的场景 面向对象的核心特征Encapsulation 封装 Encapsulation is the process of hiding the implementation details of an object 隐藏了对象的实现细节 The internal state is usually not accessible by other objects 内部的状态不为其他对象所访问 The only access to manipulate the object data is through its interface 对象的数据只能通过接口进行访问 Encapsulation allows objects to be viewed as ‘black boxes’ 封装使得对象可以被看成一个“黑盒子” It protects an object’s internal state from being corrupted by other objects. 保护数据 Also, other objects are protected from changes in the objectimplementation. 一个对象实现方法的改变，不影响其他相关对象 Communication is achieved through an ‘interface’ 对象间通过“接口”进行通信 为什么封装？ 保护隐私 保护数据安全 隔离复杂度 封装原则 An object should only reveal the interfaces needed to interact with it. Details not pertinent to the use of the object should be hidden from other objects. 只公开必须公开的属性 使用 Getters and Setters 来实现信息隐藏 Inheritance 继承 A class gets the state and behavior of another class and adds additional state and behavior 一个类从其他的类里面获取它的状态和行为，同时加上自己的一些额外的状态和行为 Ploymorphism 多态 When one class inherits from another, then polymorphism allows a subclass to stand in for the superclass. 当一个类从另一个类继承而来，多态使得子类可以代替父类 The sender of a stimulus doesn’t need to know the receiver’s class. 消息发送方不需要知道消息接收方属于哪个子类 Different receivers can interpret the message in their own way. 同一类族的接收者可以按自己的方式处理消息 使用指向父类的指针或者引用，能够调用子类的对象，这是多态的核心思想，也是设计模式的基础 Aggregation &amp; Composition 聚合&amp;组合 Aggregation describes a “has a” relationship. One object is a part of another object. XX有XX这种关系 Aggregation relationships are transitive. 聚合有传递性 if A contains B an B contains C, then A contains C Aggregation relationships are asymmetric. 聚合是不对称的 If A contains B, then B does not contain A A variant of aggregation is composition which adds the property of existence dependency. 组合是聚合的一个变种 组合特别强调整体控制着部分的生命，比如手指和手掌 但有些时候，聚合的组合的关系不是很明确，含糊不清的话一般用聚合 Interface &amp; Implementation 接口&amp;实现 Interface describes how users of the class interact with the class 描述一个类的用户如何与这个类交互 使得当对其中某一个类进行局部修改的时候，不影响其他的类 实现即完成接口所定义的功能 Abstraction 抽象 A process allowing to focus on most important aspects while ignoring less important details. 简单地说，抽象就是专注于最重要的部分而忽略掉那些不太重要部分的过程 抽象是面向对象领域发现类的主要方法]]></content>
      <categories>
        <category>Software Engineering</category>
        <category>Object-Oriented</category>
      </categories>
      <tags>
        <tag>Object-Oriented</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Neural Networks-2]]></title>
    <url>%2F2019%2F02%2F08%2FDeep-Neural-Networks-2%2F</url>
    <content type="text"><![CDATA[本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第四周深层神经网络的相关课程及作业。 Building blocks of deep neural networks本节，我们将用网络块来深入理解正向传播和反向传播的过程，由于之前部分已经详细解释了正向传播和反向传播，这里不再详述，本节只是起到补充说明加深理解的作用。 上图展示了神经网络正向传播和反向传播中数据的传递的整个过程。顺便提一下，在正向传播中，我们需要缓存 $z^{[l]}$ ，因为在反向传播中会用到。 Forward and backward propagation总结一下，如何在深层神经网络中实现前向传播和反向传播 Forward propagation for layer $l$ Input $a^{[l-1]}$ Output $a^{[l]}$, cache $(z^{[l]})$ Step： z^{[l]} = W^{[l]} \cdot a^{[l-1]}+b^{[l]} a^{[l]} = g^{[l]}(z^{[l]})After vectorization： Z^{[l]} = W^{[l]}A^{[l-1]}+b^{[l]} A^{[l]} = g^{[l]}(Z^{[l]}) Backward propagation for layer $l$ Input $da^{[l]}$ Output $da^{[l-1]}, dW^{[l]}, db^{[l]}$ Step： dz^{[l]} = da^{[l]} * g^{[l]'}(z^{[l]}) dW^{[l]} = dz^{[l]} \cdot a^{[l-1]} db^{[l]} = dz^{[l]} da^{[l-1]} = W^{[l]T} \cdot dz^{[l]}After vectorization： dZ^{[l]} = dA^{[l]} * g^{[l]'}(Z^{[l]}) dW^{[l]} = \frac{1}{m}dZ^{[l]}A^{[l-1]T} db^{[l]} = \frac{1}{m}np.sum(dZ^{[l]},axis=1,keepdims=True) dA^{[l-1]} = W^{[l]T} \cdot dZ^{[l]} Parameters vs. Hyperparameters在神经网络中，参数有：$W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]} …​$ ，超参数有：学习率 (learning rate) $\alpha​$ ，梯度下降迭代次数 (iteration) ，隐藏层 (hidden layer) 个数， 隐藏层神经元个数 $n^{[1]}, n^{[2]} …​$ ，激活函数的选择等等。这些超参数控制了最后参数的变化，因此称为超参数，在接下来的课程会深入探讨。 选择最优的超参数常常是困难的，它需要靠我们的经验，以及一次次的尝试，从而获得更好的参数来训练出更优的模型。 Homework-Building your Deep Neural Network123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301import numpy as npimport h5pyimport matplotlib.pyplot as pltfrom testCases_v3 import *from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward# initialize parameters 初始化一个 2-layer 神经网络def initialize_parameters(n_x, n_h, n_y): """ Argument: n_x -- size of the input layer n_h -- size of the hidden layer n_y -- size of the output layer Returns: parameters -- python dictionary containing your parameters: W1 -- weight matrix of shape (n_h, n_x) b1 -- bias vector of shape (n_h, 1) W2 -- weight matrix of shape (n_y, n_h) b2 -- bias vector of shape (n_y, 1) """ np.random.seed(1) W1 = np.random.randn(n_h, n_x) * 0.01 b1 = np.zeros((n_h, 1)) W2 = np.random.randn(n_y, n_h) * 0.01 b2 = np.zeros((n_y, 1)) assert (W1.shape == (n_h, n_x)) assert (b1.shape == (n_h, 1)) assert (W2.shape == (n_y, n_h)) assert (b2.shape == (n_y, 1)) parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125; return parameters# initialize parameters 初始化一个 l-layer 神经网络def initialize_parameters_deep(layer_dims): """ Arguments: layer_dims -- python array (list) containing the dimensions of each layer in our network Returns: parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL": Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1]) bl -- bias vector of shape (layer_dims[l], 1) """ np.random.seed(3) parameters = &#123;&#125; L = len(layer_dims) # number of layers in the network for l in range(1, L): parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01 parameters['b' + str(l)] = np.zeros((layer_dims[l], 1)) assert (parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1])) assert (parameters['b' + str(l)].shape == (layer_dims[l], 1)) return parameters# 前向传播，仅仅计算 Zdef linear_forward(A, W, b): """ Implement the linear part of a layer's forward propagation. Arguments: A -- activations from previous layer (or input data): (size of previous layer, number of examples) W -- weights matrix: numpy array of shape (size of current layer, size of previous layer) b -- bias vector, numpy array of shape (size of the current layer, 1) Returns: Z -- the input of the activation function, also called pre-activation parameter cache -- a python dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently """ Z = np.dot(W, A) + b assert (Z.shape == (W.shape[0], A.shape[1])) cache = (A, W, b) return Z, cache# 对于某一层前向传播的整个过程def linear_activation_forward(A_prev, W, b, activation): """ Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer Arguments: A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples) W -- weights matrix: numpy array of shape (size of current layer, size of previous layer) b -- bias vector, numpy array of shape (size of the current layer, 1) activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu" Returns: A -- the output of the activation function, also called the post-activation value cache -- a python dictionary containing "linear_cache" and "activation_cache"; stored for computing the backward pass efficiently """ if activation == "sigmoid": Z, linear_cache = linear_forward(A_prev, W, b) A, activation_cache = sigmoid(Z) elif activation == "relu": Z, linear_cache = linear_forward(A_prev, W, b) A, activation_cache = relu(Z) assert (A.shape == (W.shape[0], A_prev.shape[1])) cache = (linear_cache, activation_cache) return A, cache# 整个模型的前向传播整个过程，对于 l-layer 神经网络def L_model_forward(X, parameters): """ Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation Arguments: X -- data, numpy array of shape (input size, number of examples) parameters -- output of initialize_parameters_deep() Returns: AL -- last post-activation value caches -- list of caches containing: every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2) the cache of linear_sigmoid_forward() (there is one, indexed L-1) """ caches = [] A = X L = len(parameters) // 2 # number of layers in the neural network 因为同时有w,b两个参数，所以除2 # 循环调用，从第1层到第L-1层 for l in range(1, L): A_prev = A A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation="relu") caches.append(cache) # 第L层使用sigmoid函数 AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation="sigmoid") caches.append(cache) assert (AL.shape == (1, X.shape[1])) return AL, caches# 计算成本函数def compute_cost(AL, Y): """ Implement the cost function defined by equation (7). Arguments: AL -- probability vector corresponding to your label predictions, shape (1, number of examples) Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples) Returns: cost -- cross-entropy cost """ m = Y.shape[1] # 注意这里np.log(AL).T需要转置,因为Y和AL都是列向量 cost = (-1. / m) * (np.dot(Y, np.log(AL).T) - np.dot(1 - Y, np.log(1 - AL).T)) cost = np.squeeze(cost) # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17). assert (cost.shape == ()) return cost# 反向传播，仅仅是对于线性函数z = w.T * x + bdef linear_backward(dZ, cache): """ Implement the linear portion of backward propagation for a single layer (layer l) Arguments: dZ -- Gradient of the cost with respect to the linear output (of current layer l) cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer Returns: dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev dW -- Gradient of the cost with respect to W (current layer l), same shape as W db -- Gradient of the cost with respect to b (current layer l), same shape as b """ A_prev, W, b = cache m = A_prev.shape[1] dW = 1. / m * np.dot(dZ, A_prev.T) db = 1. / m * np.sum(dZ, axis=1, keepdims=True) dA_prev = np.dot(W.T, dZ) assert (dA_prev.shape == A_prev.shape) assert (dW.shape == W.shape) assert (db.shape == b.shape) return dA_prev, dW, db# 对于每一层整个反向传播过程，其中对于dZ = dA * g'(Z)的计算已经有函数relu_backward,sigmoid_backward帮你实现了def linear_activation_backward(dA, cache, activation): """ Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer. Arguments: dA -- post-activation gradient for current layer l cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu" Returns: dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev dW -- Gradient of the cost with respect to W (current layer l), same shape as W db -- Gradient of the cost with respect to b (current layer l), same shape as b """ linear_cache, activation_cache = cache if activation == "relu": dZ = relu_backward(dA, activation_cache) dA_prev, dW, db = linear_backward(dZ, linear_cache) elif activation == "sigmoid": dZ = sigmoid_backward(dA, activation_cache) dA_prev, dW, db = linear_backward(dZ, linear_cache) return dA_prev, dW, db# 整个模型的反向传播过程，对于 l-layer 神经网络def L_model_backward(AL, Y, caches): """ Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group Arguments: AL -- probability vector, output of the forward propagation (L_model_forward()) Y -- true "label" vector (containing 0 if non-cat, 1 if cat) caches -- list of caches containing: every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2) the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1]) Returns: grads -- A dictionary with the gradients grads["dA" + str(l)] = ... grads["dW" + str(l)] = ... grads["db" + str(l)] = ... """ grads = &#123;&#125; L = len(caches) # the number of layers m = AL.shape[1] Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL # Initializing the backpropagation 对于dAL的计算公式已经给出了 dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) current_cache = caches[L - 1] grads["dA" + str(L)], grads["dW" + str(L)], grads["db" + str(L)] = linear_activation_backward(dAL, current_cache, activation="sigmoid") for l in reversed(range(L - 1)): current_cache = caches[l] dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads["dA" + str(l + 2)], current_cache, activation="relu") grads["dA" + str(l + 1)] = dA_prev_temp grads["dW" + str(l + 1)] = dW_temp grads["db" + str(l + 1)] = db_temp return grads# 更新参数def update_parameters(parameters, grads, learning_rate): """ Update parameters using gradient descent Arguments: parameters -- python dictionary containing your parameters grads -- python dictionary containing your gradients, output of L_model_backward Returns: parameters -- python dictionary containing your updated parameters parameters["W" + str(l)] = ... parameters["b" + str(l)] = ... """ L = len(parameters) // 2 # number of layers in the neural network # Update rule for each parameter. Use a for loop. for l in range(L): parameters["W" + str(l + 1)] = parameters["W" + str(l + 1)] - learning_rate * grads["dW" + str(l + 1)] parameters["b" + str(l + 1)] = parameters["b" + str(l + 1)] - learning_rate * grads["db" + str(l + 1)] return parametersdef main(): plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots plt.rcParams['image.interpolation'] = 'nearest' plt.rcParams['image.cmap'] = 'gray' np.random.seed(1)main() Homework-Deep Neural Network Application123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209import timeimport numpy as npimport h5pyimport matplotlib.pyplot as pltimport scipyfrom PIL import Imagefrom scipy import ndimagefrom dnn_app_utils_v2 import *# 两层神经网络def two_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False): """ Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID. Arguments: X -- input data, of shape (n_x, number of examples) Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) layers_dims -- dimensions of the layers (n_x, n_h, n_y) num_iterations -- number of iterations of the optimization loop learning_rate -- learning rate of the gradient descent update rule print_cost -- If set to True, this will print the cost every 100 iterations Returns: parameters -- a dictionary containing W1, W2, b1, and b2 """ np.random.seed(1) grads = &#123;&#125; costs = [] # to keep track of the cost m = X.shape[1] # number of examples (n_x, n_h, n_y) = layers_dims # Initialize parameters dictionary, by calling one of the functions you'd previously implemented parameters = initialize_parameters(n_x, n_h, n_y) # Get W1, b1, W2 and b2 from the dictionary parameters. W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Inputs: "X, W1, b1". Output: "A1, cache1, A2, cache2". A1, cache1 = linear_activation_forward(X, W1, b1, activation="relu") A2, cache2 = linear_activation_forward(A1, W2, b2, activation="sigmoid") # Compute cost cost = compute_cost(A2, Y) # Initializing backward propagation dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2)) # Backward propagation. Inputs: "dA2, cache2, cache1". Outputs: "dA1, dW2, db2; also dA0 (not used), dW1, db1". dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation="sigmoid") dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation="relu") # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2 grads['dW1'] = dW1 grads['db1'] = db1 grads['dW2'] = dW2 grads['db2'] = db2 # Update parameters. parameters = update_parameters(parameters, grads, learning_rate) # Retrieve W1, b1, W2, b2 from parameters W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] # Print the cost every 100 training example if print_cost and i % 100 == 0: print("Cost after iteration &#123;&#125;: &#123;&#125;".format(i, np.squeeze(cost))) if print_cost and i % 100 == 0: costs.append(cost) # plot the cost plt.plot(np.squeeze(costs)) plt.ylabel('cost') plt.xlabel('iterations (per tens)') plt.title("Learning rate =" + str(learning_rate)) plt.show() return parameters# 5层神经网络def L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False): # lr was 0.009 """ Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID. Arguments: X -- data, numpy array of shape (number of examples, num_px * num_px * 3) Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) layers_dims -- list containing the input size and each layer size, of length (number of layers + 1). learning_rate -- learning rate of the gradient descent update rule num_iterations -- number of iterations of the optimization loop print_cost -- if True, it prints the cost every 100 steps Returns: parameters -- parameters learnt by the model. They can then be used to predict. """ np.random.seed(1) costs = [] # keep track of cost # Parameters initialization. parameters = initialize_parameters_deep(layers_dims) # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID. AL, caches = L_model_forward(X, parameters) # Compute cost. cost = compute_cost(AL, Y) # Backward propagation. grads = L_model_backward(AL, Y, caches) # Update parameters. parameters = update_parameters(parameters, grads, learning_rate) # Print the cost every 100 training example if print_cost and i % 100 == 0: print("Cost after iteration %i: %f" % (i, cost)) if print_cost and i % 100 == 0: costs.append(cost) # plot the cost plt.plot(np.squeeze(costs)) plt.ylabel('cost') plt.xlabel('iterations (per tens)') plt.title("Learning rate =" + str(learning_rate)) plt.show() return parametersdef main(): plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots plt.rcParams['image.interpolation'] = 'nearest' plt.rcParams['image.cmap'] = 'gray' np.random.seed(1) train_x_orig, train_y, test_x_orig, test_y, classes = load_data() # Explore your dataset m_train = train_x_orig.shape[0] num_px = train_x_orig.shape[1] m_test = test_x_orig.shape[0] # Reshape the training and test examples train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T # The "-1" makes reshape flatten the remaining dimensions test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T # Standardize data to have feature values between 0 and 1. train_x = train_x_flatten / 255. test_x = test_x_flatten / 255. """ 两层神经网络 # CONSTANTS DEFINING THE MODEL n_x = 12288 # num_px * num_px * 3 n_h = 7 n_y = 1 layers_dims = (n_x, n_h, n_y) parameters = two_layer_model(train_x, train_y, layers_dims=(n_x, n_h, n_y), num_iterations=2500, print_cost=True) """ """ 五层神经网络 """ ### CONSTANTS ### layers_dims = [12288, 20, 7, 5, 1] # 5-layer model parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations=2500, print_cost=True) # 训练集正确率 predictions_train = predict(train_x, train_y, parameters) # 测试集正确率 predictions_test = predict(test_x, test_y, parameters) # 自定义图片 my_image = "a.jpg" # change this to the name of your image file my_label_y = [1] # the true class of your image (1 -&gt; cat, 0 -&gt; non-cat) fname = "images/" + my_image image = np.array(ndimage.imread(fname, flatten=False)) my_image = scipy.misc.imresize(image, size=(num_px, num_px)).reshape((num_px * num_px * 3, 1)) my_predicted_image = predict(my_image, my_label_y, parameters) plt.imshow(image) print("y = " + str(np.squeeze(my_predicted_image)) + ", your L-layer model predicts a \"" + classes[ int(np.squeeze(my_predicted_image)),].decode("utf-8") + "\" picture.") plt.show()main()]]></content>
      <categories>
        <category>deeplearning.ai</category>
        <category>Deep learning &amp; NN</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Neural Networks-1]]></title>
    <url>%2F2019%2F02%2F08%2FDeep-Neural-Networks-1%2F</url>
    <content type="text"><![CDATA[本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第四周深层神经网络的相关课程。 Deep L-layer Neural network主要复习了之前 Logistic Regression 和单隐藏层的神经网络，并推广到多隐藏层，同时也介绍了深层神经网络的一些符号约定，基本遵循了之前的规则，所以这里不再详述。 Forward propagation in a deep network以 $4$ 层的神经网络为例，我们来推导一下对于单个样本的正向传播过程： 第一层： z^{[1]} = w^{[1]}x + b^{[1]} = w^{[1]}a^{[0]} + b^{[1]} a^{[1]} = g^{[1]}(z^{[1]})推广到第 $l$ 层： z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]} a^{[l]} = g^{[l]}(z^{[l]})第 $l$ 层向量化版本： Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} A^{[l]} = g^{[l]}(Z^{[l]})这里有一点需要注意，对于深层神经网络正向传播，它类似一个循环的过程，即从 $1$ to $l$ 的循环，对每一层分别计算正向传播的过程。你可能会想到前面所说的向量化来优化，但这里避免不了显式的 $for$ 循环。 Getting your matrix dimensions right在实现深层神经网络的过程中，想要降低 bug 的出现概率，你必须非常仔细地检查矩阵维数是否正确。本节就来讨论如何确定矩阵的维数。 以下图的神经网络为例： 不难看出 $l=5, n^{[0]}=n_x=2, n^{[1]}=3, n^{[2]}=5, n^{[3]}=4, n^{[4]}=2, n^{[5]}=1$，如果我们想要进行正向传播，那么首先需要计算 z^{[1]} = W^{[1]}x + b^{[1]}这里的 $z^{[1]}​$ 的维度为 $(n^{[1]},1)​$ ，$x​$ 的维度为 $(n^{[0]},1)​$ ，根据矩阵乘法的规则，可以得到 $w^{[1]}​$ 的维度必为 $(n^{[1]},n^{[0]})​$ ，同样可以得到 $b^{[1]}​$ 的维度为 $(n^{[1]},1)​$ 。 总结一下， $W^{[l]}​$ 的维度为 $(n^{[l]},n^{[l-1]})​$ ，$b^{[1]}​$ 的维度为 $(n^{[l]},1)​$ 。顺便提一下，$dw^{[l]},db^{[l]}​$ 的维度与 $W^{[l]},b^{[l]}​$ 相同。 接着，我们需要计算 a^{[1]} = g^{[1]}(z^{[1]})可以得到，$a^{[l]}$ 的维度与 $z^{[l]}$ 相同，同样为 $(n^{[l]},1)$。 总结一下： W^{[l]} :(n^{[l]},n^{[l-1]}) z^{[l]},a^{[l]},b^{[l]} :(n^{[l]},1)$dw,db$ 的维度与 $W,b$ 相同 接下来，我们看一下向量化后，各矩阵的维数，同样需要计算 Z^{[1]} = W^{[1]}X + b^{[1]}我们将 $m$ 个样本横向堆叠，那么 $Z^{[1]}$ 的维度为 $(n^{[1]},m)$ ，$X$ 的维度为 $(n^{[0]},m)$ ，$W^{[1]}$ 的维度为 $(n^{[1]},n^{[0]})$ ，$b^{[1]}$ 的维度为 $(n^{[0]},1)$ ，由于 Broadcasting ，所以实际运算中，会将 $b^{[1]}$ 复制 $m$ 次，形成维度为 $(n^{[0]},m)$ 的矩阵。 在向量化后，各矩阵维度如下： W^{[l]} :(n^{[l]},n^{[l-1]}) Z^{[l]},A^{[l]} :(n^{[l]},m) b^{[l]} :(n^{[l]},1)同样，$dZ,dA$ 与 $Z,A​$ 维度一样 Why deep representations本节从几个例子来探讨了为什么深度神经网络如此强大。 人脸识别 神经网络第一层，你可以把它当成一个边缘探测器 (edge detector) 从原始图片中识别人脸的边缘，下方图片中的一个小方块就是一个隐藏单元，代表着边缘的方向，从而识别边缘。神经网络第二层就将前一层的边缘信息进行组合，组合成面部的不同部分，比如：眼睛，鼻子等等。最后再将这些局部特征放在一起，比如：鼻子，眼睛，下巴，就可以识别整张人脸。可以看出，随着神经网络由浅入深，所获得的信息也是从部分到整体。更深入的部分会在卷积神经网络中讨论。 语音识别 和人脸识别有些相似，也是通过这种从简单到复杂，从部分到整体的方式，在前几层先学习一些简单的特征，再后面几层进行组合，最后去识别更复杂的东西。以语音识别为例，第一层你可能会试着探测一些低层次的音频波形特征，比如：音调，白噪音等等。然后将这些波形组合在一起，就能去探测声音的基本单元，即音位 (phonemes) ，比如 cat 发音的 /k/ /æ/ /t/ 分别为三个音位，有了基本的声音单元后，我们就可以识别单词，从而识别词组，再到整个句子。在前几层学习一些简单的特征，再后面几层进行组合，去识别更复杂的东西。]]></content>
      <categories>
        <category>deeplearning.ai</category>
        <category>Deep learning &amp; NN</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shallow Neural Networks-2]]></title>
    <url>%2F2019%2F02%2F01%2FShallow-Neural-Networks-2%2F</url>
    <content type="text"><![CDATA[本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第三周浅层神经网络的相关课程及相关作业。 Why do you need non-linear activation function为什么神经网络需要非线性的激活函数？不能使用线性的激活函数，比如 $g(z) = z$ 吗？ 假设我们使用线性的激活函数 $g(z) = z$ ，那么有： a^{[1]} = z^{[1]} = W^{[1]}x + b^{[1]} a^{[2]} = z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}把 $a^{[1]}$ 带入，则有 a^{[2]} = W^{[2]}(W^{[1]}x + b^{[1]}) + b^{[2]} = (W^{[2]}W{[1]})x + (W^{[2]}b^{[1]}+b{[2]}) = W'x+b'我们可以得到，$a^{[2]}$ 仍是输入 $x$ 的线性组合。也就是说，使用线性函数的神经网络仅仅只是把输入 $x$ 线性组合再输出。即便是包含许多隐藏层的神经网络，如果使用的是线性的激活函数，不管多少层，得到的输出依然是 $x$ 的线性组合，也就意味着隐藏层根本没有什么作用。所以，隐藏层激活函数必须是非线性的，否则将失去意义。 只有一个地方你可能会使用线性激活函数，在机器学习的回归问题中，$y$ 是一个实数，比如你像预测房地产的价格，那么 $y$ 是一个实数，而不是像二分类问题那样要么 $0$ 要么 $1$ ，这种情况下，在输出层你可能会使用线性激活函数，但隐藏层不会使用线性激活函数。 Derivatives of activation functions在反向传播的过程中，我们需要计算激活函数的导数，那么我们来看一下上述这些激活函数的导数。 Sigmoid 函数 g(z) = \frac{1}{1+e^{-z}} g'(z) = g(z)(1-g(z)) = a(1-a) tanh 函数 g(z) = \frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} g'(z) = 1-(g(z))^{2} = 1-a^2 ReLU 函数 g(z) = max(0,z) g'(z) = \begin{cases} 0,\quad z < 0 \\ 1,\quad z \geq 0 \end{cases} Leaky ReLU 函数 g(z) = max(0.01z,z) g'(z) = \begin{cases} 0.01,\quad z < 0 \\\ 1,\quad \quad z \geq 0 \end{cases}Gradient descent for neural networks好了，有了以上的铺垫，我们终于可以实现在单隐藏层神经网络上的梯度下降算法了。 由于是单隐藏层神经网络，那么我们有参数 $W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}​$ 。我们一般用 $n_x = n^{[0]}​$ 来表示输入层特征的个数，用 $n^{[1]}​$ 表示隐藏层节点个数，用 $n^{[2]}=1​$ 表示输出层节点个数。 其中，$W^{[1]}​$ 的维度为 $(n^{[1]}, n^{[0]})​$ ，$b^{[1]}​$ 的维度为 $(n^{[1]}, 1)​$ ，$W^{[2]}​$ 的维度为 $(n^{[2]}, n^{[1]})​$ ，$b^{[2]}​$ 的维度为 ​$(n^{[2]}, 1)​$ 。 假设我们在做二元分类，那么 Cost function 为： J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \frac{1}{m}\sum_{i=1}^{m}L(\hat{y}, y)整个训练神经网络的过程为： Repeat{ initialize parameters $W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]} $ compute predicts $\hat{y}^{(i)}, i \in [1,m]​$ compute $dW^{[1]}, db^{[1]}, W^{[2]}, b^{[2]}$ update $W^{[1]} = W^{[1]} - \alpha dW^{[1]}, b^{[1]} = b^{[1]} - \alpha db^{[1]} …​$ } 在训练神经网络时，随机初始化参数很重要，并不是单纯的全部初始化为 $0​$ ，我们将在后续详细讨论。 在之前，我们讨论了如果计算 predict (预测值) ，以及如何向量化实现整个过程，所以现在的关键在于，如何计算这些偏导项 $dW^{[1]}, db^{[1]}​$ … 神经网络正向传播的过程为： Z^{[1]} = W^{[1]}X + b^{[1]} A^{[1]} = g(Z^{[1]}) Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} A^{[2]} = g(Z^{[2]}) = \sigma(Z^{[2]})反向传播的过程为： dZ^{[2]} = A^{[2]}-Y dW^{[2]} = \frac{1}{m}dZ^{[2]}A^{[1]T} db^{[2]} = \frac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True) 这个过程其实和之前的 Logistic Regression 很相似。需要注意的是，这里$db^{[2]}$ 的计算直接用了 python 语句，np.sum函数的参数 axis 代表在哪个维度上求和，keepdims为了保持 $db^{[2]}$ 的形状为 $(n^{[2]},1)$ 而不是奇怪的 $(n^{[2]}, )$ dZ^{[2]} = W^{[2]T}dZ^{[2]} * g^{[1]'}(Z^{[1]}) dW^{[1]} = \frac{1}{m}dZ^{[1]}X^{T} db^{[1]}=\frac{1}{m}np.sum(dZ^{[1]},axis = 1,keepdims = True) 需要注意的是 $dZ^{[2]} = W^{[2]T}dZ^{[2]} *g^{[1]’}(Z^{[1]})$ 这里是对应元素相乘，$W^{[2]T}dZ^{[2]}$ 的形状为 $(n^{[1]}, m)$ ，$g’^{[1]}(Z^{[1]})$ 的形状也为 $(n^{[1]}, m)$ 。 关于详细的推导过程，会在接下来的部分详细说明。 Backpropagation intuition先回顾一下之前我们在实现 Logistic Regression 时是如何推导的，可以参考之前的博客 Logistic Regression Gradient Descent. 由于现在多了一层隐藏层，整个反向传播过程会更复杂一点。 首先，先考虑单个样本的情况，先画出计算图，如下图： 根据导数链式法则，可以计算出： dz^{[2]} = \frac{\partial L}{\partial z^{[2]}} = \frac{\partial L}{\partial a^{[2]}} \cdot \frac{\partial a^{[2]}}{\partial z^{[2]}}= a^{[2]}-y dW^{[2]}= \frac{\partial L}{\partial W^{[2]}} = \frac{\partial L}{\partial z^{[2]}} \cdot \frac{\partial z^{[2]}}{\partial W^{[2]}} = dz^{[2]} \cdot \frac{\partial z^{[2]}}{\partial W^{[2]}} = dz^{[2]}a^{[1]T} 注意，这里 $dW^{[2]}$ 的形状为 $(n^{[2]}, n^{[1]})$ ， $n^{[1]}$ 为隐藏层节点个数，而 $a^{[1]}$ 形状为 $(n^{[1]}, n^{[2]})$ ，故这里需要 $a^{[1]}$ 转置，即 $a^{[1]T}$ 。 db^{[2]}= \frac{\partial L}{\partial b^{[2]}} = \frac{\partial L}{\partial z^{[2]}} \cdot \frac{\partial z^{[2]}}{\partial b^{[2]}} = dz^{[2]} \cdot \frac{\partial z^{[2]}}{\partial b^{[2]}} = dz^{[2]} dz^{[1]} = \frac{\partial L}{\partial z^{[1]}} = \frac{\partial L}{\partial a^{[1]}} \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}} = \frac{\partial L}{\partial z^{[2]}} \cdot \frac{\partial z^{[2]}}{\partial a^{[1]}} \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}} = dz^{[2]} \cdot \frac{\partial z^{[2]}}{\partial a^{[1]}} \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}}即， dz^{[1]} = W^{[2]T}dz^{[2]} * g^{[1]'}(z^{[1]}) 同样，这里的 $W^{[2]T}$ 也需要转置，具体为什么，我也不是很明白，但可以从矩阵乘法的规则来判断其形状是否正确。需要注意的是，这里的乘法 $*$ 为对应元素相乘，而不是一般意义上的矩阵乘法。在实现过程中，你必须要确保矩阵的形状相匹配，这里 $W^{[2]T}$ 形状为 $(n^{[1]}, n^{[2]})$ ，$dz^{[2]}$ 的形状为 $(n^{[2]}, 1)$ ，$z^{[1]}$ 的形状为 $(n^{[1]}, 1)$ dW^{[1]} = dz^{[1]} \cdot \frac{\partial z^{[1]}}{\partial W^{[1]}} = dz^{[1]}x^{T} = dz^{[1]}a^{[0]T} db^{[1]} = dz^{[1]} \cdot \frac{\partial z^{[1]}}{\partial b^{[1]}} = dz^{[1]}到这里为止，我们就推导完了反向传播的6个公式。接下来，我们需要将其推广到 $m$ 个训练样本的向量化实现上，得到结果如下，： Random Initialization神经网络中的参数 $W$ 是不能和 Logistic Regression 那样全部初始化为 $0$ 的，我们来分析一下原因。 假设我们有这样一个神经网络，如下图所示： 假设，我们将 $W^{[1]}, W^{[2]}$ 都初始化为零矩阵，那么经过正向传播以后，我们会得到 $a^{[1]}_1 = a^{[1]}_{2}$ ，那么根据对称性，在反向传播后会有 $dz^{[1]}_{1} = dz^{[1]}_{2}$ ， $dW^{[1]}_{1}=dW^{[1]}_{2}$ ，无论你执行多少次梯度下降算法，隐藏层的每个节点都在做相同的操作。这样的话，最后我们获得的 $W^{[1]}, W^{[2]}$ 每行元素都相同，也就是说所有隐藏层中的节点都可以用一个节点来代替，多余的节点没有任何意义，这不是我们想要的。 另外，参数 $b$ 可以全部初始化为 $0$ ，不会发生上面提到的问题。 所以，我们必须随机初始化所有的参数。python 语句如下： 1234W1 = np.random.randn((2,2))*0.01b1 = np.zero((2,1))W1 = np.random.randn((1,2))*0.01b1 = 0 你可能为由疑问，为什么这里要 $*0.01$ ，为什么是 $0.01$ 而不是其他的数字？事实上，我们倾向于把矩阵初始化为非常非常小的随机值。因为如果你用 tanh 函数或者 sigmoid 函数作为激活函数， $W$ 比较小，那正向传播后得到的 $z$ 也会比较小，经过激活函数后所得到的 $a$ 也会接近于 $0$ ，而在 $0$ 附近，激活函数的斜率比较大，能大大地提高梯度下降算法的更新速度，即学习的速度。如果你使用的激活函数为 ReLU 或是 Leaky ReLU 则没有这个问题。 有时候，会有比 $0.01$ 更好用的常数，但如果你只是训练一个单隐层神经网络，或是一个相对较浅的神经网络，没有太多隐藏层，使用 $0.01$ 没有太大问题。但是当你训练一个很深的神经网络时，你可能需要尝试一下别的常数，关于常数的详细内容会在后续部分提到。 Homework123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309# Package importsimport numpy as npimport matplotlib.pyplot as pltfrom testCases_v2 import *import sklearnimport sklearn.datasetsimport sklearn.linear_modelfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasetsdef layer_sizes(X, Y): """ Arguments: X -- input dataset of shape (input size, number of examples) Y -- labels of shape (output size, number of examples) Returns: n_x -- the size of the input layer n_h -- the size of the hidden layer n_y -- the size of the output layer """ n_x = X.shape[0] # size of input layer n_h = 4 n_y = Y.shape[0] # size of output layer return (n_x, n_h, n_y)def initialize_parameters(n_x, n_h, n_y): """ Argument: n_x -- size of the input layer n_h -- size of the hidden layer n_y -- size of the output layer Returns: params -- python dictionary containing your parameters: W1 -- weight matrix of shape (n_h, n_x) b1 -- bias vector of shape (n_h, 1) W2 -- weight matrix of shape (n_y, n_h) b2 -- bias vector of shape (n_y, 1) """ np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random. W1 = np.random.randn(n_h, n_x) * 0.01 b1 = np.zeros((n_h, 1)) W2 = np.random.randn(n_y, n_h) * 0.01 b2 = np.zeros((n_y, 1)) assert (W1.shape == (n_h, n_x)) assert (b1.shape == (n_h, 1)) assert (W2.shape == (n_y, n_h)) assert (b2.shape == (n_y, 1)) parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125; return parametersdef forward_propagation(X, parameters): """ Argument: X -- input data of size (n_x, m) parameters -- python dictionary containing your parameters (output of initialization function) Returns: A2 -- The sigmoid output of the second activation cache -- a dictionary containing "Z1", "A1", "Z2" and "A2" """ # Retrieve each parameter from the dictionary "parameters" W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] # Implement Forward Propagation to calculate A2 (probabilities) Z1 = np.dot(W1, X) + b1 A1 = np.tanh(Z1) Z2 = np.dot(W2, A1) + b2 A2 = sigmoid(Z2) assert (A2.shape == (1, X.shape[1])) cache = &#123;"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2&#125; return A2, cachedef compute_cost(A2, Y, parameters): """ Computes the cross-entropy cost given in equation (13) Arguments: A2 -- The sigmoid output of the second activation, of shape (1, number of examples) Y -- "true" labels vector of shape (1, number of examples) parameters -- python dictionary containing your parameters W1, b1, W2 and b2 Returns: cost -- cross-entropy cost given equation (13) """ m = Y.shape[1] # number of example # Compute the cross-entropy cost logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y) cost = -np.sum(logprobs) / m cost = np.squeeze(cost) # makes sure cost is the dimension we expect. # E.g., turns [[17]] into 17 assert (isinstance(cost, float)) return costdef backward_propagation(parameters, cache, X, Y): """ Implement the backward propagation using the instructions above. Arguments: parameters -- python dictionary containing our parameters cache -- a dictionary containing "Z1", "A1", "Z2" and "A2". X -- input data of shape (2, number of examples) Y -- "true" labels vector of shape (1, number of examples) Returns: grads -- python dictionary containing your gradients with respect to different parameters """ m = X.shape[1] # First, retrieve W1 and W2 from the dictionary "parameters". W1 = parameters["W1"] W2 = parameters["W2"] # Retrieve also A1 and A2 from dictionary "cache". A1 = cache["A1"] A2 = cache["A2"] # Backward propagation: calculate dW1, db1, dW2, db2. dZ2 = A2 - Y dW2 = np.dot(dZ2, A1.T) / m db2 = np.sum(dZ2, axis=1, keepdims=True) / m dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2)) dW1 = np.dot(dZ1, X.T) / m db1 = np.sum(dZ1, axis=1, keepdims=True) / m grads = &#123;"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2&#125; return gradsdef update_parameters(parameters, grads, learning_rate=1.2): """ Updates parameters using the gradient descent update rule given above Arguments: parameters -- python dictionary containing your parameters grads -- python dictionary containing your gradients Returns: parameters -- python dictionary containing your updated parameters """ # Retrieve each parameter from the dictionary "parameters" W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] # Retrieve each gradient from the dictionary "grads" dW1 = grads["dW1"] db1 = grads["db1"] dW2 = grads["dW2"] db2 = grads["db2"] # Update rule for each parameter W1 = W1 - learning_rate * dW1 b1 = b1 - learning_rate * db1 W2 = W2 - learning_rate * dW2 b2 = b2 - learning_rate * db2 parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125; return parametersdef nn_model(X, Y, n_h, num_iterations=10000, print_cost=False): """ Arguments: X -- dataset of shape (2, number of examples) Y -- labels of shape (1, number of examples) n_h -- size of the hidden layer num_iterations -- Number of iterations in gradient descent loop print_cost -- if True, print the cost every 1000 iterations Returns: parameters -- parameters learnt by the model. They can then be used to predict. """ np.random.seed(3) n_x = layer_sizes(X, Y)[0] n_y = layer_sizes(X, Y)[2] # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: "n_x, n_h, n_y". Outputs = "W1, b1, W2, b2, parameters". parameters = initialize_parameters(n_x, n_h, n_y) W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache". A2, cache = forward_propagation(X, parameters) # Cost function. Inputs: "A2, Y, parameters". Outputs: "cost". cost = compute_cost(A2, Y, parameters) # Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads". grads = backward_propagation(parameters, cache, X, Y) # Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters". parameters = update_parameters(parameters, grads) # Print the cost every 1000 iterations if print_cost and i % 1000 == 0: print("Cost after iteration %i: %f" % (i, cost)) return parametersdef predict(parameters, X): """ Using the learned parameters, predicts a class for each example in X Arguments: parameters -- python dictionary containing your parameters X -- input data of size (n_x, m) Returns predictions -- vector of predictions of our model (red: 0 / blue: 1) """ # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold. A2, cache = forward_propagation(X, parameters) predictions = (A2 &gt; 0.5) return predictionsdef main(): # random seed np.random.seed(1) # set a seed so that the results are consistent # Load data X, Y = load_planar_dataset() # Visualize the data # 原来的代码会报错，同样planar_utils.py 21行也需要修改 plt.scatter(X[0, :], X[1, :], c=Y.reshape(X[0, :].shape), cmap=plt.cm.Spectral) # plt.show() # shape of dataset shape_X = X.shape shape_Y = Y.shape m = X.shape[1] # training set size # Build a model with a n_h-dimensional hidden layer parameters = nn_model(X, Y, n_h=4, num_iterations=10000, print_cost=True) # Plot the decision boundary plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y) plt.title("Decision Boundary for hidden layer size " + str(4)) plt.show() # Print accuracy predictions = predict(parameters, X) print('Accuracy: %d' % float( (np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%') # # Train the logistic regression classifier # clf = sklearn.linear_model.LogisticRegressionCV() # clf.fit(X.T, Y.T) # # # Plot the decision boundary for logistic regression # plot_decision_boundary(lambda x: clf.predict(x), X, Y) # plt.title("Logistic Regression") # plt.show() # # # Print accuracy # LR_predictions = clf.predict(X.T) # print('Accuracy of logistic regression: %d ' % float( # (np.dot(Y, LR_predictions) + np.dot(1 - Y, 1 - LR_predictions)) / float(Y.size) * 100) + # '% ' + "(percentage of correctly labelled datapoints)")main()]]></content>
      <categories>
        <category>deeplearning.ai</category>
        <category>Deep learning &amp; NN</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shallow Neural Networks-1]]></title>
    <url>%2F2019%2F02%2F01%2FShallow-Neural-Networks-1%2F</url>
    <content type="text"><![CDATA[本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第三周浅层神经网络的相关课程。 Neural Network Overview本周，你将学会如何实现神经网络。上周，我们讨论了对数几率回归 (logistic regression) ，并且使用计算图 (computation graph) 的方式了解了梯度下降算法的正向传播和反向传播的两个过程，如下图所示 : 而神经网络 (Neural Network) 是这个样子，如下图 : 我们可以把很多 sigmoid 单元堆叠起来来构成一个神经网络。在之前所学的对数几率回归中，每一个节点对应着两个计算步骤：首先计算 $z=w^{T}x + b​$ ，然后计算 $a=\sigma(z)​$ 。在这个神经网络中，三个竖排堆叠的节点就对应着这两部分的计算，那个单独的节点也对应着另一个类似的 $z, a​$ 的计算。 在神经网络中，所用的符号也会有些不一样。我们还是用 $x$ 来表示输入特征，用 $W^{[1]}, b^{[1]}$ 来表示参数，这样你就可以计算出 $z^{[1]} = W^{[1]}x + b^{[1]}$ 。这里右上角的 $[1]$ 代表着节点所属的层，你可以认为层数从 $0$ 开始算起，如上图中的 $x_1, x_2, x_3$ 就代表着第 $0$ 层（也称为输入层），三个竖排的节点就属于第 $1$ 层（也称为隐藏层），单独的那个节点属于第 $2$ 层（也称为输出层）。需要注意的是，这与之前用来标注第 $i$ 个训练样本 $(x^{(i)}, y^{(i)})$ 不同，这里用的是方括号。 那么，在这个神经网络模型中，正向传播就分为两层。 从输入层到隐藏层：在使用类似对数几率回归的方法计算了 $z^{[1]}$ 之后，再计算 $a^{[1]}=\sigma(z^{[1]})​$ 。 从隐藏层到输出层：使用相同的方法计算 $z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$ 。当然，这里的参数 $W^{[2]}, b^{[2]}$ 与 $W^{[1]}, b^{[1]}$ 不同，且第 $1$ 层的输出 $a^{[1]}$ 作为第 $2$ 层的输入 $x$ ，接着同样计算 $a^{[2]}=\sigma(z^{[2]})$ ，得到的 $a^{[2]}​$ 就是整个神经网络的输出。 同样，还需要通过反向传播计算 $da^{[2]}, dz^{[2]}$ 等等，这些将会在后面详细讨论。 One hidden layer Neural Network下图是一张单隐藏层的神经网络，也称为双层神经网络 (2 layer NN) 。我们把最左边的 $x1, x2, x3$ 称为输入层 （Input Layer) ，中间称为隐藏层 (Hidden Layer) ，最右边只有一个节点的称为输出层 (Output Layer) ，负责输出预测值 $\hat{y}$ 。在计算神经网络的层数时，不算入输入层。 由于在训练过程中，我们看不到这些中间节点的真正数值，不像输入，输出层那样，所以称为隐藏层。 之前，我们用 $x$ 来表示输入，其实它还有一种表示方式 $a^{[0]}$ ，这个 $a$ 有 activation (激活) 的意思，意味这它把不同层的值传递给下一层，起到了激活的作用。用上标 $[i]$ 表示在第 $i$ 层，用下标 $j$ 表示这层中第 $j$ 个节点，如 $a^{[1]}_{2}$ 即表示第 $1$ 层的第 $2$ 个节点。那么上图中隐藏层的4个节点可以写成矩阵的形式： a^{[1]} = \left( \begin{array}{c} a^{[1]}_{1} \\\ a^{[1]}_{2} \\\ a^{[1]}_{3} \\\ a^{[1]}_{4} \end{array} \right)Computing a Neural Network’s Output接下来，我们来看神经网络的输出是如何计算出来的。我们可以把神经网络的计算看作对数几率回归的多次重复计算。 我们先来回顾一下对数几率回归的计算过程，如下图： 这里的圆圈代表了，对数几率回归的两个步骤。我们先隐去其他节点，如右图，那么它就和对数几率回归非常相似，我们可以计算出 $z^{[1]}_{1} = w^{[1]T}_{1}x+b^{[1]}_{1}$ ， $a^{[1]}_{1} = \sigma(z^{[1]}_{1})$ ，上标代表层数，下标表示这层上的第几个节点。 以此类推，我们可以写出： 回想起之前所讲的向量化，如果我们想让程序高效的运行，就必须将其向量化。 我们首先先将 $w$ 向量化，由于有4个对数几率回归单元，而每一个回归单元都有其对应的参数向量 $w$ ，且每一个回归单元都有输入 $x_1, x_2, x_3​$ ，所以我们可以得到： W^{[1]} = \left( \begin{array}{c} w^{[1]T}_{1} \\\ w^{[1]T}_{2} \\\ w^{[1]T}_{3} \\\ w^{[1]T}_{4} \end{array} \right)_{(4 \times 3)}那么，我们可以得到如下式子： Z^{[1]} = \left( \begin{array}{c} w^{[1]T}_{1} \\\ w^{[1]T}_{2} \\\ w^{[1]T}_{3} \\\ w^{[1]T}_{4} \end{array} \right) \cdot \left( \begin{array}{c} x_1 \\\ x_2 \\\ x_3 \end{array} \right) + \left( \begin{array}{c} b^{[1]}_{1} \\\ b^{[1]}_{2} \\\ b^{[1]}_{3} \\\ b^{[1]}_{4} \end{array} \right) = \left( \begin{array}{c} w^{[1]T}_{1}x+b^{[1]}_{1} \\\ w^{[1]T}_{2}x+b^{[1]}_{2} \\\ w^{[1]T}_{3}x+b^{[1]}_{3} \\\ w^{[1]T}_{4}x+b^{[1]}_{4} \end{array} \right) = \left( \begin{array}{c} z^{[1]}_{1} \\\ z^{[1]}_{2} \\\ z^{[1]}_{3} \\\ z^{[1]}_{4} \end{array} \right) a^{[1]} = \left( \begin{array}{c} a^{[1]}_{1} \\\ a^{[1]}_{2} \\\ a^{[1]}_{3} \\\ a^{[1]}_{4} \end{array} \right) = \sigma(Z^{[1]}) =\sigma(W^{[1]}x + b^{[1]})在本神经网络中，你就应该计算 (为了更好理解，右下角标注了矩阵的形状) ： z^{[1]}_{(4 \times 1)} = W^{[1]}_{(4 \times 3)}x_{(3 \times 1)}+ b^{[1]}_{(4 \times 1)} a^{[1]}_{4 \times 1} = \sigma(z^{[1]}_{(4 \times 1)})记得我们之前说过，可以用 $a^{[0]}​$表示 $x​$ ，所以 $z^{[1]}​$ 有可以写成 $z^{[1]} = W^{[1]}a^{[0]}+ b^{[1]}​$ ，那么可以用同样的方法推导出： z^{[2]}_{(1 \times 1)} = W^{[2]}_{(1 \times 4)}a^{[1]}_{(4 \times 1)}+ b^{[2]}_{(1 \times 1)} a^{[2]}_{1 \times 1} = \sigma(z^{[2]}_{(1 \times 1)})所以在代码中，我们只需实现上述的4行代码。然而这是对单个样本的，即对于输入的特征向量 $x$ ，可以计算出 $\hat{y} = a^{[2]}​$ ，对于整个训练集的向量化将在接下来的部分介绍。 Vectorizing across multiple examples接下来，我们实现对整个训练集的向量化。假设你有 $m​$ 个训练样本，理解了上述理论，那么我们可以通过输入 $x^{(i)}​$ 计算得出 $ \hat{y}^{(i)} = a^{[2](i)}​$ 其中 $i \in [1,m]​$ 。 还记得我们之前把输入的特征向量 $x^{(1)}, x^{(2)}, …, x^{(m)}​$ 横向堆叠起来，得到了一个 $(n_x \times m)​$ 的矩阵 $X​$ ，即 \mathbf{X} =\left( \begin{array}{c}\vdots & \vdots & \ldots & \vdots \\\\x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\\vdots & \vdots & \ldots & \vdots\end{array} \right)_{(n_x \times m)}同样的，我们把 $x^{(1)}, x^{(2)}, …, x^{(m)}​$ 横向堆叠起来，那么我们需要计算的式子变为： Z^{[1]} = W^{[1]}X + b^{[1]} A^{[1]} = \sigma(Z^{[1]}) Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} A^{[2]} = \sigma(Z^{[2]})其中，$Z^{[1]}​$ 的形状为 $(4 \times m)​$ ，你可以这样理解，每一个样本对应着矩阵的一列，所以有 $m​$ 列，隐藏层中的每一个节点对应着矩阵的一行，所以 $Z^{[1]}​$ 有 $4​$ 行，规律同样适用于 $X, A​$。 Activation functions到目前为止，我们一直选择 sigmoid 函数作为 activation function (激活函数) ，但有时使用其他函数效果要好得多，它们各自有不同的特点，下面我们来介绍几个不同的激活函数 $g(x)$： tanh (双曲正切函数)，实际上是 sigmoid 函数向下平移后再经过拉伸得到的。对于隐藏单元，如果你选择 tanh 作为激活函数，它的表现几乎总是比 sigmoid 函数要好，因为 tanh 函数的输出介于 $(-1,1)$ 之间，激活函数的平均值更接近于 $0$ ，而不是 $0.5$ ，这让下一层的学习更方便一点。所以之后我们几乎不再使用 sigmoid 作为激活函数了，但有一个例外，即选择输出层的激活函数的时候，因为二分类问题的输出为 $\{0,1\}$ ，你更希望 $\hat{y}$ 介于 $0,1$ 之间，所以一般会选择 sigmoid 函数。 所以之前所举的例子中，你可以使用 tanh 函数作为隐藏层的激活函数，而选择 sigmoid 函数作为输出层的激活函数。同样的，可以使用上标来表示每一层的激活函数，如：$g^{[1]}(x) = tanh(z), g^{[2]} = \sigma(z)​$ 然而，不管是 tanh 还是 sigmoid 函数，都有一个缺点，如果 $z​$ 特别大或者特别小，那么在这一点的函数导数会很小，因此会拖慢梯度下降算法。为了弥补这个缺点，就出现了 ReLU (rectified linear unit) 函数，该函数有如下特点：当 $z​$ 为正，导数为 $1​$，当 $z​$ 为负，导数为 $0​$ ，当 $z​$ 为 $0​$ 时，导数不存在，但在实际使用中， $z​$ 几乎不会等于 $0​$ ，当然你可以在程序中直接把在 $z=0​$ 点的导数赋为 $0​$ 或 $1​$ 。 但 ReLU 也有一个缺点，当 $z$ 为负时，导数恒等于 $0$ 。所以就有了 Leaky ReLU 函数，通常表现比 ReLU 函数更好，但实际使用中频率没那么高。 ReLU 和 Leaky ReLU 的优势在于，对于 $z$ 的许多取值，激活函数的导数和 $0$ 差的很远，这也就意味着，在实践中你的神经网络的学习速度会快很多。 总结一下，在我们一般选择 sigmoid 作为输出层的激活函数，而选择 ReLU 作为其他层的激活函数，ReLU 如今被人们广泛使用。]]></content>
      <categories>
        <category>deeplearning.ai</category>
        <category>Deep learning &amp; NN</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Basic of Neural Networks-2]]></title>
    <url>%2F2019%2F01%2F26%2FBasic-of%20Neural-Networks-2%2F</url>
    <content type="text"><![CDATA[本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第二周神经网络基础相关课程及第二周作业。 VectorizationVectorization (向量化) 的意义在于：消除代码中显式的调用for循环。在深度学习领域中，你常常需要训练大数据集，所以程序运行的效率非常重要，否则需要等待很长时间才能得到结果。 在对数几率回归中，你需要计算 $z = w^{T}x + b$ ，其中 $w, x$ 都是 $n_x$ 维向量 如果你是用非向量化的实现，即传统的矩阵相乘算法伪代码如下：$ z = 0 $$ for \quad i \quad in \quad range(n_x) : $$ \quad z+= w[i] * x[i] $$z+=b ​$ 若使用向量化的实现，Python代码如下：1z = np.dot(w,x) + b 即清晰又高效 可以测试一下这两个代码在效率方面的差距，大约差了300倍。可以试想一下，如果你的代码1分钟出结果，和5个小时才出结果，那可差太远了。 为了加快深度学习的运算速度，可以使用GPU (Graphic Processing Unit) 。事实上，GPU 和 CPU 都有并行指令 (Parallelization Instructions) ，同时也叫作 SIMD (Single Instruction Multiple Data)，即单指令流多数据流，是一种采用一个控制器来控制多个处理器，同时对一组数据 (又称“数据向量”) 中的每一个分别执行相同的操作从而实现空间上的并行性的技术。numpy 是 Python 数据分析及科学计算的基础库，它有许多内置 (Built-in) 函数，主要用于数组的计算，充分利用了并行化，使得运算速度大大提高。在深度学习的领域，一般来说，能不用显式的调用for循环就不用。 这样，我们就可以使用 Vectorization 来优化梯度下降算法，先去掉内层对 feature (特征 $w1, w2 …$) 的循环 ： $J=0; db=0; dw = np.zeros(n_x,1)$$for \quad i = 1 \quad to \quad m $$\quad z^{(i)} = w^{T}x^{(i)}+b$$\quad a^{(i)} = \sigma(z^{(i)})$$\quad J += -(y^{(i)} \log a^{(i)} + (1-y^{(i)}) \log(1-a^{(i)}))$$\quad dz^{(i)} = a^{(i)}-y^{(i)}$$\quad dw+=x^{(i)}dz^{(i)} \quad \quad $ //vectorization$\quad db += dz^{(i)}$$J /= m$$dw /= m$$db /= m​$ 然后，我们再去掉对 $m$ 个训练样本的外层循环，分别从正向传播和反向传播两方面来分析： 正向传播回顾一下对数几率回归的正向传播步骤，如果你有 $m$ 个训练样本 那么对第一个样本进行预测，你需要计算$ \quad z^{(1)} = w^{T}x^{(1)} + b$$ \quad a^{(1)} = \sigma(z^{(1)})​$ 然后继续对第二个样本进行预测$ \quad z^{(2)} = w^{T}x^{(2)} + b$$ \quad a^{(2)} = \sigma(z^{(2)})$ 然后继续对第三个，第四个，…，直到第 $m$ 个 回忆一下之前在二分分类部分所讲到的用更紧凑的符号 $X​$ 表示整个训练集，即大小为 $(n_x,m)​$ 的矩阵 : \mathbf{X} = \left( \begin{array}{c} \vdots & \vdots & \ldots & \vdots \\\\ x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\ \vdots & \vdots & \ldots & \vdots \end{array} \right)那么计算 $z^{(1)}, z^{(2)}, … , z^{(m)}$ 的步骤如下 :首先先构造一个 $(1,m)$ 的矩阵 $[z^{(1)}, z^{(2)}, … , z^{(m)}]$ ，则 Z = [z^{(1)}, z^{(2)}, ... , z^{(m)}] = w^{T}X + [b, b , ... , b] = [w^{T}x^{(1)} + b, w^{T}x^{(2)} + b] , ... , w^{T}x^{(m)} + b]在 Python 中一句代码即可完成上述过程1Z = np.dot(w.T,X) + b 你可能会有疑问，明明这里的 $b$ 只是一个实数 (或者说是一个 $b_{(1,1)}$ 的矩阵) ，为什么可以和矩阵 $Z_{(1,m)}$ 相加？事实上，当做 $Z+b$ 这个操作时，Python 会自动把矩阵 $b_{(1,1)}$ 自动扩展为 $b_{(1,m)}$ 这样的一个行向量，在 Python 中这称为 Broadcasting (广播) ，现在你只要看得懂就好，接下来会更详细地说明它。 同理我们可以得到 A = [a^{(1)}, a^{(2)}, ... , a^{(m)}] = \sigma (Z)同样也只需一句 Python 代码 1A = sigmoid(Z) 反向传播接着，我们来看如何用向量化优化反向传播，计算梯度 同样，你需要计算$dz^{(1)} = a^{(1)} - y^{(1)}​$$dz^{(2)} = a^{(2)} - y^{(2)}​$$…​$一直到第 $m​$ 个 即计算 $dZ = [dz^{(1)} , dz^{(2)} , … , dz^{(m)}]$ 之前我们已经得到 $A = [a^{(1)}, a^{(2)}, … , a^{(m)}] = \sigma (Z)​$我们再定义输出标签 $Y = [y^{(1)}, y^{(2)}, … , y^{(m)}]​$ 那么， dZ = A-Y = [a^{(1)}-y^{(1)} , a^{(2)}-y^{(2)} , ... , a^{(m)}-y^{(m)}]有了 $dZ$ 我们就可以计算 $dw, db​$根据之前的公式，有 db = \frac{1}{m} \sum_{i=1}^{m}dz^{(i)} dw = \frac{1}{m}X \cdot dZ^{T} = \frac{1}{m} \left( \begin{array}{c} \vdots & \vdots & \ldots & \vdots \\\\ x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\ \vdots & \vdots & \ldots & \vdots \end{array} \right) \left( \begin{array}{c} dz^{(1)} \\\\ \vdots \\\\ dz^{(m)} \end{array} \right) = \frac{1}{m}[x^{(1)}dz^{(1)} + ...+ x^{(m)}dz^{(m)}]对应的 Python 代码即为12db = np.sum(dZ)/mdw = np.dot(x,dZ.T)/m 最后，我们可以得到向量化后的梯度下降算法1234567import numpy as npZ = np.dot(w.T,X) + bA = sigmoid(Z)dw = np.dot(x,dZ.T)/mdb = np.sum(dZ)/mw = w - alpha * dwb = b - alpha * db 你可能会有疑问，为什么这里不需要再计算 Cost function (代价函数) $J$ 了，笔者认为 $J$ 只是对数几率回归模型所需要的损失函数，借助它我们才能计算出 $dw, db$ ，从而进行迭代。在后续的作业中，计算 $J​$ 可以帮助我们对模型进一步分析。这样，我们就完成了对数几率回归的梯度下降的一次迭代，但如果你需要多次执行迭代的操作，只能显式的调用for循环。 BroadcastingBroadcasting (广播) 机制主要是为了方便不同 shape 的 array (可以理解为不同形状的矩阵) 进行数学运算 当我们将向量和一个常量做加减乘除操作时，比如对数几率回归中的 $w^{T}x+b$ ，会对向量中的每一格元素都和常量做一次操作，或者你可以理解为把这个数复制 $m \times n$ 次，使其变为一个形状相同的 $(m,n)$ 矩阵，如 : \begin{bmatrix}1 \\ 2 \\ 3 \\ 4\end{bmatrix} + 100 => \begin{bmatrix}1 \\ 2 \\ 3 \\ 4\end{bmatrix} + \begin{bmatrix}100 \\ 100 \\ 100 \\ 100\end{bmatrix} = \begin{bmatrix}101 \\ 102 \\ 103 \\ 104\end{bmatrix} 一个 $(m,n)$ 矩阵和一个 $(1,n)$ 矩阵相加 (减乘除)，会将这个 $(1,n)$ 矩阵复制 $m$ 次，使其变为 $(m,n)$ 矩阵然后相加，如 : \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix} 100 & 200 & 300\end{bmatrix} => \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix} 100 & 200 & 300 \\ 100 & 200 & 300\end{bmatrix} = \begin{bmatrix}101 & 202 & 303 \\ 104 & 205 & 306\end{bmatrix} 同样地，一个 $(m,n)$ 矩阵和一个 $(m,1)$ 矩阵相加 (减乘除)，会将这个 $(m,1)$ 矩阵复制 $n$ 次，使其变为 $(m,n)$ 矩阵然后相加 \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix}100 \\ 200\end{bmatrix} => \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix} 100 & 100 & 100 \\ 200 & 200 & 200\end{bmatrix} = \begin{bmatrix}101 & 102 & 103 \\ 204 & 205 & 206\end{bmatrix}通俗的讲，numpy 会通过复制的方法，使两个不同形状的矩阵变得一致，再执行相关操作。值得一提的是，为了保证运算按照我们的想法进行，使用 reshape() 函数是一个较好的习惯。 Homework附上所有代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276# LogisticRegression.pyimport numpy as npimport matplotlib.pyplot as pltimport h5pyimport scipyfrom PIL import Imagefrom scipy import ndimagefrom lr_utils import load_dataset# Sigmoid 函数def sigmoid(z): """ Compute the sigmoid of z Arguments: z -- A scalar or numpy array of any size. Return: s -- sigmoid(z) """ s = 1 / (1 + np.exp(-z)) return s# 初始化 w,bdef initialize_with_zeros(dim): """ This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0. Argument: dim -- size of the w vector we want (or number of parameters in this case) Returns: w -- initialized vector of shape (dim, 1) b -- initialized scalar (corresponds to the bias) """ w = np.zeros((dim, 1)) # (dim, 1) 是shape参数，代表初始化一个dim*1的矩阵 b = 0 assert (w.shape == (dim, 1)) assert (isinstance(b, float) or isinstance(b, int)) return w, b# propagate 正向与反向传播def propagate(w, b, X, Y): """ Implement the cost function and its gradient for the propagation explained above Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples) Return: cost -- negative log-likelihood cost for logistic regression dw -- gradient of the loss with respect to w, thus same shape as w db -- gradient of the loss with respect to b, thus same shape as b Tips: - Write your code step by step for the propagation. np.log(), np.dot() """ m = X.shape[1] # FORWARD PROPAGATION (FROM X TO COST) A = sigmoid(np.dot(w.T, X) + b) # compute activation cost = -1 / m * (np.dot(Y,np.log(A).T) + np.dot(1 - Y,np.log(1 - A).T)) # compute cost # BACKWARD PROPAGATION (TO FIND GRAD) dw = 1 / m * np.dot(X, (A - Y).T) db = 1 / m * np.sum(A - Y) assert (dw.shape == w.shape) assert (db.dtype == float) cost = np.squeeze(cost) # 删除shape为1的维度，比如cost=[[1]]，则经过np.squeeze处理后cost=[1] assert (cost.shape == ()) grads = &#123;"dw": dw, "db": db&#125; return grads, cost# 梯度下降def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False): """ This function optimizes w and b by running a gradient descent algorithm Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of shape (num_px * num_px * 3, number of examples) Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples) num_iterations -- number of iterations of the optimization loop learning_rate -- learning rate of the gradient descent update rule print_cost -- True to print the loss every 100 steps Returns: params -- dictionary containing the weights w and bias b grads -- dictionary containing the gradients of the weights and bias with respect to the cost function costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve. Tips: You basically need to write down two steps and iterate through them: 1) Calculate the cost and the gradient for the current parameters. Use propagate(). 2) Update the parameters using gradient descent rule for w and b. """ costs = [] for i in range(num_iterations): # Cost and gradient calculation grads, cost = propagate(w, b, X, Y) # Retrieve derivatives from grads dw = grads["dw"] db = grads["db"] # update rule w = w - dw * learning_rate b = b - db * learning_rate # Record the costs if i % 100 == 0: costs.append(cost) # Print the cost every 100 training examples if print_cost and i % 100 == 0: print("Cost after iteration %i: %f" % (i, cost)) params = &#123;"w": w, "b": b&#125; grads = &#123;"dw": dw, "db": db&#125; return params, grads, costs# 利用logistic regression判断Y的标签值def predict(w, b, X): ''' Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b) Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Returns: Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X ''' m = X.shape[1] Y_prediction = np.zeros((1, m)) w = w.reshape(X.shape[0], 1) # Compute vector "A" predicting the probabilities of a cat being present in the picture A = sigmoid(np.dot(w.T, X) + b) # A.shape = (1,m) for i in range(A.shape[1]): # Convert probabilities A[0,i] to actual predictions p[0,i] if A[0, i] &gt; 0.5: Y_prediction[0, i] = 1 else: Y_prediction[0, i] = 0 assert (Y_prediction.shape == (1, m)) return Y_prediction# 构建整个模型def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False): """ Builds the logistic regression model by calling the function you've implemented previously Arguments: X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train) Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train) X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test) Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test) num_iterations -- hyperparameter representing the number of iterations to optimize the parameters learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize() print_cost -- Set to true to print the cost every 100 iterations Returns: d -- dictionary containing information about the model. """ # initialize parameters with zeros w, b = initialize_with_zeros(X_train.shape[0]) # Gradient descent parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Retrieve parameters w and b from dictionary "parameters" w = parameters["w"] b = parameters["b"] # Predict test/train set examples Y_prediction_test = predict(w, b, X_test) Y_prediction_train = predict(w, b, X_train) # Print train/test Errors print("train accuracy: &#123;&#125; %".format( 100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100)) print("test accuracy: &#123;&#125; %".format( 100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100)) d = &#123; "costs": costs, "Y_prediction_test": Y_prediction_test, "Y_prediction_train": Y_prediction_train, "w": w, "b": b, "learning_rate": learning_rate, "num_iterations": num_iterations &#125; return ddef main(): train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset( ) m_train = train_set_x_orig.shape[0] m_test = test_set_x_orig.shape[0] num_px = train_set_x_orig.shape[2] train_set_x_flatten = train_set_x_orig.reshape(m_train, -1).T test_set_x_flatten = test_set_x_orig.reshape(m_test, -1).T train_set_x = train_set_x_flatten / 255. test_set_x = test_set_x_flatten / 255. # train model d = model( train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True) # 判断单张图片是否有猫 my_image = "a.jpg" # change this to the name of your image file # We preprocess the image to fit your algorithm. fname = "images/" + my_image image = np.array(ndimage.imread(fname, flatten=False)) my_image = scipy.misc.imresize(image, size=(num_px, num_px)).reshape((1, num_px * num_px * 3)).T my_predicted_image = predict(d["w"], d["b"], my_image) print("y = " + str(np.squeeze(my_predicted_image)) + ", your algorithm predicts a \"" + classes[ int(np.squeeze(my_predicted_image)),].decode("utf-8") + "\" picture.") plt.imshow(image) plt.show()main()]]></content>
      <categories>
        <category>deeplearning.ai</category>
        <category>Deep learning &amp; NN</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Basic of Neural Networks-1]]></title>
    <url>%2F2019%2F01%2F21%2FBasic-of-Neural-Networks-1%2F</url>
    <content type="text"><![CDATA[本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第二周神经网络基础的相关课程。 Binary Classification在二分分类问题中，目标是训练出一个分类器，以特征向量x (feature vector)为输入，以y (output label)为输出，y一般只有 ${0,1}​$ 两个离散值。以图像识别问题为例，判断图片中是否由猫存在，0代表noncat，1代表cat 通常，我们用 $(x,y)​$ 来表示一个单独的样本，其中x(feature vector)是$n_x​$维的向量 ( $n_x​$ 为样本特征个数，即决定输出的因素) ，y(output label)为输出，取值为 $y\in\{0,1\}​$则m个训练样本 (training example) 可表示为 \{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\}用$ m=m_{train} $表示训练样本的个数 最后，我们可以用更紧凑的符号 $X$ 表示整个训练集，$X$ 由训练集中的 $x^{(1)}$，$x^{(2)}$，…，$x^{(m)}$ 作为列向量组成，$X\in{\Bbb R}^{n_x*m}$，即 X.shape = $(n_x,m)$ \mathbf{X} = \left( \begin{array}{c} \vdots & \vdots & \ldots & \vdots \\\\ x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\ \vdots & \vdots & \ldots & \vdots \end{array} \right)同时，把y也放入列中，用 $Y$ 来表示，$Y\in{\Bbb R}^{1*m}$，即 Y.shape = $(1,m)​$ \mathbf{Y} = \left( \begin{array}{c} y^{(1)} & y^{(2)} & \ldots & y^{(m)} \ \end{array} \right)Logistic Regression参照了周志华的西瓜书，把 Logisitic Regression 翻译为对数纪律回归，简称为对率回归。对数几率回归是一种解决二分分类问题的机器学习方法，用于预测某种实物的可能性。 Given x, you want $\hat{y} = P(y=1 \mid x)​$. In other words, if x is a picture, as talked about above, you want $\hat{y}​$ to tell you the chance that there is a cat in the picture. 根据输入 $x$ 和参数 $w, b$，计算出 $\hat{y}$ ，下面介绍了两种方法 : Parameter : $w\in{\Bbb R}^{n_x}, b\in{\Bbb R}​$Output : $\hat{y}​$ One way : $\hat{y} = w^{T}x + b​$ (Linear regression) Not good for binary classification Because you want $\hat{y}​$ to be the chance that $y​$ equals to one. In this situation $\hat{y}​$ can be much bigger than 1 or negative. The other way : $\hat{y} = \sigma(w^{T}x + b)$ (Logistic Regression) $\sigma(z) = \frac{1}{1+e^{-z}} ​$ 通过$\sigma(z)$函数，可以将输出限定在$[0,1]$之间 Logistic Regression Cost Function给出$\{(x^{(1)},y^{(1)})…,(x^{(m)},y^{(m)})\}​$，希望通过训练集，找到参数 $w, b​$ 使得 $\hat{y}^{(i)} \approx y^{(i)}​$ 。所以，我们需要定义一个loss function，通过这个loss function来衡量你的预测输出值 $\hat{y}​$ 与 $y​$ 的实际值由多接近 对于m个训练样本，我们通常用上标 $(i)​$ 来指明数据与第 $i​$ 个样本有关。 通常，我们这样定义Loss function (损失函数) : L(\hat{y},y) = \frac{1}{2}(\hat{y} - y)^2但在对数几率回归中一般不使用，因为它是non-convex (非凸的) ，将来使用梯度下降算法 (Gradient Descent)时无法找到全局最优值 在对数几率回归中，我们使用的损失函数为 : L(\hat{y},y) = -(y \log\hat{y} + (1-y) \log(1-\hat{y})) If y = 1 : $L(\hat{y},y) = -\log(\hat{y})​$, you want $\hat{y}​$ to be large if y = 0 : $L(\hat{y},y) = -\log(1-\hat{y})​$, you want $\hat{y}​$ to be small 所以，这个损失函数和 $L(\hat{y},y) = \frac{1}{2}(\hat{y} - y)^2$ 类似，都希望 $L$ 越小越好 上述的Loss function衡量了单个训练样本的表现，对于m个样本，我们定义Cost function (代价函数) ，它衡量了全体训练样本的表现 J(w,b) = \frac{1}{m} \sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)}) = -\frac{1}{m} \sum_{i=1}^{m}y^{(i)} \log\hat{y}^{(i)} + (1-y^{(i)}) \log(1-\hat{y}^{(i)})Loss function只适用于单个训练样本，Cost function是基于参数的总代价。所以，在训练对数几率回归模型时，我们要找到合适的参数 $w, b$ 使得Cost function尽可能的小 Gradient Descent我们将使用梯度下降 (Gradient Descent) 算法来找出合适的参数 $w,b$，使得Cost function 即 $J(w,b)$ 最小 最上方的小红点为初始点，对于对数几率回归，一般使用0来初始化，随机初始化也有效，但通常不这么做 梯度下降过程： 从初始点开始，朝最陡的下坡方向走一步 重复上述过程，不断修正 $w, b​$ 使得 $J(w,b)​$ 接近全局最优值 (global opitmal) 代码表述为： Repeat {$w := w - \alpha \frac{\partial J(w,b)}{\partial w}​$ &nbsp; &nbsp;&nbsp;&nbsp;在代码中 $\frac{\partial J(w,b)}{\partial w}​$ 记作”dw”$b := b - \alpha \frac{\partial J(w,b)}{\partial b}​$ &nbsp; &nbsp;&nbsp;&nbsp;在代码中 $\frac{\partial J(w,b)}{\partial b}​$ 记作”db”} Computation Graph神经网络的训练包含了两个过程： 正向传播 (Forward Propagation)，从输入经过一层层神经网络，最后得到 $\hat{y}$ ，从而计算代价函数 $J$ 反向传播 (Back Propagation)，根据损失函数 $L(\hat{y},y)$ 来反方向的计算每一层参数的偏导数，从而更新参数 下面我们用计算图 (Computation Graph) 来理解这个过程 从左向右，可以计算出 $J​$ 的值，对应着神经网络中输入经过计算得到代价函数 $J(w,b)​$ 值的过程 从右向左，根据求导的链式法则，可以得到： \frac{\partial J}{\partial a} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial a} = 3 \cdot 1 = 3 \frac{\partial J}{\partial b} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial u} \cdot \frac{\partial u}{\partial b} = 3 \cdot 1 \cdot c = 6 \frac{\partial J}{\partial c} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial u} \cdot \frac{\partial u}{\partial c} = 3 \cdot 1 \cdot b = 9在反向传播中，一般我们只关心最终输出值 (在这个例子中是 $J$ ) ，需要计算 $J$ 对于某个变量 (记作var) 的导数，即 $\frac {dJ}{dvar}​$，在Python代码中简写为dvar Logistic Regression Gradient Descent现在，我们来实现对数几率回归梯度下降算法，只考虑单个样本的情况 : $z = w^{T}x + b$ $\hat{y} = a = \sigma({z})​$ $L(\hat{y},y) = -(y \log\hat{y} + (1-y) \log(1-\hat{y}))​$ 假设样本只有两个，分别为 $x1, x2$，则计算图如下 : 在对数几率回归中，我们需要做的是，改变参数 $w, b$ 的值，来最小化损失函数，即需要计算出 $dw, dz$ 向后传播计算损失函数 $L​$ 的偏导数步骤如下： $da = \frac {\partial L(a,y)}{\partial a} = -\frac {y}{a} + \frac{1-y}{1-a}​$ $dz = \frac {\partial L}{\partial z} = \frac {\partial L}{\partial a} \cdot \frac {da}{dz}= (-\frac {y}{a} + \frac{1-y}{1-a}) \cdot a \cdot (1-a) = a - y​$ $dw_1 = \frac {\partial L}{\partial w_1} = \frac {\partial L}{\partial z} \cdot \frac { \partial z}{\partial w_1} = x_1 \cdot dz ​$ $dw_2 = \frac {\partial L}{\partial w_2} = \frac {\partial L}{\partial z} \cdot \frac { \partial z}{\partial w_2} = x_2 \cdot dz ​$ $db = \frac {\partial L}{\partial b} = \frac {\partial L}{\partial z} \cdot \frac { \partial z}{\partial b} = dz​$ 所以，在对数几率回归梯度下降算法中你需要做的是 $ dz = a - y$ $dw_1 = x_1 \cdot dz ​$ $dw_2 = x_2 \cdot dz ​$ $db = dz​$ 更新$w_1​$, $w_1 = w_1 - \alpha dw_1​$ 更新$w_2$, $w_2 = w_2 - \alpha dw_2$ 更新$b​$, $b = b - \alpha db​$ Gradient descent on $m$ examples之前只实现了单个样本的梯度下降算法，现在我们将梯度下降算法应用到整个训练集 $J(w,b) = \frac{1}{m} \sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)}) $ $a^{(i)} = \hat{y}^{(i)} = \sigma(z^{(i)}) = \sigma(w^{T}x^{(i)}+b)​$ $\frac {\partial}{\partial w_1}J(w,b) = \frac {1}{m} \sum_{i=1}^{m} \frac {\partial}{\partial w_1}L(a^{(i)},y^{(i)}) = \frac {1}{m} \sum_{i=1}^{m}dw_1^{(i)}$ $dw_1^{(i)}​$按照之前单个样本的情况计算 伪代码如下 : $J=0; dw_1=0; dw_2=0; db=0;$$for \quad i = 1 \quad to \quad m $$\quad z^{(i)} = w^{T}x^{(i)}+b$$\quad a^{(i)} = \sigma(z^{(i)})$$\quad J += -(y^{(i)} \log a^{(i)} + (1-y^{(i)}) \log(1-a^{(i)}))$$\quad dz^{(i)} = a^{(i)}-y^{(i)}$$\quad dw_1 += x_1^{(i)}dz^{(i)}$$\quad dw_2 += x_2^{(i)}dz^{(i)} \qquad$$\quad…\quad\quad\quad\quad\quad\quad$ //这里应该是一个循环，这里 $n_x = 2$$\quad db += dz^{(i)}$​$J /= m$$dw_1 /= m$$dw_2 /= m$$db /= m$ $w_1 = w_1 - \alpha dw_1​$$w_2 = w_2 - \alpha dw_2​$$b = b - \alpha db​$ 但这种方法，有两个循环，一个是最外层的循环，循环 $m$ 个训练样本，另一个是 $dw_1, dw_2$ (feature) 的循环，在这个例子中 $n_x = 2$。随着训练集越来越大，应该尽量避免使用for循环，而使用向量化技术 (vectorization)]]></content>
      <categories>
        <category>deeplearning.ai</category>
        <category>Deep learning &amp; NN</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java常量池]]></title>
    <url>%2F2019%2F01%2F17%2FJava-Constant-Pool%2F</url>
    <content type="text"><![CDATA[常量池 相同的值只存储一份，节省内存，共享访问，提高运行效率 基本类型的包装类 Boolean Byte Short Integer Long Character Float Double 八种基本类型的包装类 常量值范围 Boolean：true, false Byte Character : \u0000 - \u007f Short Integer Long : -128 - 127 Float Double : 无常量池 ==与equals() 对于基本数据类型，==比较他们的数值 对于对象，==比较两个对象在内存中的存放地址，可以通过重写equals()来比较两个对象的内容是否相等 字符串常量 Java为常量字符串建立了常量池缓存机制123456String s1 = "abc";String s2 = "ab" + "c";String s3 = "a" + "b" + "c"; //都是常量，是确定的，编译器将优化System.out.println(s1==s2); //trueSystem.out.println(s1==s3); //trueSystem.out.println(s2==s3); //true 基本类型的包装类和字符串的两种创建方式 字面值赋值，放在栈内存（将被常量化） Integer a = 1; String b = &quot;abc&quot;; new对象进行创建，放在堆内存（不会常量化） Integer c = new Integer(1); String d = new String(&quot;abc&quot;); 栈内存读取速度快，容量小 堆内存读取速度慢，容量大，可以通俗的理解为Java认为new出来的对象所占内存较大（不确定，而字面值是确定的），所以需要放在堆内存 Integer常量池的例子12345678910111213141516171819int i1 = 10;Integer i2 = 10; //自动装箱，10本来只是int，是基本类型，而我们需要把它变成一个对象，相当于包装了一层System.out.println(i1==i2) //true//自动拆箱 基本类型和包装类进行比较，包装类自动拆箱 Integer i3 = new Integer(10);System.out.println(i1==i3) //true 同理，包装类自动拆箱System.out.println(i2==i3) //false i2,i3都是对象，而i2是常量，在常量池，i3是new出来的对象，在堆内存中 Integer i4 = new Integer(5);Integer i5 = new Integer(5);System.out.println(i1 == (i4+i5)); //trueSystem.out.println(i1 == (i4+i5)); //trueSystem.out.println(i1 == (i4+i5)); //true//i4+i5的操作将会使i4,i5自动拆箱为基本类型并运算得到10，而根据之前所提到的，基本类型和包装类进行比较，包装类自动拆箱，所以都为trueInteger i6 = i4 + i5;System.out.println(i1==i6); //true，同理i4+i5的操作使i4,i5自动拆箱，得到10，相当于Integer i6 = 10;System.out.println(i3==i6); //false String常量池的例子字符串常量池存在于方法区，方法区包含的都是在整个程序中唯一的元素，如static变量 一个简单的例子 1234567String s1 = "abc";String s2 = "abc";String s3 = new String("abc");String s4 = new String("abc");System.out.println(s1==s2); //true 都是常量池System.out.println(s1==s3); //false 一个是栈内存，一个是堆内存System.out.println(s3==s4); //false 都是堆内存，但是不同对象 图解：(&quot;由&#39;代替) graph LR; subgraph 方法区 s['abc'] end subgraph 堆 A["s3 = new String('abc')"] B["s4 = new String('abc')"] end subgraph 栈 s1 s2 s3 s4 end s1-->s s2-->s A-->s B-->s s3-->A s4-->B 更为复杂的例子123456789101112String s5 = "abcdef";String s6 = s1 + "def"; //涉及到变量（不确定的），编译器不会优化String s7 = "abc" + "def"; //都是常量，编译器会优化成abcdefString s8 = "abc" + new String("def"); //涉及到new对象，编译器不优化System.out.println(s6==s7); //falseSystem.out.println(s6==s8); //falseSystem.out.println(s7==s8); //falseSystem.out.println(s5==s7); //trueString s9 = s3 + "def"; //由于s3是new的，涉及到new对象，编译器不优化System.out.println(s7==s9); //false//对于s5~s9，只有s5,s7是在常量池中，其余都在堆内存上，且地址互不相同]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim Tutorial]]></title>
    <url>%2F2019%2F01%2F14%2FVim-Tutorial%2F</url>
    <content type="text"><![CDATA[This tutorial includes some basic vim commands and I hope that it will be helpful. Moving the cursor h : left j : down k : up l : right It takes time to get used to it. Navigation w: move the cursor one word forward (to the first letter) b : one word backward (also to the first letter) e : one word forward (to the last letter) fx : forward to the letter x ( : to the start of the sentence ) : start of the sentence 0 : start of line $ : end of line { : start of paragraph } : end of paragraph G : end of file ctrl+G : to see the cursor location and file status gg : start of file xG : to the number x line of file typing a number before a motion repeats it that many times! Delete x: delete the character at the cursor dw: delete all the characters between the cursor and the first letter of the next word e.g. Please delete the word. (Assume the cursor is at l) After you press dw, the sentence becomes Please dethe word delete de: delete all the characters between the cursor and the next space e.g. Please delete the word. (Assume the cursor is at l) After you press de, the sentence becomes Please de the word delete d$ : delete to end of line dd : delete whole line p : After you delete something, press p to paste things you delete wherever you like. Insert a : insert after the cursor A : insert after the end of line i : insert before the cursor I : insert before the start of line o : insert in the next line O : insert in the previous line Search /yourSearchString + &lt;Enter&gt; : search for yourSearchString n : to search for the same string again (press &lt;Enter&gt; to exit) N : to search for the same string again, but in opposite direction ctrl+o : to go back to where you came from ctrl+i : to go forward set option :set ic : ignore case :set noic : disable ignore case :set hls : highlight the matches :set nohls : disable highlight matches :set is : increase search :set nois: disable increase search % : move the cursor to the other matching parenthesis Replace rx : replace the character at cursor with x ce : almost the same as de, but this time will place you in Insert Mode s/old/new : replace the first occurrence of ‘old’ with ‘new’ s/old/new/g : replace all occurrence of ‘old’ with ‘new’ in one line #,#/old/new/g : #,# are the line numbers of the range of lines where the replace should be done %s/old/new/g : replace all occurrence of ‘old’ with ‘new’ in the whole file %s/old/new/g : replace all occurrence of ‘old’ with ‘new’ in the whole file, with a prompt whether to replace or not Undo &amp; Redo u : undo the last command U : undo the command excuting on the while line ctrl+R : redo the command Copy &amp; Paste y : to copy p : to paste e.g. Start Visual Mode with v and move the cursor to chose whatever you want, type y to copy the highlighted text and type p to paste the text. Others . : repeat the last command &lt;start position&gt;&lt;command&gt;&lt;end position&gt; : many commands follow this pattern e.g. 0y$ means copy the whole line 0 move the cursor to the start of line y copy $ move the cursor to the end of line ctrl+n : auto complete]]></content>
      <categories>
        <category>vim</category>
      </categories>
      <tags>
        <tag>vim</tag>
      </tags>
  </entry>
</search>
