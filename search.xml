<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Shallow Neural Networks</title>
      <link href="/2019/02/01/Deep-Learning-Week-Three/"/>
      <url>/2019/02/01/Deep-Learning-Week-Three/</url>
      
        <content type="html"><![CDATA[<p>本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第三周浅层次神经网络的相关课程。</p><h3 id="Neural-Network-Overview"><a href="#Neural-Network-Overview" class="headerlink" title="Neural Network Overview"></a><strong>Neural Network Overview</strong></h3><p>本周，你将学会如何实现神经网络。上周，我们讨论了对数几率回归 (logistic regression) ，并且使用计算图 (computation graph) 的方式了解了梯度下降算法的正向传播和反向传播的两个过程，如下图所示 :<br></p><a id="more"></a><p><img src="/2019/02/01/Deep-Learning-Week-Three/1.png" alt=""></p><p>而神经网络 (Neural Network) 是这个样子，如下图 :</p><p><img src="/2019/02/01/Deep-Learning-Week-Three/2.png" alt=""></p><p>我们可以把很多 sigmoid 单元堆叠起来来构成一个神经网络。在之前所学的对数几率回归中，每一个节点对应着两个计算步骤：首先计算 $z=w^{T}x + b​$ ，然后计算 $a=\sigma(z)​$ 。在这个神经网络中，三个竖排堆叠的节点就对应着这两部分的计算，那个单独的节点也对应着另一个类似的 $z, a​$ 的计算。 <br></p><p>在神经网络中，所用的符号也会有些不一样。我们还是用 $x$ 来表示输入特征，用 $W^{[1]}, b^{[1]}$ 来表示参数，这样你就可以计算出 $z^{[1]} = W^{[1]}x + b^{[1]}$ 。这里右上角的 $[1]$ 代表着节点所属的层，你可以认为层数从 $0$ 开始算起，如上图中的 $x_1, x_2, x_3$ 就代表着第 $0$ 层（也称为输入层），三个竖排的节点就属于第 $1$ 层（也称为隐藏层），单独的那个节点属于第 $2$ 层（也称为输出层）。需要注意的是，这与之前用来标注第 $i$ 个训练样本 $(x^{(i)}, y^{(i)})$ 不同，这里用的是方括号。<br></p><p>那么，在这个神经网络模型中，正向传播就分为两层。</p><ul><li>从输入层到隐藏层：在使用类似对数几率回归的方法计算了 $z^{[1]}$ 之后，再计算 $a^{[1]}=\sigma(z^{[1]})​$ 。</li><li>从隐藏层到输出层：使用相同的方法计算 $z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$ 。当然，这里的参数  $W^{[2]}, b^{[2]}$ 与 $W^{[1]}, b^{[1]}$ 不同，且第 $1$ 层的输出 $a^{[1]}$ 作为第 $2$ 层的输入 $x$ ，接着同样计算 $a^{[2]}=\sigma(z^{[2]})$ ，得到的 $a^{[2]}​$ 就是整个神经网络的输出。</li></ul><p>同样，还需要通过反向传播计算 $da^{[2]}, dz^{[2]}$ 等等，这些将会在后面详细讨论。</p><h3 id="One-hidden-layer-Neural-Network"><a href="#One-hidden-layer-Neural-Network" class="headerlink" title="One hidden layer Neural Network"></a><strong>One hidden layer Neural Network</strong></h3><p>下图是一张单隐藏层的神经网络，也称为双层神经网络 (2 layer NN) 。我们把最左边的 $x1, x2, x3$ 称为输入层 （Input Layer) ，中间称为隐藏层 (Hidden Layer) ，最右边只有一个节点的称为输出层 (Output Layer) ，负责输出预测值 $\hat{y}$ 。在计算神经网络的层数时，不算入输入层。</p><p><img src="/2019/02/01/Deep-Learning-Week-Three/3.png" alt=""></p><p>由于在训练过程中，我们看不到这些中间节点的真正数值，不像输入，输出层那样，所以称为隐藏层。</p><p>之前，我们用 $x$ 来表示输入，其实它还有一种表示方式 $a^{[0]}$ ，这个 $a$ 有 activation (激活) 的意思，意味这它把不同层的值传递给下一层，起到了激活的作用。用上标 $[i]$ 表示在第 $i$ 层，用下标 $j$ 表示这层中第 $j$ 个节点，如 $a^{[1]}_{2}$ 即表示第 $1$ 层的第 $2$ 个节点。那么上图中隐藏层的4个节点可以写成矩阵的形式：</p><script type="math/tex; mode=display">a^{[1]} =\left( \begin{array}{c}a^{[1]}_{1} \\\a^{[1]}_{2} \\\a^{[1]}_{3} \\\a^{[1]}_{4}\end{array} \right)</script><h3 id="Computing-a-Neural-Network’s-Output"><a href="#Computing-a-Neural-Network’s-Output" class="headerlink" title="Computing a Neural Network’s Output"></a><strong>Computing a Neural Network’s Output</strong></h3><p>接下来，我们来看神经网络的输出是如何计算出来的。我们可以把神经网络的计算看作对数几率回归的多次重复计算。</p><p>我们先来回顾一下对数几率回归的计算过程，如下图：</p><p><img src="/2019/02/01/Deep-Learning-Week-Three/4.png" alt=""></p><p>这里的圆圈代表了，对数几率回归的两个步骤。我们先隐去其他节点，如右图，那么它就和对数几率回归非常相似，我们可以计算出 $z^{[1]}_{1} = w^{[1]T}_{1}x+b^{[1]}_{1}$ ， $a^{[1]}_{1} = \sigma(z^{[1]}_{1})$ ，上标代表层数，下标表示这层上的第几个节点。</p><p>以此类推，我们可以写出：</p><p><img src="/2019/02/01/Deep-Learning-Week-Three/5.png" alt=""></p><p>回想起之前所讲的向量化，如果我们想让程序高效的运行，就必须将其向量化。</p><p>我们首先先将 $w$ 向量化，由于有4个对数几率回归单元，而每一个回归单元都有其对应的参数向量 $w$ ，且每一个回归单元都有输入 $x_1, x_2, x_3​$ ，所以我们可以得到：</p><script type="math/tex; mode=display">W^{[1]} =\left( \begin{array}{c}w^{[1]T}_{1} \\\w^{[1]T}_{2} \\\w^{[1]T}_{3} \\\w^{[1]T}_{4}\end{array} \right)_{(4 \times 3)}</script><p>那么，我们可以得到如下式子：</p><script type="math/tex; mode=display">Z^{[1]} =\left( \begin{array}{c}w^{[1]T}_{1} \\\w^{[1]T}_{2} \\\w^{[1]T}_{3} \\\w^{[1]T}_{4}\end{array} \right)\cdot\left( \begin{array}{c}x_1 \\\x_2 \\\x_3\end{array} \right)+\left( \begin{array}{c}b^{[1]}_{1} \\\b^{[1]}_{2} \\\b^{[1]}_{3} \\\b^{[1]}_{4}\end{array} \right)=\left( \begin{array}{c}w^{[1]T}_{1}x+b^{[1]}_{1} \\\w^{[1]T}_{2}x+b^{[1]}_{2} \\\w^{[1]T}_{3}x+b^{[1]}_{3} \\\w^{[1]T}_{4}x+b^{[1]}_{4}\end{array} \right)=\left( \begin{array}{c}z^{[1]}_{1} \\\z^{[1]}_{2} \\\z^{[1]}_{3} \\\z^{[1]}_{4}\end{array} \right)</script><script type="math/tex; mode=display">a^{[1]} =\left( \begin{array}{c}a^{[1]}_{1} \\\a^{[1]}_{2} \\\a^{[1]}_{3} \\\a^{[1]}_{4}\end{array} \right)=\sigma(Z^{[1]})=\sigma(W^{[1]}x + b^{[1]})</script><p>在本神经网络中，你就应该计算 (为了更好理解，右下角标注了矩阵的形状) ：</p><script type="math/tex; mode=display">z^{[1]}_{(4 \times 1)} = W^{[1]}_{(4 \times 3)}x_{(3 \times 1)}+ b^{[1]}_{(4 \times 1)}</script><script type="math/tex; mode=display">a^{[1]}_{4 \times 1} = \sigma(z^{[1]}_{(4 \times 1)})</script><p>记得我们之前说过，可以用 $a^{[0]}​$表示 $x​$ ，所以 $z^{[1]}​$ 有可以写成 $z^{[1]} = W^{[1]}a^{[0]}+ b^{[1]}​$ ，那么可以用同样的方法推导出：</p><script type="math/tex; mode=display">z^{[2]}_{(1 \times 1)} = W^{[2]}_{(1 \times 4)}a^{[1]}_{(4 \times 1)}+ b^{[2]}_{(1 \times 1)}</script><script type="math/tex; mode=display">a^{[2]}_{1 \times 1} = \sigma(z^{[2]}_{(1 \times 1)})</script><p>所以在代码中，我们只需实现上述的4行代码。然而这是对单个样本的，即对于输入的特征向量 $x$ ，可以计算出 $\hat{y} = a^{[2]}​$ ，对于整个训练集的向量化将在接下来的部分介绍。</p><h3 id="Vectorizing-across-multiple-examples"><a href="#Vectorizing-across-multiple-examples" class="headerlink" title="Vectorizing across multiple examples"></a><strong>Vectorizing across multiple examples</strong></h3><p>接下来，我们实现对整个训练集的向量化。假设你有 $m​$ 个训练样本，理解了上述理论，那么我们可以通过输入 $x^{(i)}​$ 计算得出 $ \hat{y}^{(i)} = a^{[2](i)}​$ 其中 $i \in [1,m]​$ 。</p><p>还记得我们之前把输入的特征向量 $x^{(1)}, x^{(2)}, …, x^{(m)}​$ 横向堆叠起来，得到了一个 $(n_x \times m)​$ 的矩阵 $X​$ ，即</p><script type="math/tex; mode=display">\mathbf{X} =\left( \begin{array}{c}\vdots & \vdots & \ldots & \vdots \\\\x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\\vdots & \vdots & \ldots & \vdots\end{array} \right)_{(n_x \times m)}</script><p>同样的，我们把 $x^{(1)}, x^{(2)}, …, x^{(m)}​$ 横向堆叠起来，那么我们需要计算的式子变为：</p><script type="math/tex; mode=display">Z^{[1]} = W^{[1]}X + b^{[1]}</script><script type="math/tex; mode=display">A^{[1]} = \sigma(Z^{[1]})</script><script type="math/tex; mode=display">Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}</script><script type="math/tex; mode=display">A^{[2]} = \sigma(Z^{[2]})</script><p>其中，$Z^{[1]}​$ 的形状为 $(4 \times m)​$ ，你可以这样理解，每一个样本对应着矩阵的一列，所以有 $m​$ 列，隐藏层中的每一个节点对应着矩阵的一行，所以 $Z^{[1]}​$ 有 $4​$ 行，规律同样适用于 $X, A​$。</p><h3 id="Activation-functions"><a href="#Activation-functions" class="headerlink" title="Activation functions"></a><strong>Activation functions</strong></h3><p>到目前为止，我们一直选择 sigmoid 函数作为 activation function (激活函数) ，但有时使用其他函数效果要好得多，它们各自有不同的特点，下面我们来介绍几个不同的激活函数 $g(x)$：</p><p><img src="/2019/02/01/Deep-Learning-Week-Three/6.png" alt=""></p><ul><li>tanh (双曲正切函数)，实际上是 sigmoid 函数向下平移后再经过拉伸得到的。对于隐藏单元，如果你选择 tanh 作为激活函数，它的表现几乎总是比 sigmoid 函数要好，因为 tanh 函数的输出介于 $(-1,1)$ 之间，激活函数的平均值更接近于 $0$ ，而不是 $0.5$ ，这让下一层的学习更方便一点。所以之后我们几乎不再使用 sigmoid 作为激活函数了，但有一个例外，即选择输出层的激活函数的时候，因为二分类问题的输出为 $\{0,1\}$ ，你更希望 $\hat{y}$ 介于 $0,1$ 之间，所以一般会选择 sigmoid 函数。</li><li>所以之前所举的例子中，你可以使用 tanh 函数作为隐藏层的激活函数，而选择 sigmoid 函数作为输出层的激活函数。同样的，可以使用上标来表示每一层的激活函数，如：$g^{[1]}(x) = tanh(z), g^{[2]} = \sigma(z)​$ </li><li>然而，不管是 tanh 还是 sigmoid 函数，都有一个缺点，如果 $z$ 特别大或者特别小，那么在这一点的函数导数会很小，因此会拖慢梯度下降算法。为了弥补这个缺点，就出现了 ReLU (rectified linear unit) 函数，该函数有如下特点：当 $z$ 为正，导数为 $1$，当 $z$ 为负，导数为 $0$ ，当 $z$ 为 $0$ 时，导数不存在，但在实际使用中， $z$ 几乎不会等于 $0$ ，当然你可以在程序中直接把在 $z=0$ 点的导数赋为 $0$ 或 $1$ 。</li><li>但 ReLU 也有一个缺点，当 $z$ 为负时，导数恒等于 $0$ 。所以就有了 Leaky ReLU 函数，通常表现比 ReLU 函数更好，但实际使用中频率没那么高。</li><li>ReLU 和 Leaky ReLU 的优势在于，对于 $z$ 的许多取值，激活函数的导数和 $0$ 差的很远，这也就意味着，在实践中你的神经网络的学习速度会快很多。</li><li>总结一下，在我们一般选择 sigmoid 作为输出层的激活函数，而选择 ReLU 作为其他层的激活函数，ReLU 如今被人们广泛使用。</li></ul>]]></content>
      
      
      <categories>
          
          <category> deeplearning.ai </category>
          
          <category> Deep learning &amp; NN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Basic of Neural Networks-2</title>
      <link href="/2019/01/26/Deep-Learning-Week-Two/"/>
      <url>/2019/01/26/Deep-Learning-Week-Two/</url>
      
        <content type="html"><![CDATA[<p>本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第二周神经网络基础相关课程及第二周作业。</p><h3 id="Vectorization-向量化"><a href="#Vectorization-向量化" class="headerlink" title="Vectorization (向量化)"></a><strong>Vectorization (向量化)</strong></h3><p>Vectorization (向量化) 的意义在于：消除代码中显式的调用for循环。在深度学习领域中，你常常需要训练大数据集，所以程序运行的效率非常重要，否则需要等待很长时间才能得到结果。<br></p><a id="more"></a><p>在对数几率回归中，你需要计算 $z = w^{T}x + b$ ，其中 $w, x$ 都是 $n_x$ 维向量</p><blockquote><p>如果你是用非向量化的实现，即传统的矩阵相乘算法伪代码如下：<br>$ z = 0 $<br>$ for  \quad i  \quad in  \quad range(n_x) : $<br>$ \quad z+= w[i] * x[i] $<br>$z+=b ​$</p><p>若使用向量化的实现，Python代码如下：<br><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z = np.dot(w,x) + b</span><br></pre></td></tr></table></figure></p><p>即清晰又高效</p></blockquote><p>可以测试一下这两个代码在效率方面的差距，大约差了300倍。可以试想一下，如果你的代码1分钟出结果，和5个小时才出结果，那可差太远了。<br></p><p>为了加快深度学习的运算速度，可以使用GPU (Graphic Processing Unit) 。事实上，GPU 和 CPU 都有并行指令 (Parallelization Instructions) ，同时也叫作 SIMD (Single Instruction Multiple Data)，即单指令流多数据流，是一种采用一个控制器来控制多个处理器，同时对一组数据 (又称“数据向量”) 中的每一个分别执行相同的操作从而实现空间上的并行性的技术。numpy 是 Python 数据分析及科学计算的基础库，它有许多内置 (Built-in) 函数，主要用于数组的计算，充分利用了并行化，使得运算速度大大提高。在深度学习的领域，一般来说，能不用显式的调用for循环就不用。<br></p><p>这样，我们就可以使用 Vectorization 来优化梯度下降算法，先去掉内层对 feature (特征 $w1, w2 …$) 的循环 ：</p><blockquote><p>$J=0; db=0; dw = np.zeros(n_x,1)$<br>$for \quad i = 1 \quad to \quad m $<br>$\quad z^{(i)} = w^{T}x^{(i)}+b$<br>$\quad a^{(i)} = \sigma(z^{(i)})$<br>$\quad J += -(y^{(i)} \log a^{(i)} + (1-y^{(i)}) \log(1-a^{(i)}))$<br>$\quad dz^{(i)} = a^{(i)}-y^{(i)}$<br>$\quad dw+=x^{(i)}dz^{(i)} \quad \quad $ //vectorization<br>$\quad db  += dz^{(i)}$<br>$J /= m$<br>$dw /= m$<br>$db /= m​$</p></blockquote><p>然后，我们再去掉对 $m$ 个训练样本的外层循环，分别从正向传播和反向传播两方面来分析：</p><blockquote><h5 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a><strong>正向传播</strong></h5><p>回顾一下对数几率回归的正向传播步骤，如果你有 $m$ 个训练样本</p><p>那么对第一个样本进行预测，你需要计算<br>$ \quad z^{(1)} = w^{T}x^{(1)} + b$<br>$ \quad a^{(1)} = \sigma(z^{(1)})​$</p><p>然后继续对第二个样本进行预测<br>$ \quad z^{(2)} = w^{T}x^{(2)} + b$<br>$ \quad a^{(2)} = \sigma(z^{(2)})$</p><p>然后继续对第三个，第四个，…，直到第 $m$ 个</p><p>回忆一下之前在二分分类部分所讲到的用更紧凑的符号 $X​$ 表示整个训练集，即大小为 $(n_x,m)​$ 的矩阵 : </p><script type="math/tex; mode=display">\mathbf{X} =\left( \begin{array}{c}\vdots & \vdots & \ldots & \vdots \\\\x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\\vdots & \vdots & \ldots & \vdots\end{array} \right)</script><p>那么计算 $z^{(1)}, z^{(2)}, … , z^{(m)}$ 的步骤如下 :<br>首先先构造一个 $(1,m)$ 的矩阵 $[z^{(1)}, z^{(2)}, … , z^{(m)}]$ ，则 </p><script type="math/tex; mode=display">Z = [z^{(1)}, z^{(2)}, ... , z^{(m)}] = w^{T}X + [b, b , ... , b] = [w^{T}x^{(1)} + b, w^{T}x^{(2)} + b] , ... , w^{T}x^{(m)} + b]</script><p>在 Python 中一句代码即可完成上述过程<br><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T,X) + b</span><br></pre></td></tr></table></figure></p><p>你可能会有疑问，明明这里的 $b$ 只是一个实数 (或者说是一个 $b_{(1,1)}$ 的矩阵) ，为什么可以和矩阵 $Z_{(1,m)}$ 相加？事实上，当做 $Z+b$ 这个操作时，Python 会自动把矩阵 $b_{(1,1)}$ 自动扩展为 $b_{(1,m)}$ 这样的一个行向量，在 Python 中这称为 Broadcasting (广播) ，现在你只要看得懂就好，接下来会更详细地说明它。</p><p>同理我们可以得到</p><script type="math/tex; mode=display">A = [a^{(1)}, a^{(2)}, ... , a^{(m)}] = \sigma (Z)</script><p>同样也只需一句 Python 代码</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A = sigmoid(Z)</span><br></pre></td></tr></table></figure><h5 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a><strong>反向传播</strong></h5><p>接着，我们来看如何用向量化优化反向传播，计算梯度</p><p>同样，你需要计算<br>$dz^{(1)} = a^{(1)} - y^{(1)}​$<br>$dz^{(2)} = a^{(2)} - y^{(2)}​$<br>$…​$<br>一直到第 $m​$ 个</p><p>即计算 $dZ = [dz^{(1)} , dz^{(2)} , … , dz^{(m)}]$</p><p>之前我们已经得到 $A = [a^{(1)}, a^{(2)}, … , a^{(m)}] = \sigma (Z)​$<br>我们再定义输出标签 $Y = [y^{(1)}, y^{(2)}, … , y^{(m)}]​$</p><p>那么，</p><script type="math/tex; mode=display">dZ = A-Y = [a^{(1)}-y^{(1)} , a^{(2)}-y^{(2)} , ... , a^{(m)}-y^{(m)}]</script><p>有了 $dZ$ 我们就可以计算 $dw, db​$<br>根据之前的公式，有 </p><script type="math/tex; mode=display">db = \frac{1}{m} \sum_{i=1}^{m}dz^{(i)}</script><script type="math/tex; mode=display">dw = \frac{1}{m}X \cdot dZ^{T} = \frac{1}{m}\left( \begin{array}{c}\vdots & \vdots & \ldots & \vdots \\\\x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\\vdots & \vdots & \ldots & \vdots\end{array} \right)\left( \begin{array}{c}dz^{(1)} \\\\\vdots   \\\\dz^{(m)}\end{array} \right)= \frac{1}{m}[x^{(1)}dz^{(1)} + ...+ x^{(m)}dz^{(m)}]</script><p>对应的 Python 代码即为<br><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db = np.sum(dZ)/m</span><br><span class="line">dw = np.dot(x,dZ.T)/m</span><br></pre></td></tr></table></figure></p><p>最后，我们可以得到向量化后的梯度下降算法<br><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line">Z = np.dot(w.T,X) + b</span><br><span class="line">A = sigmoid(Z)</span><br><span class="line">dw = np.dot(x,dZ.T)/m</span><br><span class="line">db = np.sum(dZ)/m</span><br><span class="line">w = w - alpha * dw</span><br><span class="line">b = b - alpha * db</span><br></pre></td></tr></table></figure></p><p>你可能会有疑问，为什么这里不需要再计算 Cost function (代价函数) $J$ 了，笔者认为 $J$ 只是对数几率回归模型所需要的损失函数，借助它我们才能计算出 $dw, db$ ，从而进行迭代。在后续的作业中，计算 $J​$ 可以帮助我们对模型进一步分析。<br>这样，我们就完成了对数几率回归的梯度下降的一次迭代，但如果你需要多次执行迭代的操作，只能显式的调用for循环。</p></blockquote><h3 id="Broadcasting-广播"><a href="#Broadcasting-广播" class="headerlink" title="Broadcasting (广播) "></a><strong>Broadcasting (广播) </strong></h3><p>Broadcasting (广播) 机制主要是为了方便不同 shape 的 array (可以理解为不同形状的矩阵) 进行数学运算</p><ul><li>当我们将向量和一个常量做加减乘除操作时，比如对数几率回归中的 $w^{T}x+b$ ，会对向量中的每一格元素都和常量做一次操作，或者你可以理解为把这个数复制 $m \times n$ 次，使其变为一个形状相同的 $(m,n)$ 矩阵，如 :</li></ul><script type="math/tex; mode=display">\begin{bmatrix}1 \\ 2 \\ 3 \\ 4\end{bmatrix} + 100 => \begin{bmatrix}1 \\ 2 \\ 3 \\ 4\end{bmatrix} + \begin{bmatrix}100 \\ 100 \\ 100 \\ 100\end{bmatrix} = \begin{bmatrix}101 \\ 102 \\ 103 \\ 104\end{bmatrix}</script><ul><li>一个 $(m,n)$ 矩阵和一个 $(1,n)$ 矩阵相加 (减乘除)，会将这个 $(1,n)$ 矩阵复制 $m$ 次，使其变为 $(m,n)$ 矩阵然后相加，如 :</li></ul><script type="math/tex; mode=display">\begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix} 100 & 200 & 300\end{bmatrix} => \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix} 100 & 200 & 300 \\ 100 & 200 & 300\end{bmatrix} = \begin{bmatrix}101 & 202 & 303 \\ 104 & 205 & 306\end{bmatrix}</script><ul><li>同样地，一个 $(m,n)$ 矩阵和一个 $(m,1)$ 矩阵相加 (减乘除)，会将这个 $(m,1)$ 矩阵复制 $n$ 次，使其变为 $(m,n)$ 矩阵然后相加</li></ul><script type="math/tex; mode=display">\begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix}100 \\ 200\end{bmatrix} => \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix} 100 & 100 & 100 \\ 200 & 200 & 200\end{bmatrix} = \begin{bmatrix}101 & 102 & 103 \\ 204 & 205 & 206\end{bmatrix}</script><p>通俗的讲，numpy 会通过复制的方法，使两个不同形状的矩阵变得一致，再执行相关操作。值得一提的是，为了保证运算按照我们的想法进行，使用 reshape() 函数是一个较好的习惯。</p><h3 id="Homework"><a href="#Homework" class="headerlink" title="Homework"></a><strong>Homework</strong></h3><p>附上所有代码：</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># LogisticRegression.py</span></span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">import</span> h5py</span><br><span class="line"><span class="hljs-keyword">import</span> scipy</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image</span><br><span class="line"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> ndimage</span><br><span class="line"><span class="hljs-keyword">from</span> lr_utils <span class="hljs-keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Sigmoid 函数</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(z)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Compute the sigmoid of z</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    z -- A scalar or numpy array of any size.</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Return:</span></span><br><span class="line"><span class="hljs-string">    s -- sigmoid(z)</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    s = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-z))</span><br><span class="line">    <span class="hljs-keyword">return</span> s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 初始化 w,b</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize_with_zeros</span><span class="hljs-params">(dim)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Argument:</span></span><br><span class="line"><span class="hljs-string">    dim -- size of the w vector we want (or number of parameters in this case)</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    w -- initialized vector of shape (dim, 1)</span></span><br><span class="line"><span class="hljs-string">    b -- initialized scalar (corresponds to the bias)</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    w = np.zeros((dim, <span class="hljs-number">1</span>))  <span class="hljs-comment"># (dim, 1) 是shape参数，代表初始化一个dim*1的矩阵</span></span><br><span class="line">    b = <span class="hljs-number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">assert</span> (w.shape == (dim, <span class="hljs-number">1</span>))</span><br><span class="line">    <span class="hljs-keyword">assert</span> (isinstance(b, float) <span class="hljs-keyword">or</span> isinstance(b, int))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> w, b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># propagate 正向与反向传播</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">propagate</span><span class="hljs-params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implement the cost function and its gradient for the propagation explained above</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="hljs-string">    b -- bias, a scalar</span></span><br><span class="line"><span class="hljs-string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="hljs-string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Return:</span></span><br><span class="line"><span class="hljs-string">    cost -- negative log-likelihood cost for logistic regression</span></span><br><span class="line"><span class="hljs-string">    dw -- gradient of the loss with respect to w, thus same shape as w</span></span><br><span class="line"><span class="hljs-string">    db -- gradient of the loss with respect to b, thus same shape as b</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Tips:</span></span><br><span class="line"><span class="hljs-string">    - Write your code step by step for the propagation. np.log(), np.dot()</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="hljs-number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># FORWARD PROPAGATION (FROM X TO COST)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T, X) + b)  <span class="hljs-comment"># compute activation</span></span><br><span class="line">    cost = <span class="hljs-number">-1</span> / m * (np.dot(Y,np.log(A).T) + np.dot(<span class="hljs-number">1</span> - Y,np.log(<span class="hljs-number">1</span> - A).T))  <span class="hljs-comment"># compute cost</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># BACKWARD PROPAGATION (TO FIND GRAD)</span></span><br><span class="line">    dw = <span class="hljs-number">1</span> / m * np.dot(X, (A - Y).T)</span><br><span class="line">    db = <span class="hljs-number">1</span> / m * np.sum(A - Y)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">assert</span> (dw.shape == w.shape)</span><br><span class="line">    <span class="hljs-keyword">assert</span> (db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost)  <span class="hljs-comment"># 删除shape为1的维度，比如cost=[[1]]，则经过np.squeeze处理后cost=[1]</span></span><br><span class="line">    <span class="hljs-keyword">assert</span> (cost.shape == ())</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="hljs-string">"dw"</span>: dw, <span class="hljs-string">"db"</span>: db&#125;</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> grads, cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 梯度下降</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">optimize</span><span class="hljs-params">(w, b, X, Y, num_iterations, learning_rate, print_cost=False)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    This function optimizes w and b by running a gradient descent algorithm</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="hljs-string">    b -- bias, a scalar</span></span><br><span class="line"><span class="hljs-string">    X -- data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="hljs-string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="hljs-string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="hljs-string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="hljs-string">    print_cost -- True to print the loss every 100 steps</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    params -- dictionary containing the weights w and bias b</span></span><br><span class="line"><span class="hljs-string">    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function</span></span><br><span class="line"><span class="hljs-string">    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Tips:</span></span><br><span class="line"><span class="hljs-string">    You basically need to write down two steps and iterate through them:</span></span><br><span class="line"><span class="hljs-string">        1) Calculate the cost and the gradient for the current parameters. Use propagate().</span></span><br><span class="line"><span class="hljs-string">        2) Update the parameters using gradient descent rule for w and b.</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Cost and gradient calculation</span></span><br><span class="line">        grads, cost = propagate(w, b, X, Y)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Retrieve derivatives from grads</span></span><br><span class="line">        dw = grads[<span class="hljs-string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="hljs-string">"db"</span>]</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># update rule</span></span><br><span class="line">        w = w - dw * learning_rate</span><br><span class="line">        b = b - db * learning_rate</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Record the costs</span></span><br><span class="line">        <span class="hljs-keyword">if</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Print the cost every 100 training examples</span></span><br><span class="line">        <span class="hljs-keyword">if</span> print_cost <span class="hljs-keyword">and</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:</span><br><span class="line">            print(<span class="hljs-string">"Cost after iteration %i: %f"</span> % (i, cost))</span><br><span class="line"></span><br><span class="line">    params = &#123;<span class="hljs-string">"w"</span>: w, <span class="hljs-string">"b"</span>: b&#125;</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="hljs-string">"dw"</span>: dw, <span class="hljs-string">"db"</span>: db&#125;</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> params, grads, costs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 利用logistic regression判断Y的标签值</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(w, b, X)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="hljs-string">    b -- bias, a scalar</span></span><br><span class="line"><span class="hljs-string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="hljs-number">1</span>]</span><br><span class="line">    Y_prediction = np.zeros((<span class="hljs-number">1</span>, m))</span><br><span class="line">    w = w.reshape(X.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Compute vector "A" predicting the probabilities of a cat being present in the picture</span></span><br><span class="line">    A = sigmoid(np.dot(w.T, X) + b)  <span class="hljs-comment"># A.shape = (1,m)</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(A.shape[<span class="hljs-number">1</span>]):</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Convert probabilities A[0,i] to actual predictions p[0,i]</span></span><br><span class="line">        <span class="hljs-keyword">if</span> A[<span class="hljs-number">0</span>, i] &gt; <span class="hljs-number">0.5</span>:</span><br><span class="line">            Y_prediction[<span class="hljs-number">0</span>, i] = <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="hljs-number">0</span>, i] = <span class="hljs-number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">assert</span> (Y_prediction.shape == (<span class="hljs-number">1</span>, m))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> Y_prediction</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 构建整个模型</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">model</span><span class="hljs-params">(X_train,</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-params">          Y_train,</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-params">          X_test,</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-params">          Y_test,</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-params">          num_iterations=<span class="hljs-number">2000</span>,</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-params">          learning_rate=<span class="hljs-number">0.5</span>,</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-params">          print_cost=False)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Builds the logistic regression model by calling the function you've implemented previously</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span></span><br><span class="line"><span class="hljs-string">    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)</span></span><br><span class="line"><span class="hljs-string">    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span></span><br><span class="line"><span class="hljs-string">    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)</span></span><br><span class="line"><span class="hljs-string">    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters</span></span><br><span class="line"><span class="hljs-string">    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()</span></span><br><span class="line"><span class="hljs-string">    print_cost -- Set to true to print the cost every 100 iterations</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    d -- dictionary containing information about the model.</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># initialize parameters with zeros</span></span><br><span class="line">    w, b = initialize_with_zeros(X_train.shape[<span class="hljs-number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Gradient descent</span></span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations,</span><br><span class="line">                                        learning_rate, print_cost)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Retrieve parameters w and b from dictionary "parameters"</span></span><br><span class="line">    w = parameters[<span class="hljs-string">"w"</span>]</span><br><span class="line">    b = parameters[<span class="hljs-string">"b"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Predict test/train set examples</span></span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line">    Y_prediction_train = predict(w, b, X_train)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Print train/test Errors</span></span><br><span class="line">    print(<span class="hljs-string">"train accuracy: &#123;&#125; %"</span>.format(</span><br><span class="line">        <span class="hljs-number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="hljs-number">100</span>))</span><br><span class="line">    print(<span class="hljs-string">"test accuracy: &#123;&#125; %"</span>.format(</span><br><span class="line">        <span class="hljs-number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="hljs-number">100</span>))</span><br><span class="line"></span><br><span class="line">    d = &#123;</span><br><span class="line">        <span class="hljs-string">"costs"</span>: costs,</span><br><span class="line">        <span class="hljs-string">"Y_prediction_test"</span>: Y_prediction_test,</span><br><span class="line">        <span class="hljs-string">"Y_prediction_train"</span>: Y_prediction_train,</span><br><span class="line">        <span class="hljs-string">"w"</span>: w,</span><br><span class="line">        <span class="hljs-string">"b"</span>: b,</span><br><span class="line">        <span class="hljs-string">"learning_rate"</span>: learning_rate,</span><br><span class="line">        <span class="hljs-string">"num_iterations"</span>: num_iterations</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span></span><br><span class="line">    train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset(</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    m_train = train_set_x_orig.shape[<span class="hljs-number">0</span>]</span><br><span class="line">    m_test = test_set_x_orig.shape[<span class="hljs-number">0</span>]</span><br><span class="line">    num_px = train_set_x_orig.shape[<span class="hljs-number">2</span>]</span><br><span class="line"></span><br><span class="line">    train_set_x_flatten = train_set_x_orig.reshape(m_train, <span class="hljs-number">-1</span>).T</span><br><span class="line">    test_set_x_flatten = test_set_x_orig.reshape(m_test, <span class="hljs-number">-1</span>).T</span><br><span class="line">    train_set_x = train_set_x_flatten / <span class="hljs-number">255.</span></span><br><span class="line">    test_set_x = test_set_x_flatten / <span class="hljs-number">255.</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># train model</span></span><br><span class="line">    d = model(</span><br><span class="line">        train_set_x,</span><br><span class="line">        train_set_y,</span><br><span class="line">        test_set_x,</span><br><span class="line">        test_set_y,</span><br><span class="line">        num_iterations=<span class="hljs-number">2000</span>,</span><br><span class="line">        learning_rate=<span class="hljs-number">0.005</span>,</span><br><span class="line">        print_cost=<span class="hljs-keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 判断单张图片是否有猫</span></span><br><span class="line">    my_image = <span class="hljs-string">"a.jpg"</span>  <span class="hljs-comment"># change this to the name of your image file</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># We preprocess the image to fit your algorithm.</span></span><br><span class="line">    fname = <span class="hljs-string">"images/"</span> + my_image</span><br><span class="line">    image = np.array(ndimage.imread(fname, flatten=<span class="hljs-keyword">False</span>))</span><br><span class="line">    my_image = scipy.misc.imresize(image, size=(num_px, num_px)).reshape((<span class="hljs-number">1</span>, num_px * num_px * <span class="hljs-number">3</span>)).T</span><br><span class="line">    my_predicted_image = predict(d[<span class="hljs-string">"w"</span>], d[<span class="hljs-string">"b"</span>], my_image)</span><br><span class="line"></span><br><span class="line">    print(<span class="hljs-string">"y = "</span> + str(np.squeeze(my_predicted_image)) + <span class="hljs-string">", your algorithm predicts a \""</span> + classes[</span><br><span class="line">        int(np.squeeze(my_predicted_image)),].decode(<span class="hljs-string">"utf-8"</span>) + <span class="hljs-string">"\" picture."</span>)</span><br><span class="line"></span><br><span class="line">    plt.imshow(image)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure><p>lr_utils.py<br><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># lr_utils.py</span></span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> h5py</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_dataset</span><span class="hljs-params">()</span>:</span></span><br><span class="line">    train_dataset = h5py.File(<span class="hljs-string">'datasets/train_catvnoncat.h5'</span>, <span class="hljs-string">"r"</span>)</span><br><span class="line">    train_set_x_orig = np.array(train_dataset[<span class="hljs-string">"train_set_x"</span>][:])  <span class="hljs-comment"># your train set features</span></span><br><span class="line">    train_set_y_orig = np.array(train_dataset[<span class="hljs-string">"train_set_y"</span>][:])  <span class="hljs-comment"># your train set labels</span></span><br><span class="line"></span><br><span class="line">    test_dataset = h5py.File(<span class="hljs-string">'datasets/test_catvnoncat.h5'</span>, <span class="hljs-string">"r"</span>)</span><br><span class="line">    test_set_x_orig = np.array(test_dataset[<span class="hljs-string">"test_set_x"</span>][:])  <span class="hljs-comment"># your test set features</span></span><br><span class="line">    test_set_y_orig = np.array(test_dataset[<span class="hljs-string">"test_set_y"</span>][:])  <span class="hljs-comment"># your test set labels</span></span><br><span class="line"></span><br><span class="line">    classes = np.array(test_dataset[<span class="hljs-string">"list_classes"</span>][:])  <span class="hljs-comment"># the list of classes</span></span><br><span class="line">    </span><br><span class="line">    train_set_y_orig = train_set_y_orig.reshape((<span class="hljs-number">1</span>, train_set_y_orig.shape[<span class="hljs-number">0</span>]))</span><br><span class="line">    test_set_y_orig = test_set_y_orig.reshape((<span class="hljs-number">1</span>, test_set_y_orig.shape[<span class="hljs-number">0</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">return</span> train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> deeplearning.ai </category>
          
          <category> Deep learning &amp; NN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Basic of Neural Networks-1</title>
      <link href="/2019/01/21/Deep-Learning-Week-One/"/>
      <url>/2019/01/21/Deep-Learning-Week-One/</url>
      
        <content type="html"><![CDATA[<p>本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第二周神经网络基础的相关课程。</p><h3 id="Binary-Classification-二分分类"><a href="#Binary-Classification-二分分类" class="headerlink" title="Binary Classification (二分分类)**"></a>Binary Classification (二分分类)**</h3><p>在二分分类问题中，目标是训练出一个分类器，以特征向量<code>x (feature vector)</code>为输入，以<code>y (output label)</code>为输出，<code>y</code>一般只有 ${0,1}​$ 两个离散值。以图像识别问题为例，判断图片中是否由猫存在，0代表noncat，1代表cat<br></p><a id="more"></a><p>通常，我们用 $(x,y)​$ 来表示一个单独的样本，其中<code>x(feature vector)</code>是$n_x​$维的向量 ( $n_x​$ 为样本特征个数，即决定输出的因素) ，<code>y(output label)</code>为输出，取值为 $y\in\{0,1\}​$<br><br>则m个训练样本 (training example) 可表示为</p><script type="math/tex; mode=display">\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\}</script><p>用$ m=m_{train} $表示训练样本的个数<br></p><p>最后，我们可以用更紧凑的符号 $X$ 表示整个训练集，$X$ 由训练集中的 $x^{(1)}$，$x^{(2)}$，…，$x^{(m)}$ 作为列向量组成，$X\in{\Bbb R}^{n_x*m}$，即 X.shape = $(n_x,m)$</p><script type="math/tex; mode=display">\mathbf{X} =\left( \begin{array}{c}\vdots & \vdots & \ldots & \vdots \\\\x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\\vdots & \vdots & \ldots & \vdots\end{array} \right)</script><p>同时，把<code>y</code>也放入列中，用 $Y$ 来表示，$Y\in{\Bbb R}^{1*m}$，即 Y.shape = $(1,m)​$</p><script type="math/tex; mode=display">\mathbf{Y} =\left( \begin{array}{c}y^{(1)} & y^{(2)} & \ldots & y^{(m)} \\end{array} \right)</script><h3 id="Logistic-Regression-对数几率回归"><a href="#Logistic-Regression-对数几率回归" class="headerlink" title="Logistic Regression (对数几率回归)"></a><strong>Logistic Regression (对数几率回归)</strong></h3><p>参照了周志华的西瓜书，把 Logisitic Regression 翻译为对数纪律回归，简称为对率回归。对数几率回归是一种解决二分分类问题的机器学习方法，用于预测某种实物的可能性。<br></p><blockquote><p>Given x, you want $\hat{y} = P(y=1 \mid x)​$. </p><p>In other words, if x is a picture, as talked about above, you want $\hat{y}​$ to tell you the chance that there is a cat in the picture.</p></blockquote><p>根据输入 $x$ 和参数 $w, b$，计算出 $\hat{y}$ ，下面介绍了两种方法 :</p><blockquote><p>Parameter : $w\in{\Bbb R}^{n_x}, b\in{\Bbb R}​$<br>Output : $\hat{y}​$</p><blockquote><p>One way : $\hat{y} = w^{T}x + b​$ (Linear regression) </p><ul><li>Not good for binary classification</li><li>Because you want $\hat{y}​$ to be the chance that $y​$ equals to one. In this situation  $\hat{y}​$ can be much bigger than 1 or negative.</li></ul><p>The other way : $\hat{y} = \sigma(w^{T}x + b)$ (Logistic Regression) </p><ul><li>$\sigma(z) =  \frac{1}{1+e^{-z}} ​$</li><li>通过$\sigma(z)$函数，可以将输出限定在$[0,1]$之间</li></ul></blockquote></blockquote><h3 id="Logistic-Regression-Cost-Function-对数几率回归损失函数"><a href="#Logistic-Regression-Cost-Function-对数几率回归损失函数" class="headerlink" title="Logistic Regression Cost Function (对数几率回归损失函数)"></a><strong>Logistic Regression Cost Function (对数几率回归损失函数)</strong></h3><p>给出$\{(x^{(1)},y^{(1)})…,(x^{(m)},y^{(m)})\}​$，希望通过训练集，找到参数 $w, b​$ 使得 $\hat{y}^{(i)} \approx  y^{(i)}​$ 。所以，我们需要定义一个<code>loss function</code>，通过这个<code>loss function</code>来衡量你的预测输出值 $\hat{y}​$ 与 $y​$ 的实际值由多接近 <br></p><p>对于m个训练样本，我们通常用上标 $(i)​$ 来指明数据与第 $i​$ 个样本有关。<br></p><p>通常，我们这样定义Loss function (损失函数) :</p><script type="math/tex; mode=display">L(\hat{y},y) = \frac{1}{2}(\hat{y} - y)^2</script><p>但在对数几率回归中一般不使用，因为它是non-convex (非凸的) ，将来使用<code>梯度下降算法 (Gradient Descent)</code>时无法找到全局最优值 <br></p><p>在对数几率回归中，我们使用的损失函数为 : </p><script type="math/tex; mode=display">L(\hat{y},y) = -(y \log\hat{y} + (1-y) \log(1-\hat{y}))</script><blockquote><p>If y = 1 : $L(\hat{y},y) = -\log(\hat{y})​$, you want $\hat{y}​$ to be large</p><p>if y = 0 : $L(\hat{y},y) = -\log(1-\hat{y})​$, you want  $\hat{y}​$ to be small</p><p>所以，这个损失函数和 $L(\hat{y},y) = \frac{1}{2}(\hat{y} - y)^2$ 类似，都希望 $L$ 越小越好</p></blockquote><p>上述的<code>Loss function</code>衡量了单个训练样本的表现，对于m个样本，我们定义<code>Cost function</code> (代价函数) ，它衡量了全体训练样本的表现</p><script type="math/tex; mode=display">J(w,b) = \frac{1}{m} \sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)}) = -\frac{1}{m} \sum_{i=1}^{m}y^{(i)} \log\hat{y}^{(i)} + (1-y^{(i)}) \log(1-\hat{y}^{(i)})</script><p><code>Loss function</code>只适用于单个训练样本，<code>Cost function</code>是基于参数的总代价。所以，在训练对数几率回归模型时，我们要找到合适的参数 $w, b$ 使得<code>Cost function</code>尽可能的小<br></p><h3 id="Gradient-Descent-梯度下降"><a href="#Gradient-Descent-梯度下降" class="headerlink" title="Gradient Descent (梯度下降)"></a><strong>Gradient Descent (梯度下降)</strong></h3><p>我们将使用<code>梯度下降 (Gradient Descent) 算法</code>来找出合适的参数 $w,b$，使得<code>Cost function</code> 即 $J(w,b)$ 最小 <br></p><ul><li><img src="/2019/01/21/Deep-Learning-Week-One/gradient_descent.png" alt=""></li></ul><p>最上方的小红点为初始点，对于对数几率回归，一般使用0来初始化，随机初始化也有效，但通常不这么做 <br></p><blockquote><p>梯度下降过程：</p><ul><li>从初始点开始，朝最陡的下坡方向走一步</li><li>重复上述过程，不断修正 $w, b​$ 使得 $J(w,b)​$ 接近全局最优值 (global opitmal)</li></ul></blockquote><p>代码表述为：</p><blockquote><p>Repeat {<br>$w := w - \alpha \frac{\partial J(w,b)}{\partial w}​$ &nbsp; &nbsp;&nbsp;&nbsp;在代码中 $\frac{\partial J(w,b)}{\partial w}​$ 记作”dw”<br>$b := b - \alpha \frac{\partial J(w,b)}{\partial b}​$ &nbsp; &nbsp;&nbsp;&nbsp;在代码中 $\frac{\partial J(w,b)}{\partial b}​$ 记作”db”<br>}</p></blockquote><h3 id="Computation-Graph-计算图"><a href="#Computation-Graph-计算图" class="headerlink" title="Computation Graph (计算图)"></a><strong>Computation Graph (计算图)</strong></h3><p>神经网络的训练包含了两个过程：</p><ul><li>正向传播 (Forward Propagation)，从输入经过一层层神经网络，最后得到 $\hat{y}$ ，从而计算代价函数 $J$</li><li>反向传播 (Back Propagation)，根据损失函数 $L(\hat{y},y)$ 来反方向的计算每一层参数的偏导数，从而更新参数</li></ul><p>下面我们用计算图 (Computation Graph) 来理解这个过程</p><p><img src="/2019/01/21/Deep-Learning-Week-One/computation_graph.png" alt=""></p><p>从左向右，可以计算出 $J​$ 的值，对应着神经网络中输入经过计算得到代价函数 $J(w,b)​$ 值的过程<br></p><p>从右向左，根据求导的链式法则，可以得到：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial a} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial a} = 3 \cdot 1 = 3</script><script type="math/tex; mode=display">\frac{\partial J}{\partial b} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial u} \cdot \frac{\partial u}{\partial b} = 3 \cdot 1 \cdot c = 6</script><script type="math/tex; mode=display">\frac{\partial J}{\partial c} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial u} \cdot \frac{\partial u}{\partial c} = 3 \cdot 1 \cdot b = 9</script><p>在反向传播中，一般我们只关心最终输出值 (在这个例子中是 $J$ ) ，需要计算 $J$ 对于某个变量 (记作var) 的导数，即 $\frac {dJ}{dvar}​$，在Python代码中简写为<code>dvar</code> <br></p><h3 id="Logistic-Regression-Gradient-Descent-对数几率回归梯度下降"><a href="#Logistic-Regression-Gradient-Descent-对数几率回归梯度下降" class="headerlink" title="Logistic Regression Gradient Descent (对数几率回归梯度下降)"></a><strong>Logistic Regression Gradient Descent (对数几率回归梯度下降)</strong></h3><p>现在，我们来实现对数几率回归梯度下降算法，只考虑单个样本的情况 :</p><blockquote><p>$z = w^{T} + b$</p><p>$\hat{y} = a = \sigma({z})​$</p><p>$L(\hat{y},y) = -(y \log\hat{y} + (1-y) \log(1-\hat{y}))​$</p></blockquote><p>假设样本只有两个，分别为 $x1, x2​$，则计算图如下 :</p><p><img src="/2019/01/21/Deep-Learning-Week-One/logistic_regression_gradient_descent.png" alt=""></p><p>在对数几率回归中，我们需要做的是，改变参数 $w, b$ 的值，来最小化损失函数，即需要计算出 $dw, dz$ <br></p><p>向后传播计算损失函数 $L​$ 的偏导数步骤如下：</p><blockquote><ul><li>$da = \frac {dL(a,y)}{da} = -\frac {y}{a} + \frac{1-y}{1-a}​$</li><li>$dz = \frac {dL}{dz} = \frac {dL}{da} \cdot \frac {da}{dz}= (-\frac {y}{a} + \frac{1-y}{1-a}) \cdot a \cdot (1-a) = a - y​$</li><li>$dw_1 =  \frac {dL}{dw_1} = x_1 \cdot dz $</li><li>$dw_2 =  \frac {dL}{dw_2} = x_2 \cdot dz $</li><li>$db = dz$</li></ul></blockquote><p>所以，在对数几率回归梯度下降算法中你需要做的是</p><blockquote><ul><li>$ dz = a - y$</li><li>$dw_1 = x_1 \cdot dz ​$</li><li>$dw_2 = x_2 \cdot dz ​$</li><li>$db = dz​$</li><li>更新$w_1​$,  $w_1 = w_1 - \alpha dw_1​$</li><li>更新$w_2$,  $w_2 = w_2 - \alpha dw_2$</li><li>更新$b​$,  $b = b - \alpha db​$</li></ul></blockquote><h3 id="Gradient-descent-on-m-examples-将梯度下降算法应用到整个训练集"><a href="#Gradient-descent-on-m-examples-将梯度下降算法应用到整个训练集" class="headerlink" title="Gradient descent on $m$ examples (将梯度下降算法应用到整个训练集)"></a><strong>Gradient descent on $m$ examples (将梯度下降算法应用到整个训练集)</strong></h3><p>之前只实现了单个样本的梯度下降算法，现在我们将梯度下降算法应用到整个训练集<br></p><blockquote><p>$J(w,b) = \frac{1}{m} \sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)}) $</p><p>$a^{(i)} = \hat{y}^{(i)} = \sigma(z^{(i)}) = \sigma(w^{T}x^{(i)}+b)$</p><p>$\frac {\partial}{\partial w_1}J(w,b) = \frac {1}{m} \sum_{i=1}^{m} \frac {\partial}{\partial w_1}L(a^{(i)},y^{(i)}) = \frac {1}{m} \sum_{i=1}^{m}dw_1^{(i)}$ </p><ul><li>$dw_1^{(i)}​$按照之前单个样本的情况计算</li></ul></blockquote><p>伪代码如下 :</p><blockquote><p>$J=0; dw_1=0; dw_2=0; db=0;$<br>$for \quad i = 1 \quad to \quad m $<br>$\quad z^{(i)} = w^{T}x^{(i)}+b$<br>$\quad a^{(i)} = \sigma(z^{(i)})$<br>$\quad J += -(y^{(i)} \log a^{(i)} + (1-y^{(i)}) \log(1-a^{(i)}))$<br>$\quad dz^{(i)} = a^{(i)}-y^{(i)}$<br>$\quad dw_1 += x_1^{(i)}dz^{(i)}$<br>$\quad dw_2 += x_2^{(i)}dz^{(i)} \qquad$<br>$\quad…\quad\quad\quad\quad\quad\quad$ //这里应该是一个循环，这里 $n_x = 2$<br>$\quad db  += dz^{(i)}$<br>​$J /= m$<br>$dw_1 /= m$<br>$dw_2 /= m$<br>$db /= m$</p><p>$w_1 = w_1 - \alpha dw_1​$<br>$w_2 = w_2 - \alpha dw_2​$<br>$b = b - \alpha db​$</p></blockquote><p>但这种方法，有两个循环，一个是最外层的循环，循环 $m$ 个训练样本，另一个是 $dw_1, dw_2$ (feature) 的循环，在这个例子中 $n_x = 2$。随着训练集越来越大，应该尽量避免使用for循环，而使用向量化技术 (vectorization)</p>]]></content>
      
      
      <categories>
          
          <category> deeplearning.ai </category>
          
          <category> Deep learning &amp; NN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java常量池</title>
      <link href="/2019/01/17/Java-Constant-Pool/"/>
      <url>/2019/01/17/Java-Constant-Pool/</url>
      
        <content type="html"><![CDATA[<h3 id="常量池"><a href="#常量池" class="headerlink" title="常量池"></a>常量池</h3><ul><li>相同的值只存储一份，节省内存，共享访问，提高运行效率</li></ul><a id="more"></a><h3 id="基本类型的包装类"><a href="#基本类型的包装类" class="headerlink" title="基本类型的包装类"></a>基本类型的包装类</h3><ul><li><code>Boolean</code> <code>Byte</code> <code>Short</code> <code>Integer</code> <code>Long</code> <code>Character</code> <code>Float</code> <code>Double</code> 八种基本类型的包装类</li><li>常量值范围<ul><li><code>Boolean</code>：true, false</li><li><code>Byte</code> <code>Character</code> : \u0000 - \u007f</li><li><code>Short</code> <code>Integer</code> <code>Long</code> : -128 - 127</li><li><code>Float</code> <code>Double</code> : 无常量池</li></ul></li></ul><h3 id="与equals"><a href="#与equals" class="headerlink" title="==与equals()"></a>==与equals()</h3><ul><li>对于基本数据类型，<code>==</code>比较他们的数值</li><li>对于对象，<code>==</code>比较两个对象在内存中的存放地址，可以通过重写<code>equals()</code>来比较两个对象的内容是否相等</li></ul><h3 id="字符串常量"><a href="#字符串常量" class="headerlink" title="字符串常量"></a>字符串常量</h3><ul><li>Java为常量字符串建立了常量池缓存机制<figure class="highlight java hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">String s1 = <span class="hljs-string">"abc"</span>;</span><br><span class="line">String s2 = <span class="hljs-string">"ab"</span> + <span class="hljs-string">"c"</span>;</span><br><span class="line">String s3 = <span class="hljs-string">"a"</span> + <span class="hljs-string">"b"</span> + <span class="hljs-string">"c"</span>; <span class="hljs-comment">//都是常量，是确定的，编译器将优化</span></span><br><span class="line">System.out.println(s1==s2); <span class="hljs-comment">//true</span></span><br><span class="line">System.out.println(s1==s3); <span class="hljs-comment">//true</span></span><br><span class="line">System.out.println(s2==s3); <span class="hljs-comment">//true</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="基本类型的包装类和字符串的两种创建方式"><a href="#基本类型的包装类和字符串的两种创建方式" class="headerlink" title="基本类型的包装类和字符串的两种创建方式"></a>基本类型的包装类和字符串的两种创建方式</h3><ul><li>字面值赋值，放在栈内存<strong>（将被常量化）</strong><ul><li><code>Integer a = 1;</code> </li><li><code>String b = &quot;abc&quot;;</code></li></ul></li><li>new对象进行创建，放在堆内存<strong>（不会常量化）</strong><ul><li><code>Integer c = new Integer(1);</code></li><li><code>String d = new String(&quot;abc&quot;);</code></li></ul></li><li>栈内存读取速度快，容量小</li><li>堆内存读取速度慢，容量大，可以通俗的理解为Java认为new出来的对象所占内存较大（不确定，而字面值是确定的），所以需要放在堆内存</li></ul><h3 id="Integer常量池的例子"><a href="#Integer常量池的例子" class="headerlink" title="Integer常量池的例子"></a>Integer常量池的例子</h3><figure class="highlight java hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">int</span> i1 = <span class="hljs-number">10</span>;</span><br><span class="line">Integer i2 = <span class="hljs-number">10</span>; <span class="hljs-comment">//自动装箱，10本来只是int，是基本类型，而我们需要把它变成一个对象，相当于包装了一层</span></span><br><span class="line">System.out.println(i1==i2) <span class="hljs-comment">//true</span></span><br><span class="line"><span class="hljs-comment">//自动拆箱 基本类型和包装类进行比较，包装类自动拆箱</span></span><br><span class="line"></span><br><span class="line">Integer i3 = <span class="hljs-keyword">new</span> Integer(<span class="hljs-number">10</span>);</span><br><span class="line">System.out.println(i1==i3) <span class="hljs-comment">//true 同理，包装类自动拆箱</span></span><br><span class="line">System.out.println(i2==i3) <span class="hljs-comment">//false i2,i3都是对象，而i2是常量，在常量池，i3是new出来的对象，在堆内存中 </span></span><br><span class="line"></span><br><span class="line">Integer i4 = <span class="hljs-keyword">new</span> Integer(<span class="hljs-number">5</span>);</span><br><span class="line">Integer i5 = <span class="hljs-keyword">new</span> Integer(<span class="hljs-number">5</span>);</span><br><span class="line">System.out.println(i1 == (i4+i5)); <span class="hljs-comment">//true</span></span><br><span class="line">System.out.println(i1 == (i4+i5)); <span class="hljs-comment">//true</span></span><br><span class="line">System.out.println(i1 == (i4+i5)); <span class="hljs-comment">//true</span></span><br><span class="line"><span class="hljs-comment">//i4+i5的操作将会使i4,i5自动拆箱为基本类型并运算得到10，而根据之前所提到的，基本类型和包装类进行比较，包装类自动拆箱，所以都为true</span></span><br><span class="line"></span><br><span class="line">Integer i6 = i4 + i5;</span><br><span class="line">System.out.println(i1==i6); <span class="hljs-comment">//true，同理i4+i5的操作使i4,i5自动拆箱，得到10，相当于Integer i6 = 10;</span></span><br><span class="line">System.out.println(i3==i6); <span class="hljs-comment">//false</span></span><br></pre></td></tr></table></figure><h3 id="String常量池的例子"><a href="#String常量池的例子" class="headerlink" title="String常量池的例子"></a>String常量池的例子</h3><p>字符串常量池存在于方法区，方法区包含的都是在整个程序中唯一的元素，如static变量</p><ul><li><p>一个简单的例子</p><figure class="highlight java hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">String s1 = <span class="hljs-string">"abc"</span>;</span><br><span class="line">String s2 = <span class="hljs-string">"abc"</span>;</span><br><span class="line">String s3 = <span class="hljs-keyword">new</span> String(<span class="hljs-string">"abc"</span>);</span><br><span class="line">String s4 = <span class="hljs-keyword">new</span> String(<span class="hljs-string">"abc"</span>);</span><br><span class="line">System.out.println(s1==s2); <span class="hljs-comment">//true 都是常量池</span></span><br><span class="line">System.out.println(s1==s3); <span class="hljs-comment">//false 一个是栈内存，一个是堆内存</span></span><br><span class="line">System.out.println(s3==s4); <span class="hljs-comment">//false 都是堆内存，但是不同对象</span></span><br></pre></td></tr></table></figure></li><li><p>图解：(<code>&quot;</code>由<code>&#39;</code>代替)</p><pre class="mermaid">graph LR;subgraph 方法区  s['abc']endsubgraph 堆  A["s3 = new String('abc')"]  B["s4 = new String('abc')"]endsubgraph 栈  s1  s2  s3  s4ends1-->ss2-->sA-->sB-->ss3-->As4-->B</pre></li><li>更为复杂的例子<figure class="highlight java hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">String s5 = <span class="hljs-string">"abcdef"</span>;</span><br><span class="line">String s6 = s1 + <span class="hljs-string">"def"</span>; <span class="hljs-comment">//涉及到变量（不确定的），编译器不会优化</span></span><br><span class="line">String s7 = <span class="hljs-string">"abc"</span> + <span class="hljs-string">"def"</span>; <span class="hljs-comment">//都是常量，编译器会优化成abcdef</span></span><br><span class="line">String s8 = <span class="hljs-string">"abc"</span> + <span class="hljs-keyword">new</span> String(<span class="hljs-string">"def"</span>); <span class="hljs-comment">//涉及到new对象，编译器不优化</span></span><br><span class="line">System.out.println(s6==s7); <span class="hljs-comment">//false</span></span><br><span class="line">System.out.println(s6==s8); <span class="hljs-comment">//false</span></span><br><span class="line">System.out.println(s7==s8); <span class="hljs-comment">//false</span></span><br><span class="line">System.out.println(s5==s7); <span class="hljs-comment">//true</span></span><br><span class="line"></span><br><span class="line">String s9 = s3 + <span class="hljs-string">"def"</span>; <span class="hljs-comment">//由于s3是new的，涉及到new对象，编译器不优化</span></span><br><span class="line">System.out.println(s7==s9); <span class="hljs-comment">//false</span></span><br><span class="line"><span class="hljs-comment">//对于s5~s9，只有s5,s7是在常量池中，其余都在堆内存上，且地址互不相同</span></span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vim Tutorial</title>
      <link href="/2019/01/14/Vim-Tutorial/"/>
      <url>/2019/01/14/Vim-Tutorial/</url>
      
        <content type="html"><![CDATA[<p>This tutorial includes some basic vim commands and I hope that it will be helpful.</p><a id="more"></a><h3 id="Moving-the-cursor"><a href="#Moving-the-cursor" class="headerlink" title="Moving the cursor"></a>Moving the cursor</h3><ul><li><code>h</code> : left</li><li><code>j</code> : down</li><li><code>k</code> : up</li><li><code>l</code> : right</li></ul><p>It takes time to get used to it.</p><h3 id="Navigation"><a href="#Navigation" class="headerlink" title="Navigation"></a>Navigation</h3><ul><li><code>w</code>: move the cursor one word forward (to the first letter)</li><li><code>b</code> : one word backward (also to the first letter)</li><li><code>e</code> : one word forward (to the last letter)</li><li><code>fx</code> : forward to the letter <code>x</code></li><li><code>(</code> : to the start of the sentence</li><li><code>)</code> : start of the sentence</li><li><code>0</code> : start of line</li><li><code>$</code> : end of line </li><li><code>{</code> : start of paragraph</li><li><code>}</code> : end of paragraph </li><li><code>G</code> : end of file<ul><li><code>ctrl+G</code> : to see the cursor location and file status </li></ul></li><li><code>gg</code> : start of file</li><li><code>xG</code> : to the number <code>x</code> line of file<ul><li><strong>typing a number before a motion repeats it that many times!</strong></li></ul></li></ul><h3 id="Delete"><a href="#Delete" class="headerlink" title="Delete"></a>Delete</h3><ul><li><code>x</code>: delete the character at the cursor</li><li><code>dw</code>: delete all the characters between the cursor and the first letter of the next word<ul><li>e.g. Please de<code>l</code>ete the word. (Assume the cursor is at <code>l</code>)</li><li>After you press dw, the sentence becomes <code>Please dethe word delete</code></li></ul></li><li><code>de</code>: delete all the characters between the cursor and the next space<ul><li>e.g. Please de<code>l</code>ete the word. (Assume the cursor is at <code>l</code>)</li><li>After you press de, the sentence becomes <code>Please de the word delete</code></li></ul></li><li><code>d$</code> : delete to end of line</li><li><code>dd</code> : delete whole line</li><li><code>p</code> : After you delete something, press p to paste things you delete wherever you like.</li></ul><h3 id="Insert"><a href="#Insert" class="headerlink" title="Insert"></a>Insert</h3><ul><li><code>a</code> : insert after the cursor</li><li><code>A</code> : insert after the end of line</li><li><code>i</code> : insert before the cursor</li><li><code>I</code> : insert before the start of line </li><li><code>o</code> : insert in the next line</li><li><code>O</code> : insert in the previous line</li></ul><h3 id="Search"><a href="#Search" class="headerlink" title="Search"></a>Search</h3><ul><li><code>/yourSearchString + &lt;Enter&gt;</code> : search for <code>yourSearchString</code><ul><li><code>n</code> : to search for the same string again (press <code>&lt;Enter&gt;</code> to exit)</li><li><code>N</code> : to search for the same string again, but in opposite direction</li><li><code>ctrl+o</code> : to go back to where you came from</li><li><code>ctrl+i</code> : to go forward</li><li>set option<ul><li><code>:set ic</code> : ignore case</li><li><code>:set noic</code> : disable ignore case</li><li><code>:set hls</code> : highlight the matches</li><li><code>:set nohls</code> : disable highlight matches</li><li><code>:set is</code> : increase search</li><li><code>:set nois</code>: disable increase search</li></ul></li></ul></li><li><code>%</code> : move the cursor to the other matching parenthesis</li></ul><h3 id="Replace"><a href="#Replace" class="headerlink" title="Replace"></a>Replace</h3><ul><li><code>rx</code> : replace the character at cursor with <code>x</code></li><li><code>ce</code> : almost the same as <code>de</code>, but this time will place you in Insert Mode</li><li><code>s/old/new</code> : replace the first occurrence of ‘old’ with ‘new’</li><li><code>s/old/new/g</code> : replace all occurrence of ‘old’ with ‘new’ in one line</li><li><code>#,#/old/new/g</code> : #,# are the line numbers of the range of lines where the replace should be done</li><li><code>%s/old/new/g</code> : replace all occurrence of ‘old’ with ‘new’ in the whole file</li><li><code>%s/old/new/g</code> : replace all occurrence of ‘old’ with ‘new’ in the whole file, with a prompt whether to replace or not</li></ul><h3 id="Undo-amp-Redo"><a href="#Undo-amp-Redo" class="headerlink" title="Undo &amp; Redo"></a>Undo &amp; Redo</h3><ul><li><code>u</code> : undo the last command</li><li><code>U</code> : undo the command excuting on the while line</li><li><code>ctrl+R</code> : redo the command</li></ul><h3 id="Copy-amp-Paste"><a href="#Copy-amp-Paste" class="headerlink" title="Copy &amp; Paste"></a>Copy &amp; Paste</h3><ul><li><code>y</code> : to copy</li><li><code>p</code> : to paste<ul><li>e.g. Start Visual Mode with <code>v</code> and move the cursor to chose whatever you want, type <code>y</code> to copy the highlighted text and type <code>p</code> to paste the text.</li></ul></li></ul><h3 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h3><ul><li><code>.</code> : repeat the last command</li><li><code>&lt;start position&gt;&lt;command&gt;&lt;end position&gt;</code> : many commands follow this pattern<ul><li>e.g. <code>0y$</code> means copy the whole line<ul><li><code>0</code> move the cursor to the start of line</li><li><code>y</code> copy</li><li><code>$</code> move the cursor to the end of line</li></ul></li></ul></li><li><code>ctrl+n</code> : auto complete</li></ul>]]></content>
      
      
      <categories>
          
          <category> vim </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vim </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
