<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Neural Network and Deep Learning (Week Two)</title>
      <link href="/2019/01/26/Deep-Learning-Week-Two/"/>
      <url>/2019/01/26/Deep-Learning-Week-Two/</url>
      
        <content type="html"><![CDATA[<h3 id="Vectorization-向量化"><a href="#Vectorization-向量化" class="headerlink" title="Vectorization (向量化)"></a><strong>Vectorization (向量化)</strong></h3><p>Vectorization (向量化) 的意义在于：消除代码中显式的调用for循环。在深度学习领域中，你常常需要训练大数据集，所以程序运行的效率非常重要，否则需要等待很长时间才能得到结果。<br></p><a id="more"></a><p>在对数几率回归中，你需要计算 $z = w^{T}x + b$ ，其中 $w, x$ 都是 $n_x$ 维向量</p><blockquote><p>如果你是用非向量化的实现，即传统的矩阵相乘算法伪代码如下：<br>$ z = 0 $<br>$ for  \quad i  \quad in  \quad range(n_x) : $<br>$ \quad z+= w[i] * x[i] $<br>$z+=b ​$</p><p>若使用向量化的实现，Python代码如下：<br><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z = np.dot(w,x) + b</span><br></pre></td></tr></table></figure></p><p>即清晰又高效</p></blockquote><p>可以测试一下这两个代码在效率方面的差距，大约差了300倍。可以试想一下，如果你的代码1分钟出结果，和5个小时才出结果，那可差太远了。<br></p><p>为了加快深度学习的运算速度，可以使用GPU (Graphic Processing Unit) 。事实上，GPU 和 CPU 都有并行指令 (Parallelization Instructions) ，同时也叫作 SIMD (Single Instruction Multiple Data)，即单指令流多数据流，是一种采用一个控制器来控制多个处理器，同时对一组数据 (又称“数据向量”) 中的每一个分别执行相同的操作从而实现空间上的并行性的技术。numpy 是 Python 数据分析及科学计算的基础库，它有许多内置 (Built-in) 函数，主要用于数组的计算，充分利用了并行化，使得运算速度大大提高。在深度学习的领域，一般来说，能不用显式的调用for循环就不用。<br></p><p>这样，我们就可以使用 Vectorization 来优化梯度下降算法，先去掉内层对 feature (特征 $w1, w2 …$) 的循环 ：</p><blockquote><p>$J=0; db=0; dw = np.zeros(n_x,1)$<br>$for \quad i = 1 \quad to \quad m $<br>$\quad z^{(i)} = w^{T}x^{(i)}+b$<br>$\quad a^{(i)} = \sigma(z^{(i)})$<br>$\quad J += -(y^{(i)} \log a^{(i)} + (1-y^{(i)}) \log(1-a^{(i)}))$<br>$\quad dz^{(i)} = a^{(i)}-y^{(i)}$<br>$\quad dw+=x^{(i)}dz^{(i)} \quad \quad $ //vectorization<br>$\quad db  += dz^{(i)}$<br>$J /= m$<br>$dw /= m$<br>$db /= m​$</p></blockquote><p>然后，我们再去掉对 $m$ 个训练样本的外层循环，分别从正向传播和反向传播两方面来分析：</p><blockquote><h5 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a><strong>正向传播</strong></h5><p>回顾一下对数几率回归的正向传播步骤，如果你有 $m$ 个训练样本</p><p>那么对第一个样本进行预测，你需要计算<br>$ \quad z^{(1)} = w^{T}x^{(1)} + b$<br>$ \quad a^{(1)} = \sigma(z^{(1)})​$</p><p>然后继续对第二个样本进行预测<br>$ \quad z^{(2)} = w^{T}x^{(2)} + b$<br>$ \quad a^{(2)} = \sigma(z^{(2)})$</p><p>然后继续对第三个，第四个，…，直到第 $m$ 个</p><p>回忆一下之前在二分分类部分所讲到的用更紧凑的符号 $X​$ 表示整个训练集，即大小为 $(n_x,m)​$ 的矩阵 : </p><script type="math/tex; mode=display">\mathbf{X} =\left( \begin{array}{c}\vdots & \vdots & \ldots & \vdots \\\\x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\\vdots & \vdots & \ldots & \vdots\end{array} \right)</script><p>那么计算 $z^{(1)}, z^{(2)}, … , z^{(m)}$ 的步骤如下 :<br>首先先构造一个 $(1,m)$ 的矩阵 $[z^{(1)}, z^{(2)}, … , z^{(m)}]$ ，则 </p><script type="math/tex; mode=display">Z = [z^{(1)}, z^{(2)}, ... , z^{(m)}] = w^{T}X + [b, b , ... , b] = [w^{T}x^{(1)} + b, w^{T}x^{(2)} + b] , ... , w^{T}x^{(m)} + b]</script><p>在 Python 中一句代码即可完成上述过程<br><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T,X) + b</span><br></pre></td></tr></table></figure></p><p>你可能会有疑问，明明这里的 $b$ 只是一个实数 (或者说是一个 $b_{(1,1)}$ 的矩阵) ，为什么可以和矩阵 $Z_{(1,m)}$ 相加？事实上，当做 $Z+b$ 这个操作时，Python 会自动把矩阵 $b_{(1,1)}$ 自动扩展为 $b_{(1,m)}$ 这样的一个行向量，在 Python 中这称为 Broadcasting (广播) ，现在你只要看得懂就好，接下来会更详细地说明它。</p><p>同理我们可以得到</p><script type="math/tex; mode=display">A = [a^{(1)}, a^{(2)}, ... , a^{(m)}] = \sigma (Z)</script><p>同样也只需一句 Python 代码</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A = sigmoid(Z)</span><br></pre></td></tr></table></figure><h5 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a><strong>反向传播</strong></h5><p>接着，我们来看如何用向量化优化反向传播，计算梯度</p><p>同样，你需要计算<br>$dz^{(1)} = a^{(1)} - y^{(1)}​$<br>$dz^{(2)} = a^{(2)} - y^{(2)}​$<br>$…​$<br>一直到第 $m​$ 个</p><p>即计算 $dZ = [dz^{(1)} , dz^{(2)} , … , dz^{(m)}]$</p><p>之前我们已经得到 $A = [a^{(1)}, a^{(2)}, … , a^{(m)}] = \sigma (Z)​$<br>我们再定义输出标签 $Y = [y^{(1)}, y^{(2)}, … , y^{(m)}]​$</p><p>那么，</p><script type="math/tex; mode=display">dZ = A-Y = [a^{(1)}-y^{(1)} , a^{(2)}-y^{(2)} , ... , a^{(m)}-y^{(m)}]</script><p>有了 $dZ$ 我们就可以计算 $dw, db​$<br>根据之前的公式，有 </p><script type="math/tex; mode=display">db = \frac{1}{m} \sum_{i=1}^{m}dz^{(i)}</script><script type="math/tex; mode=display">dw = \frac{1}{m}X \cdot dZ^{T} = \frac{1}{m}\left( \begin{array}{c}\vdots & \vdots & \ldots & \vdots \\\\x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\\vdots & \vdots & \ldots & \vdots\end{array} \right)\left( \begin{array}{c}dz^{(1)} \\\\\vdots   \\\\dz^{(m)}\end{array} \right)= \frac{1}{m}[x^{(1)}dz^{(1)} + ...+ x^{(m)}dz^{(m)}]</script><p>对应的 Python 代码即为<br><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db = np.sum(dZ)/m</span><br><span class="line">dw = np.dot(x,dZ.T)/m</span><br></pre></td></tr></table></figure></p><p>最后，我们可以得到向量化后的梯度下降算法<br><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line">Z = np.dot(w.T,X) + b</span><br><span class="line">A = sigmoid(Z)</span><br><span class="line">dw = np.dot(x,dZ.T)/m</span><br><span class="line">db = np.sum(dZ)/m</span><br><span class="line">w = w - alpha * dw</span><br><span class="line">b = b - alpha * db</span><br></pre></td></tr></table></figure></p><p>你可能会有疑问，为什么这里不需要再计算 Cost function (代价函数) $J$ 了，笔者认为 $J$ 只是对数几率回归模型所需要的损失函数，借助它我们才能计算出 $dw, db$ ，从而进行迭代，而不需要真正计算 $J$，至于为什么之前计算了 $J$ ，我也不知道。<br>这样，我们就完成了对数几率回归的梯度下降的一次迭代，但如果你需要多次执行迭代的操作，只能显式的调用for循环。</p></blockquote><h3 id="Broadcasting-广播"><a href="#Broadcasting-广播" class="headerlink" title="Broadcasting (广播) "></a><strong>Broadcasting (广播) </strong></h3><p>Broadcasting (广播) 机制主要是为了方便不同 shape 的 array (可以理解为不同形状的矩阵) 进行数学运算</p><ul><li>当我们将向量和一个常量做加减乘除操作时，比如对数几率回归中的 $w^{T}x+b$ ，会对向量中的每一格元素都和常量做一次操作，或者你可以理解为把这个数复制 $m \times n$ 次，使其变为一个形状相同的 $(m,n)$ 矩阵，如 :</li></ul><script type="math/tex; mode=display">\begin{bmatrix}1 \\ 2 \\ 3 \\ 4\end{bmatrix} + 100 => \begin{bmatrix}1 \\ 2 \\ 3 \\ 4\end{bmatrix} + \begin{bmatrix}100 \\ 100 \\ 100 \\ 100\end{bmatrix} = \begin{bmatrix}101 \\ 102 \\ 103 \\ 104\end{bmatrix}</script><ul><li>一个 $(m,n)$ 矩阵和一个 $(1,n)$ 矩阵相加 (减乘除)，会将这个 $(1,n)$ 矩阵复制 $m$ 次，使其变为 $(m,n)$ 矩阵然后相加，如 :</li></ul><script type="math/tex; mode=display">\begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix} 100 & 200 & 300\end{bmatrix} => \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix} 100 & 200 & 300 \\ 100 & 200 & 300\end{bmatrix} = \begin{bmatrix}101 & 202 & 303 \\ 104 & 205 & 306\end{bmatrix}</script><ul><li>同样地，一个 $(m,n)$ 矩阵和一个 $(m,1)$ 矩阵相加 (减乘除)，会将这个 $(m,1)$ 矩阵复制 $n$ 次，使其变为 $(m,n)$ 矩阵然后相加</li></ul><script type="math/tex; mode=display">\begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix}100 \\ 200\end{bmatrix} => \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix} 100 & 100 & 100 \\ 200 & 200 & 200\end{bmatrix} = \begin{bmatrix}101 & 102 & 103 \\ 204 & 205 & 206\end{bmatrix}</script><p>通俗的讲，numpy 会通过复制的方法，使两个不同形状的矩阵变得一致，再执行相关操作。值得一提的是，为了保证运算按照我们的想法进行，使用 reshape() 函数是一个较好的习惯。</p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> Neural Network </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Network and Deep Learning (Week One)</title>
      <link href="/2019/01/21/Deep-Learning-Week-One/"/>
      <url>/2019/01/21/Deep-Learning-Week-One/</url>
      
        <content type="html"><![CDATA[<h3 id="Binary-Classification-二分分类"><a href="#Binary-Classification-二分分类" class="headerlink" title="Binary Classification (二分分类)"></a><strong>Binary Classification (二分分类)</strong></h3><p>在二分分类问题中，目标是训练出一个分类器，以特征向量<code>x (feature vector)</code>为输入，以<code>y (output label)</code>为输出，<code>y</code>一般只有 ${0,1}​$ 两个离散值。以图像识别问题为例，判断图片中是否由猫存在，0代表noncat，1代表cat<br></p><a id="more"></a><p>通常，我们用 $(x,y)​$ 来表示一个单独的样本，其中<code>x(feature vector)</code>是$n_x​$维的向量 ( $n_x​$ 为样本特征个数，即决定输出的因素) ，<code>y(output label)</code>为输出，取值为 $y\in\{0,1\}​$<br><br>则m个训练样本 (training example) 可表示为</p><script type="math/tex; mode=display">\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\}</script><p>用$ m=m_{train} $表示训练样本的个数<br></p><p>最后，我们可以用更紧凑的符号 $X$ 表示整个训练集，$X$ 由训练集中的 $x^{(1)}$，$x^{(2)}$，…，$x^{(m)}$ 作为列向量组成，$X\in{\Bbb R}^{n_x*m}$，即 X.shape = $(n_x,m)$</p><script type="math/tex; mode=display">\mathbf{X} =\left( \begin{array}{c}\vdots & \vdots & \ldots & \vdots \\\\x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\\vdots & \vdots & \ldots & \vdots\end{array} \right)</script><p>同时，把<code>y</code>也放入列中，用 $Y$ 来表示，$Y\in{\Bbb R}^{1*m}$，即 Y.shape = $(1,m)​$</p><script type="math/tex; mode=display">\mathbf{Y} =\left( \begin{array}{c}y^{(1)} & y^{(2)} & \ldots & y^{(m)} \\end{array} \right)</script><h3 id="Logistic-Regression-对数几率回归"><a href="#Logistic-Regression-对数几率回归" class="headerlink" title="Logistic Regression (对数几率回归)"></a><strong>Logistic Regression (对数几率回归)</strong></h3><p>参照了周志华的西瓜书，把 Logisitic Regression 翻译为对数纪律回归，简称为对率回归。对数几率回归是一种解决二分分类问题的机器学习方法，用于预测某种实物的可能性。<br></p><blockquote><p>Given x, you want $\hat{y} = P(y=1 \mid x)​$. </p><p>In other words, if x is a picture, as talked about above, you want $\hat{y}​$ to tell you the chance that there is a cat in the picture.</p></blockquote><p>根据输入 $x$ 和参数 $w, b$，计算出 $\hat{y}$ ，下面介绍了两种方法 :</p><blockquote><p>Parameter : $w\in{\Bbb R}^{n_x}, b\in{\Bbb R}​$<br>Output : $\hat{y}​$</p><blockquote><p>One way : $\hat{y} = w^{T}x + b​$ (Linear regression) </p><ul><li>Not good for binary classification</li><li>Because you want $\hat{y}​$ to be the chance that $y​$ equals to one. In this situation  $\hat{y}​$ can be much bigger than 1 or negative.</li></ul><p>The other way : $\hat{y} = \sigma(w^{T}x + b)$ (Logistic Regression) </p><ul><li>$\sigma(z) =  \frac{1}{1+e^{-z}} ​$</li><li>通过$\sigma(z)$函数，可以将输出限定在$[0,1]$之间</li></ul></blockquote></blockquote><h3 id="Logistic-Regression-Cost-Function-对数几率回归损失函数"><a href="#Logistic-Regression-Cost-Function-对数几率回归损失函数" class="headerlink" title="Logistic Regression Cost Function (对数几率回归损失函数)"></a><strong>Logistic Regression Cost Function (对数几率回归损失函数)</strong></h3><p>给出$\{(x^{(1)},y^{(1)})…,(x^{(m)},y^{(m)})\}​$，希望通过训练集，找到参数 $w, b​$ 使得 $\hat{y}^{(i)} \approx  y^{(i)}​$ 。所以，我们需要定义一个<code>loss function</code>，通过这个<code>loss function</code>来衡量你的预测输出值 $\hat{y}​$ 与 $y​$ 的实际值由多接近 <br></p><p>对于m个训练样本，我们通常用上标 $(i)​$ 来指明数据与第 $i​$ 个样本有关。<br></p><p>通常，我们这样定义Loss function (损失函数) :</p><script type="math/tex; mode=display">L(\hat{y},y) = \frac{1}{2}(\hat{y} - y)^2</script><p>但在对数几率回归中一般不使用，因为它是non-convex (非凸的) ，将来使用<code>梯度下降算法 (Gradient Descent)</code>时无法找到全局最优值 <br></p><p>在对数几率回归中，我们使用的损失函数为 : </p><script type="math/tex; mode=display">L(\hat{y},y) = -(y \log\hat{y} + (1-y) \log(1-\hat{y}))</script><blockquote><p>If y = 1 : $L(\hat{y},y) = -\log(\hat{y})​$, you want $\hat{y}​$ to be large</p><p>if y = 0 : $L(\hat{y},y) = -\log(1-\hat{y})​$, you want  $\hat{y}​$ to be small</p><p>所以，这个损失函数和 $L(\hat{y},y) = \frac{1}{2}(\hat{y} - y)^2$ 类似，都希望 $L$ 越小越好</p></blockquote><p>上述的<code>Loss function</code>衡量了单个训练样本的表现，对于m个样本，我们定义<code>Cost function</code> (代价函数) ，它衡量了全体训练样本的表现</p><script type="math/tex; mode=display">J(w,b) = \frac{1}{m} \sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)}) = -\frac{1}{m} \sum_{i=1}^{m}y^{(i)} \log\hat{y}^{(i)} + (1-y^{(i)}) \log(1-\hat{y}^{(i)})</script><p><code>Loss function</code>只适用于单个训练样本，<code>Cost function</code>是基于参数的总代价。所以，在训练对数几率回归模型时，我们要找到合适的参数 $w, b$ 使得<code>Cost function</code>尽可能的小<br></p><h3 id="Gradient-Descent-梯度下降"><a href="#Gradient-Descent-梯度下降" class="headerlink" title="Gradient Descent (梯度下降)"></a><strong>Gradient Descent (梯度下降)</strong></h3><p>我们将使用<code>梯度下降 (Gradient Descent) 算法</code>来找出合适的参数 $w,b$，使得<code>Cost function</code> 即 $J(w,b)$ 最小 <br></p><ul><li><img src="/2019/01/21/Deep-Learning-Week-One/gradient_descent.png" alt=""></li></ul><p>最上方的小红点为初始点，对于对数几率回归，一般使用0来初始化，随机初始化也有效，但通常不这么做 <br></p><blockquote><p>梯度下降过程：</p><ul><li>从初始点开始，朝最陡的下坡方向走一步</li><li>重复上述过程，不断修正 $w, b​$ 使得 $J(w,b)​$ 接近全局最优值 (global opitmal)</li></ul></blockquote><p>代码表述为：</p><blockquote><p>Repeat {<br>$w := w - \alpha \frac{\partial J(w,b)}{\partial w}​$ &nbsp; &nbsp;&nbsp;&nbsp;在代码中 $\frac{\partial J(w,b)}{\partial w}​$ 记作”dw”<br>$b := b - \alpha \frac{\partial J(w,b)}{\partial b}​$ &nbsp; &nbsp;&nbsp;&nbsp;在代码中 $\frac{\partial J(w,b)}{\partial b}​$ 记作”db”<br>}</p></blockquote><h3 id="Computation-Graph-计算图"><a href="#Computation-Graph-计算图" class="headerlink" title="Computation Graph (计算图)"></a><strong>Computation Graph (计算图)</strong></h3><p>神经网络的训练包含了两个过程：</p><ul><li>正向传播 (Forward Propagation)，从输入经过一层层神经网络，最后得到 $\hat{y}$ ，从而计算代价函数 $J$</li><li>反向传播 (Back Propagation)，根据损失函数 $L(\hat{y},y)$ 来反方向的计算每一层参数的偏导数，从而更新参数</li></ul><p>下面我们用计算图 (Computation Graph) 来理解这个过程</p><p><img src="/2019/01/21/Deep-Learning-Week-One/computation_graph.png" alt=""></p><p>从左向右，可以计算出 $J​$ 的值，对应着神经网络中输入经过计算得到代价函数 $J(w,b)​$ 值的过程<br></p><p>从右向左，根据求导的链式法则，可以得到：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial a} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial a} = 3 \cdot 1 = 3</script><script type="math/tex; mode=display">\frac{\partial J}{\partial b} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial u} \cdot \frac{\partial u}{\partial b} = 3 \cdot 1 \cdot c = 6</script><script type="math/tex; mode=display">\frac{\partial J}{\partial c} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial u} \cdot \frac{\partial u}{\partial c} = 3 \cdot 1 \cdot b = 9</script><p>在反向传播中，一般我们只关心最终输出值 (在这个例子中是 $J$ ) ，需要计算 $J$ 对于某个变量 (记作var) 的导数，即 $\frac {dJ}{dvar}​$，在Python代码中简写为<code>dvar</code> <br></p><h3 id="Logistic-Regression-Gradient-Descent-对数几率回归梯度下降"><a href="#Logistic-Regression-Gradient-Descent-对数几率回归梯度下降" class="headerlink" title="Logistic Regression Gradient Descent (对数几率回归梯度下降)"></a><strong>Logistic Regression Gradient Descent (对数几率回归梯度下降)</strong></h3><p>现在，我们来实现对数几率回归梯度下降算法，只考虑单个样本的情况 :</p><blockquote><p>$z = w^{T} + b$</p><p>$\hat{y} = a = \sigma({z})​$</p><p>$L(\hat{y},y) = -(y \log\hat{y} + (1-y) \log(1-\hat{y}))​$</p></blockquote><p>假设样本只有两个，分别为 $x1, x2​$，则计算图如下 :</p><p><img src="/2019/01/21/Deep-Learning-Week-One/logistic_regression_gradient_descent.png" alt=""></p><p>在对数几率回归中，我们需要做的是，改变参数 $w, b$ 的值，来最小化损失函数，即需要计算出 $dw, dz$ <br></p><p>向后传播计算损失函数 $L​$ 的偏导数步骤如下：</p><blockquote><ul><li>$da = \frac {dL(a,y)}{da} = -\frac {y}{a} + \frac{1-y}{1-a}​$</li><li>$dz = \frac {dL}{dz} = \frac {dL}{da} \cdot \frac {da}{dz}= (-\frac {y}{a} + \frac{1-y}{1-a}) \cdot a \cdot (1-a) = a - y​$</li><li>$dw_1 =  \frac {dL}{dw_1} = x_1 \cdot dz $</li><li>$dw_2 =  \frac {dL}{dw_2} = x_2 \cdot dz $</li><li>$db = dz$</li></ul></blockquote><p>所以，在对数几率回归梯度下降算法中你需要做的是</p><blockquote><ul><li>$ dz = a - y$</li><li>$dw_1 = x_1 \cdot dz ​$</li><li>$dw_2 = x_2 \cdot dz ​$</li><li>$db = dz​$</li><li>更新$w_1​$,  $w_1 = w_1 - \alpha dw_1​$</li><li>更新$w_2$,  $w_2 = w_2 - \alpha dw_2$</li><li>更新$b​$,  $b = b - \alpha db​$</li></ul></blockquote><h3 id="Gradient-descent-on-m-examples-将梯度下降算法应用到整个训练集"><a href="#Gradient-descent-on-m-examples-将梯度下降算法应用到整个训练集" class="headerlink" title="Gradient descent on $m$ examples (将梯度下降算法应用到整个训练集)"></a><strong>Gradient descent on $m$ examples (将梯度下降算法应用到整个训练集)</strong></h3><p>之前只实现了单个样本的梯度下降算法，现在我们将梯度下降算法应用到整个训练集<br></p><blockquote><p>$J(w,b) = \frac{1}{m} \sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)}) $</p><p>$a^{(i)} = \hat{y}^{(i)} = \sigma(z^{(i)}) = \sigma(w^{T}x^{(i)}+b)$</p><p>$\frac {\partial}{\partial w_1}J(w,b) = \frac {1}{m} \sum_{i=1}^{m} \frac {\partial}{\partial w_1}L(a^{(i)},y^{(i)}) = \frac {1}{m} \sum_{i=1}^{m}dw_1^{(i)}$ </p><ul><li>$dw_1^{(i)}​$按照之前单个样本的情况计算</li></ul></blockquote><p>伪代码如下 :</p><blockquote><p>$J=0; dw_1=0; dw_2=0; db=0;$<br>$for \quad i = 1 \quad to \quad m $<br>$\quad z^{(i)} = w^{T}x^{(i)}+b$<br>$\quad a^{(i)} = \sigma(z^{(i)})$<br>$\quad J += -(y^{(i)} \log a^{(i)} + (1-y^{(i)}) \log(1-a^{(i)}))$<br>$\quad dz^{(i)} = a^{(i)}-y^{(i)}$<br>$\quad dw_1 += x_1^{(i)}dz^{(i)}$<br>$\quad dw_2 += x_2^{(i)}dz^{(i)} \qquad$<br>$\quad…\quad\quad\quad\quad\quad\quad$ //这里应该是一个循环，这里 $n_x = 2$<br>$\quad db  += dz^{(i)}$<br>​$J /= m$<br>$dw_1 /= m$<br>$dw_2 /= m$<br>$db /= m$</p><p>$w_1 = w_1 - \alpha dw_1​$<br>$w_2 = w_2 - \alpha dw_2​$<br>$b = b - \alpha db​$</p></blockquote><p>但这种方法，有两个循环，一个是最外层的循环，循环 $m$ 个训练样本，另一个是 $dw_1, dw_2$ (feature) 的循环，在这个例子中 $n_x = 2$。随着训练集越来越大，应该尽量避免使用for循环，而使用向量化技术 (vectorization)</p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> Neural Network </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java常量池</title>
      <link href="/2019/01/17/Java-Constant-Pool/"/>
      <url>/2019/01/17/Java-Constant-Pool/</url>
      
        <content type="html"><![CDATA[<h3 id="常量池"><a href="#常量池" class="headerlink" title="常量池"></a>常量池</h3><ul><li>相同的值只存储一份，节省内存，共享访问，提高运行效率</li></ul><a id="more"></a><h3 id="基本类型的包装类"><a href="#基本类型的包装类" class="headerlink" title="基本类型的包装类"></a>基本类型的包装类</h3><ul><li><code>Boolean</code> <code>Byte</code> <code>Short</code> <code>Integer</code> <code>Long</code> <code>Character</code> <code>Float</code> <code>Double</code> 八种基本类型的包装类</li><li>常量值范围<ul><li><code>Boolean</code>：true, false</li><li><code>Byte</code> <code>Character</code> : \u0000 - \u007f</li><li><code>Short</code> <code>Integer</code> <code>Long</code> : -128 - 127</li><li><code>Float</code> <code>Double</code> : 无常量池</li></ul></li></ul><h3 id="与equals"><a href="#与equals" class="headerlink" title="==与equals()"></a>==与equals()</h3><ul><li>对于基本数据类型，<code>==</code>比较他们的数值</li><li>对于对象，<code>==</code>比较两个对象在内存中的存放地址，可以通过重写<code>equals()</code>来比较两个对象的内容是否相等</li></ul><h3 id="字符串常量"><a href="#字符串常量" class="headerlink" title="字符串常量"></a>字符串常量</h3><ul><li>Java为常量字符串建立了常量池缓存机制<figure class="highlight java hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">String s1 = <span class="hljs-string">"abc"</span>;</span><br><span class="line">String s2 = <span class="hljs-string">"ab"</span> + <span class="hljs-string">"c"</span>;</span><br><span class="line">String s3 = <span class="hljs-string">"a"</span> + <span class="hljs-string">"b"</span> + <span class="hljs-string">"c"</span>; <span class="hljs-comment">//都是常量，是确定的，编译器将优化</span></span><br><span class="line">System.out.println(s1==s2); <span class="hljs-comment">//true</span></span><br><span class="line">System.out.println(s1==s3); <span class="hljs-comment">//true</span></span><br><span class="line">System.out.println(s2==s3); <span class="hljs-comment">//true</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="基本类型的包装类和字符串的两种创建方式"><a href="#基本类型的包装类和字符串的两种创建方式" class="headerlink" title="基本类型的包装类和字符串的两种创建方式"></a>基本类型的包装类和字符串的两种创建方式</h3><ul><li>字面值赋值，放在栈内存<strong>（将被常量化）</strong><ul><li><code>Integer a = 1;</code> </li><li><code>String b = &quot;abc&quot;;</code></li></ul></li><li>new对象进行创建，放在堆内存<strong>（不会常量化）</strong><ul><li><code>Integer c = new Integer(1);</code></li><li><code>String d = new String(&quot;abc&quot;);</code></li></ul></li><li>栈内存读取速度快，容量小</li><li>堆内存读取速度慢，容量大，可以通俗的理解为Java认为new出来的对象所占内存较大（不确定，而字面值是确定的），所以需要放在堆内存</li></ul><h3 id="Integer常量池的例子"><a href="#Integer常量池的例子" class="headerlink" title="Integer常量池的例子"></a>Integer常量池的例子</h3><figure class="highlight java hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">int</span> i1 = <span class="hljs-number">10</span>;</span><br><span class="line">Integer i2 = <span class="hljs-number">10</span>; <span class="hljs-comment">//自动装箱，10本来只是int，是基本类型，而我们需要把它变成一个对象，相当于包装了一层</span></span><br><span class="line">System.out.println(i1==i2) <span class="hljs-comment">//true</span></span><br><span class="line"><span class="hljs-comment">//自动拆箱 基本类型和包装类进行比较，包装类自动拆箱</span></span><br><span class="line"></span><br><span class="line">Integer i3 = <span class="hljs-keyword">new</span> Integer(<span class="hljs-number">10</span>);</span><br><span class="line">System.out.println(i1==i3) <span class="hljs-comment">//true 同理，包装类自动拆箱</span></span><br><span class="line">System.out.println(i2==i3) <span class="hljs-comment">//false i2,i3都是对象，而i2是常量，在常量池，i3是new出来的对象，在堆内存中 </span></span><br><span class="line"></span><br><span class="line">Integer i4 = <span class="hljs-keyword">new</span> Integer(<span class="hljs-number">5</span>);</span><br><span class="line">Integer i5 = <span class="hljs-keyword">new</span> Integer(<span class="hljs-number">5</span>);</span><br><span class="line">System.out.println(i1 == (i4+i5)); <span class="hljs-comment">//true</span></span><br><span class="line">System.out.println(i1 == (i4+i5)); <span class="hljs-comment">//true</span></span><br><span class="line">System.out.println(i1 == (i4+i5)); <span class="hljs-comment">//true</span></span><br><span class="line"><span class="hljs-comment">//i4+i5的操作将会使i4,i5自动拆箱为基本类型并运算得到10，而根据之前所提到的，基本类型和包装类进行比较，包装类自动拆箱，所以都为true</span></span><br><span class="line"></span><br><span class="line">Integer i6 = i4 + i5;</span><br><span class="line">System.out.println(i1==i6); <span class="hljs-comment">//true，同理i4+i5的操作使i4,i5自动拆箱，得到10，相当于Integer i6 = 10;</span></span><br><span class="line">System.out.println(i3==i6); <span class="hljs-comment">//false</span></span><br></pre></td></tr></table></figure><h3 id="String常量池的例子"><a href="#String常量池的例子" class="headerlink" title="String常量池的例子"></a>String常量池的例子</h3><p>字符串常量池存在于方法区，方法区包含的都是在整个程序中唯一的元素，如static变量</p><ul><li><p>一个简单的例子</p><figure class="highlight java hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">String s1 = <span class="hljs-string">"abc"</span>;</span><br><span class="line">String s2 = <span class="hljs-string">"abc"</span>;</span><br><span class="line">String s3 = <span class="hljs-keyword">new</span> String(<span class="hljs-string">"abc"</span>);</span><br><span class="line">String s4 = <span class="hljs-keyword">new</span> String(<span class="hljs-string">"abc"</span>);</span><br><span class="line">System.out.println(s1==s2); <span class="hljs-comment">//true 都是常量池</span></span><br><span class="line">System.out.println(s1==s3); <span class="hljs-comment">//false 一个是栈内存，一个是堆内存</span></span><br><span class="line">System.out.println(s3==s4); <span class="hljs-comment">//false 都是堆内存，但是不同对象</span></span><br></pre></td></tr></table></figure></li><li><p>图解：(<code>&quot;</code>由<code>&#39;</code>代替)</p><pre class="mermaid">graph LR;subgraph 方法区  s['abc']endsubgraph 堆  A["s3 = new String('abc')"]  B["s4 = new String('abc')"]endsubgraph 栈  s1  s2  s3  s4ends1-->ss2-->sA-->sB-->ss3-->As4-->B</pre></li><li>更为复杂的例子<figure class="highlight java hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">String s5 = <span class="hljs-string">"abcdef"</span>;</span><br><span class="line">String s6 = s1 + <span class="hljs-string">"def"</span>; <span class="hljs-comment">//涉及到变量（不确定的），编译器不会优化</span></span><br><span class="line">String s7 = <span class="hljs-string">"abc"</span> + <span class="hljs-string">"def"</span>; <span class="hljs-comment">//都是常量，编译器会优化成abcdef</span></span><br><span class="line">String s8 = <span class="hljs-string">"abc"</span> + <span class="hljs-keyword">new</span> String(<span class="hljs-string">"def"</span>); <span class="hljs-comment">//涉及到new对象，编译器不优化</span></span><br><span class="line">System.out.println(s6==s7); <span class="hljs-comment">//false</span></span><br><span class="line">System.out.println(s6==s8); <span class="hljs-comment">//false</span></span><br><span class="line">System.out.println(s7==s8); <span class="hljs-comment">//false</span></span><br><span class="line">System.out.println(s5==s7); <span class="hljs-comment">//true</span></span><br><span class="line"></span><br><span class="line">String s9 = s3 + <span class="hljs-string">"def"</span>; <span class="hljs-comment">//由于s3是new的，涉及到new对象，编译器不优化</span></span><br><span class="line">System.out.println(s7==s9); <span class="hljs-comment">//false</span></span><br><span class="line"><span class="hljs-comment">//对于s5~s9，只有s5,s7是在常量池中，其余都在堆内存上，且地址互不相同</span></span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vim Tutorial</title>
      <link href="/2019/01/14/Vim-Tutorial/"/>
      <url>/2019/01/14/Vim-Tutorial/</url>
      
        <content type="html"><![CDATA[<p>This tutorial includes some basic vim commands and I hope that it will be helpful.</p><a id="more"></a><h3 id="Moving-the-cursor"><a href="#Moving-the-cursor" class="headerlink" title="Moving the cursor"></a>Moving the cursor</h3><ul><li><code>h</code> : left</li><li><code>j</code> : down</li><li><code>k</code> : up</li><li><code>l</code> : right</li></ul><p>It takes time to get used to it.</p><h3 id="Navigation"><a href="#Navigation" class="headerlink" title="Navigation"></a>Navigation</h3><ul><li><code>w</code>: move the cursor one word forward (to the first letter)</li><li><code>b</code> : one word backward (also to the first letter)</li><li><code>e</code> : one word forward (to the last letter)</li><li><code>fx</code> : forward to the letter <code>x</code></li><li><code>(</code> : to the start of the sentence</li><li><code>)</code> : start of the sentence</li><li><code>0</code> : start of line</li><li><code>$</code> : end of line </li><li><code>{</code> : start of paragraph</li><li><code>}</code> : end of paragraph </li><li><code>G</code> : end of file<ul><li><code>ctrl+G</code> : to see the cursor location and file status </li></ul></li><li><code>gg</code> : start of file</li><li><code>xG</code> : to the number <code>x</code> line of file<ul><li><strong>typing a number before a motion repeats it that many times!</strong></li></ul></li></ul><h3 id="Delete"><a href="#Delete" class="headerlink" title="Delete"></a>Delete</h3><ul><li><code>x</code>: delete the character at the cursor</li><li><code>dw</code>: delete all the characters between the cursor and the first letter of the next word<ul><li>e.g. Please de<code>l</code>ete the word. (Assume the cursor is at <code>l</code>)</li><li>After you press dw, the sentence becomes <code>Please dethe word delete</code></li></ul></li><li><code>de</code>: delete all the characters between the cursor and the next space<ul><li>e.g. Please de<code>l</code>ete the word. (Assume the cursor is at <code>l</code>)</li><li>After you press de, the sentence becomes <code>Please de the word delete</code></li></ul></li><li><code>d$</code> : delete to end of line</li><li><code>dd</code> : delete whole line</li><li><code>p</code> : After you delete something, press p to paste things you delete wherever you like.</li></ul><h3 id="Insert"><a href="#Insert" class="headerlink" title="Insert"></a>Insert</h3><ul><li><code>a</code> : insert after the cursor</li><li><code>A</code> : insert after the end of line</li><li><code>i</code> : insert before the cursor</li><li><code>I</code> : insert before the start of line </li><li><code>o</code> : insert in the next line</li><li><code>O</code> : insert in the previous line</li></ul><h3 id="Search"><a href="#Search" class="headerlink" title="Search"></a>Search</h3><ul><li><code>/yourSearchString + &lt;Enter&gt;</code> : search for <code>yourSearchString</code><ul><li><code>n</code> : to search for the same string again (press <code>&lt;Enter&gt;</code> to exit)</li><li><code>N</code> : to search for the same string again, but in opposite direction</li><li><code>ctrl+o</code> : to go back to where you came from</li><li><code>ctrl+i</code> : to go forward</li><li>set option<ul><li><code>:set ic</code> : ignore case</li><li><code>:set noic</code> : disable ignore case</li><li><code>:set hls</code> : highlight the matches</li><li><code>:set nohls</code> : disable highlight matches</li><li><code>:set is</code> : increase search</li><li><code>:set nois</code>: disable increase search</li></ul></li></ul></li><li><code>%</code> : move the cursor to the other matching parenthesis</li></ul><h3 id="Replace"><a href="#Replace" class="headerlink" title="Replace"></a>Replace</h3><ul><li><code>rx</code> : replace the character at cursor with <code>x</code></li><li><code>ce</code> : almost the same as <code>de</code>, but this time will place you in Insert Mode</li><li><code>s/old/new</code> : replace the first occurrence of ‘old’ with ‘new’</li><li><code>s/old/new/g</code> : replace all occurrence of ‘old’ with ‘new’ in one line</li><li><code>#,#/old/new/g</code> : #,# are the line numbers of the range of lines where the replace should be done</li><li><code>%s/old/new/g</code> : replace all occurrence of ‘old’ with ‘new’ in the whole file</li><li><code>%s/old/new/g</code> : replace all occurrence of ‘old’ with ‘new’ in the whole file, with a prompt whether to replace or not</li></ul><h3 id="Undo-amp-Redo"><a href="#Undo-amp-Redo" class="headerlink" title="Undo &amp; Redo"></a>Undo &amp; Redo</h3><ul><li><code>u</code> : undo the last command</li><li><code>U</code> : undo the command excuting on the while line</li><li><code>ctrl+R</code> : redo the command</li></ul><h3 id="Copy-amp-Paste"><a href="#Copy-amp-Paste" class="headerlink" title="Copy &amp; Paste"></a>Copy &amp; Paste</h3><ul><li><code>y</code> : to copy</li><li><code>p</code> : to paste<ul><li>e.g. Start Visual Mode with <code>v</code> and move the cursor to chose whatever you want, type <code>y</code> to copy the highlighted text and type <code>p</code> to paste the text.</li></ul></li></ul><h3 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h3><ul><li><code>.</code> : repeat the last command</li><li><code>&lt;start position&gt;&lt;command&gt;&lt;end position&gt;</code> : many commands follow this pattern<ul><li>e.g. <code>0y$</code> means copy the whole line<ul><li><code>0</code> move the cursor to the start of line</li><li><code>y</code> copy</li><li><code>$</code> move the cursor to the end of line</li></ul></li></ul></li><li><code>ctrl+n</code> : auto complete</li></ul>]]></content>
      
      
      <categories>
          
          <category> vim </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vim </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
