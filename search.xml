<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Objected-Oriented Analysis and Design-4</title>
      <link href="/2019/02/19/Object-Oriented-4/"/>
      <url>/2019/02/19/Object-Oriented-4/</url>
      
        <content type="html"><![CDATA[<p>本文为华东师范大学开设的面向对象分析与设计慕课课程第四周笔记，系统地介绍了面向对象分析法的CRC法，同时举了一些例子帮助理解。</p><p>首先，我们再来复习一下面向对象分析的三种方法：</p><ul><li>Conceptual model (Larman) 概念模型，又称“名词法”</li><li>Analysis model with stereotypes (Jacobson) 分析模型</li><li>CRC cards (Beck, Cunningham) CRC 法</li></ul><a id="more"></a><blockquote><p>A good analyst knows more than one strategy and even may mix strategies in order to identify the objects and relationships for the design phase. 一个好的分析师掌握多种技术，知道如何混合使用各种技术，目标只有一个：发现对象、定义对象之间的关系。</p></blockquote><h3 id="标识概念类和对象"><a href="#标识概念类和对象" class="headerlink" title="标识概念类和对象"></a><strong>标识概念类和对象</strong></h3><p>原则：</p><ul><li>类，表示一组具有相同行为、属性的对象 Remember that a class represents a group (classification) of objects with the same behaviors</li><li>类，在表示对象群体的时候，一般用单数 Generally, classes that represent objects should be given names that are singular nouns</li><li>根据类，可以创建所需要数量的对象个体 We are free to instantiate as many of each object as needed</li></ul><blockquote><p>可能的抉择：</p><ul><li>一个名词，是作为概念类合适，还是作为某个类的属性更合适 ？ Sometimes it is challenging to decide whether something should be represented as a class<ul><li>主要还是根据实际情况，取决于我们要解决的问题。一般来说，对问题了解得越细越透彻，越有把握做出决定</li></ul></li></ul></blockquote><p>在适当的细节层面定义概念类 We want to define classes with the proper amount of<br>detail</p><ul><li>当发现一个类非常复杂时，要考虑拆分成多个小一点的类 When a class becomes too complex, it often should be decomposed into multiple smaller classes to distribute the responsibilities</li><li>但是，又不能有太多的类！But , not too many classes</li></ul><p>在表示概念类的过程中，同时要考虑每个类的职责分配，但是不需要在领域模型中明示。</p><p>总的来说，对于即将要开发的系统，每项任务 (每个职责) 都需要有一个或多个类去处理，在一开始表示成较为粗狂的职责描述，不必要定义每个类的每个操作</p><h3 id="CRC方法标识概念类"><a href="#CRC方法标识概念类" class="headerlink" title="CRC方法标识概念类"></a><strong>CRC方法标识概念类</strong></h3><h5 id="CRC-Class-Responsibility-Cooperation-分别代表什么？"><a href="#CRC-Class-Responsibility-Cooperation-分别代表什么？" class="headerlink" title="CRC (Class Responsibility Cooperation) 分别代表什么？"></a><strong>CRC (Class Responsibility Cooperation) 分别代表什么？</strong></h5><ul><li>Classes (of objects) 类</li><li>Responsibilities (of the objects in each class) 职责</li><li>Collaborations (with objects in other classes) 协作<ul><li>In UML, these will be examples of “associations” </li></ul></li></ul><p>CRC又称为索引卡片 (CRC card)，每张卡片代表一个类，每张卡片上写出这个类承担的职责、与其合作交互的其他类名</p><blockquote><p>例子：</p><p><img src="/2019/02/19/Object-Oriented-4/1.png" alt=""></p></blockquote><h5 id="CRC的特点"><a href="#CRC的特点" class="headerlink" title="CRC的特点"></a><strong>CRC的特点</strong></h5><ul><li>非正式的、不是很细节的 Informal, non-detailed</li><li>采用小组“头脑风暴”的形式提出概念 Used for group brain-storming</li><li>CRC的目标不是提供完整的设计 Not intended to provide a complete design</li><li>CRC产生的结果需要进一步精化 End result is a first cut at classes for an object-oriented model</li></ul><h5 id="CRC的输入信息：用例模型"><a href="#CRC的输入信息：用例模型" class="headerlink" title="CRC的输入信息：用例模型"></a><strong>CRC的输入信息：用例模型</strong></h5><ul><li><p>用例图、边界、用例描述，清楚地描述了系统需求，作为CRC概念类分析的起点 A good starting point for CRC analysis is a clear statement of all of the use-cases.</p></li><li><p>用例描述的正常事件流、异常事件流，可以作为CRC的“角色扮演”的脚本 Use-cases, or their accompanying scenarios, can be used as a kind of script for the role-playing method (角色扮演) of checking the CRC cards</p><ul><li>“角色扮演” 也可以用顺序图代替</li></ul></li></ul><h3 id="CRC方法建模案例：ATM取款机软件"><a href="#CRC方法建模案例：ATM取款机软件" class="headerlink" title="CRC方法建模案例：ATM取款机软件"></a><strong>CRC方法建模案例：ATM取款机软件</strong></h3><h5 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a><strong>需求描述</strong></h5><p>University Bank will be opening in January, 2020. We plan to use a<br>full service automated teller machine (ATM) system</p><ul><li>The <strong>ATM system</strong> will interact with the <strong>customer</strong> through a <strong>display screen</strong>, numeric and special <strong>input keys</strong>, a <strong>bankcard reader</strong>, a <strong>deposit slot</strong>, and a <strong>receipt printer</strong></li><li><strong>Customers</strong> may make <strong>deposits</strong>, <strong>withdrawals</strong>, and <strong>balance inquires</strong> using the <strong>ATM machine</strong>, but the update to <strong>accounts</strong> will be handled through an <strong>interface</strong> to the <strong>Accounts system</strong></li><li><strong>Customers</strong> will be assigned a <strong>Personal Identification Number (PIN)</strong> and <strong>clearance level</strong> by the <strong>Security system</strong>. The PIN can be verified prior to any <strong>transaction</strong></li><li>In the future, we would also like to support <strong>routine operations</strong> such as a change of <strong>address</strong> or <strong>phone number</strong> using the ATM</li></ul><p>通过头脑风暴，列出概念，已加粗表示</p><h5 id="标识核心概念类-Identifying-Core-Classes"><a href="#标识核心概念类-Identifying-Core-Classes" class="headerlink" title="标识核心概念类 Identifying Core Classes "></a><strong>标识核心概念类 Identifying Core Classes </strong></h5><p>Moving from brainstorming to analysis, divide the candidate classes into categories in the following order:</p><ul><li><p>关键的critical classes (i.e., the “winners”), for which we will definitely write CRC cards Items that directly relate to the main entities of the application</p><blockquote><p>在ATM例子中，如：Account, Deposit, Withdrawal, BalanceUnquiry 这些类</p></blockquote></li><li><p>无关的 irrelevant candidates (i.e., the “losers”), which we will definitely eliminate Items that are clearly outside the system scope</p><blockquote><p>在ATM例子中，如：Printer, Prompt 这些与我们软件无关的类，可能与用户界面相关，但与模型无关</p></blockquote></li><li><p>待定的 undecided candidates (i.e., the “maybes”), which we will review further for categorization</p><ul><li>Items that we may not be able to categorize without first clarifying the system boundaries and definition</li></ul></li></ul><h5 id="明确系统范围-Clarifying-System-Scope"><a href="#明确系统范围-Clarifying-System-Scope" class="headerlink" title="明确系统范围 Clarifying System Scope"></a><strong>明确系统范围 Clarifying System Scope</strong></h5><p>To continue design, we must determine <strong>what is and what is not part of the system</strong></p><p>Scope of the ATM system</p><ul><li>Does it handle everything—the banking application, the user<br>interface, and the interactions between them?</li><li>Is the ATM responsible for updating accounting records or just recording and mediating the transaction activity?<ul><li>更新账户的操作一般也不会让ATM操作，一般是传到银行服务器上操作</li></ul></li></ul><blockquote><p>在ATM例子中，我们需要明确以上问题，有时画图是一个不错的方法来确定系统边界 (如：用例图)</p></blockquote><h5 id="去掉不必要的核心类"><a href="#去掉不必要的核心类" class="headerlink" title="去掉不必要的核心类"></a><strong>去掉不必要的核心类</strong></h5><p>进一步检查，发现不适合应用系统的类，检查那些与系统相关、但是在系统外部的类；系统需要有接口与它们通信，但不需要在系统内部为它们建模</p><blockquote><p>在ATM的例子中，比如用户类，ATM并不关心用户的各种信息，比如年龄性别，它只关心用户手里拿的银行卡的信息，以及用户在界面上的操作。</p></blockquote><p>可能你还需要合并同义词</p><blockquote><p>在ATM例子中，BankCustomer和AccountHolder其实是一个概念。但特别要注意的是，同一个名词在不同的地方表示不同的含义时，可能需要增加一个新的核心类。</p></blockquote><p>辨析一个概念是属性还是类，符合以下两点的，一般是属性</p><ul><li><p>它不做具体的事情 it does not do anything — it has no operations</p><blockquote><p>在ATM例子中，Balance and FundsAvailable have few meaningful operations and both are closely associated with Account</p></blockquote></li><li><p>它不能改变状态 it cannot change state</p><blockquote><p>If a PIN is viewed as being immutable(不变的), not changing state, then it probably should be an attribute of Account. However, if a PIN can change state — say from among valid, invalid, and suspended — then it should be a class</p></blockquote></li></ul><p>通过以上分析，我们可以得到ATM的核心类：FinancialTransaction, Account, BalanceInquiry, Withdrawal, Deposit, AuthorizeSystemInteraction, BankCard</p><h5 id="为核心类分配职责"><a href="#为核心类分配职责" class="headerlink" title="为核心类分配职责"></a><strong>为核心类分配职责</strong></h5><p>为每个核心类准备一个CRC卡，写上职责 Write responsibilities on CRC cards for each core class</p><p>为概念类定义协作者，此过程与定义职责的过程是交错进行的 Task of finding collaborators for classes often intermixed with finding responsibilities</p><p>集中在 What, 而不是 How</p><p>定义职责的方法</p><ul><li>首先，以“头脑风暴”为核心类列出各种可能的职责，然后再精化，</li><li>如果大多数职责都归属于少数几个类，则设计出了点问题<ul><li>没有充分利用多态、封装等特性</li><li>很多类的功能退化为”记录信息”，只知道自己的信息</li></ul></li></ul><blockquote><p>ATM example: class Account</p><p>A tendency might be to give most of the responsibility to the Account class, which becomes a strong, busy “manager” procedure giving commands to relatively weak, ill-defined “worker” classes</p><ul><li>这样Account这个类可重用性就差了</li></ul></blockquote><p>应该给与每个类一个明确的职责 Give each class a distinct role in the system. Strive to make each class a well-defined, complete, cohesive abstraction</p><blockquote><p>ATM example: class Withdrawal</p><p>the responsibility “Withdraw Funds”, making it potentially useful to any other class needing to do a withdrawal</p><ul><li>取钱应该是ATM例子中比较灵活的功能，如果设计的好，就能在多个地方重用 </li></ul></blockquote><p>Factoring out complexity (分解复杂性) also involves identifying specialized behaviors that occurs repeatedly and, as appropriate, spinning off(引出) new classes 如果有一件非常繁琐的事情，可以由几个类共同完成，也可以定义出新的类</p><blockquote><p>ATM example: The capturing and responding to user requests might be factored out into a new class ‘Form’ to ask user for information</p></blockquote><p>使用抽象 Use abstraction</p><ul><li>Build hierarchies of classes. Abstract the essence of related classes by identifying<ul><li>where they have common responsibilities</li><li>where they do the same thing, but do it differently<ul><li><strong>same “what”, different “how”</strong></li></ul></li><li>That is, look for opportunities for the classes to use <strong>polymorphism</strong> to implement the same responsibility differently</li><li>The new parent classes may be <strong>abstract classes</strong>. The abstract class exists to link together similar concrete types of objects</li></ul></li></ul><blockquote><p>ATM example: Create an abstract class <strong>Transaction</strong> that is a superclass for Withdrawal, Deposit, etc. It can have an abstract responsibility “execute a financial transaction”, that is implemented differently for each subclass</p></blockquote><h5 id="分配协作-Assigning-Collaborations"><a href="#分配协作-Assigning-Collaborations" class="headerlink" title="分配协作 Assigning Collaborations"></a><strong>分配协作 Assigning Collaborations</strong></h5><p>标识类之间的关系 Identify relationships among classes</p><ul><li>Each class is specialist in some set of knowledge and behaviors</li><li>Classes must cooperate to accomplish nontrivial tasks</li></ul><p>使用“基于场景”的角色扮演，发现/测试协作 Use scenariobased role-play to find and/or test these collaborations</p><ul><li><strong>Scenario is a system behavior and sequence of system events to realize it</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> Software Engineering </category>
          
          <category> Object-Oriented </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object-Oriented </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Objected-Oriented Analysis and Design-3</title>
      <link href="/2019/02/17/Object-Oriented-3/"/>
      <url>/2019/02/17/Object-Oriented-3/</url>
      
        <content type="html"><![CDATA[<p>本文为华东师范大学开设的面向对象分析与设计慕课课程第三周笔记，主要介绍了面向对象分析法的名词法与分析模型法，同时举了一些例子(本文略)来更好理解整个分析到设计的过程。</p><h3 id="Overview-of-Object-Oriented-analysis-and-design"><a href="#Overview-of-Object-Oriented-analysis-and-design" class="headerlink" title="Overview of Object-Oriented analysis and design "></a><strong>Overview of Object-Oriented analysis and design </strong></h3><h5 id="良好的对象设计意味着什么"><a href="#良好的对象设计意味着什么" class="headerlink" title="良好的对象设计意味着什么?"></a><strong>良好的对象设计意味着什么?</strong></h5><p>是构建高质量软件系统的基本要求，如</p><ul><li>Architectural cohesion 架构性的内聚</li><li>Reusability 可重用性</li><li>Maintenance 可维护性</li><li>Scalability 可扩展性</li><li>Flexibility 灵活性</li></ul><a id="more"></a><h5 id="为软件对象分配指责"><a href="#为软件对象分配指责" class="headerlink" title="为软件对象分配指责"></a><strong>为软件对象分配指责</strong></h5><ul><li>A critical ability in OO development is to skillfully <strong>assign responsibilities to software objects</strong>.<ul><li><strong>Responsibility-driven design 职责驱动的设计</strong></li><li>Performed during Design workflow 在设计的过程中完成职责分配</li><li>Follows <strong>GRASP</strong> principles 遵循GRASP原则<ul><li>General Responsibility Assignment Software Pattern 通用职责分配软件模式</li></ul></li><li>Successful designs become Patterns 成功的设计可以成为模式</li></ul></li></ul><h5 id="What-is-Analysis-什么是分析"><a href="#What-is-Analysis-什么是分析" class="headerlink" title="What is Analysis 什么是分析"></a><strong>What is Analysis 什么是分析</strong></h5><p>Analysis is investigation of the problem and requirements, rather than a<br>solution. 面对你所要解决的问题领域，搞清楚问题是什么，而不考虑怎么解决</p><h5 id="What-is-Design-什么是设计"><a href="#What-is-Design-什么是设计" class="headerlink" title="What is Design 什么是设计"></a><strong>What is Design 什么是设计</strong></h5><p>Design is a conceptual solution that fulfills the requirements. 概念性的、满足需求的解决方案</p><blockquote><p>分析与设计的关系：<strong>do the right thing (analysis), and do the thing right (design) 做正确的事情 (分析) ,正确地做事情 (设计)</strong> </p></blockquote><h5 id="What-is-Implement-什么是实现"><a href="#What-is-Implement-什么是实现" class="headerlink" title="What is Implement 什么是实现"></a><strong>What is Implement 什么是实现</strong></h5><p>Implementation is expression of the design in code.</p><h5 id="What-is-Deployment-什么是部署"><a href="#What-is-Deployment-什么是部署" class="headerlink" title="What is Deployment 什么是部署"></a><strong>What is Deployment 什么是部署</strong></h5><p>Deployment is the actual installation in the host environment.</p><h5 id="What-is-OOA-什么是面向对象分析"><a href="#What-is-OOA-什么是面向对象分析" class="headerlink" title="What is OOA 什么是面向对象分析"></a><strong>What is OOA 什么是面向对象分析</strong></h5><p>finding and describing the objects or concepts in the problem domain 发现并描述问题领域里的对象或者概念 (概念类)</p><h5 id="What-is-OOD-面向对象设计"><a href="#What-is-OOD-面向对象设计" class="headerlink" title="What is OOD 面向对象设计?"></a><strong>What is OOD 面向对象设计?</strong></h5><p>defining software objects and how they collaborate to fulfill the requirements 定义软件对象、以及它们之间如何协作完成功能的 (设计类)</p><blockquote><p>For example, Airplane example of object and class discovery</p><ul><li>OOA: in the case of the flight information system, some of the concepts include: Plane, Flight, and Pilot.</li><li>OOD: a Plane software object may have a tailNumber attribute and a getFlightHistory() method</li><li>then, implement</li></ul></blockquote><h3 id="OOAD-Simple-Example-OOAD的基本过程"><a href="#OOAD-Simple-Example-OOAD的基本过程" class="headerlink" title="OOAD Simple Example OOAD的基本过程"></a><strong>OOAD Simple Example OOAD的基本过程</strong></h3><p>以一个普通的扔骰子的游戏为例，游戏规则如下，玩家先后扔2个骰子，若两个骰子的点数和大于7则为胜利，反之为失败。</p><ul><li>定义用例</li><li><p>定义领域模型 (domain model)</p><ul><li><p>domain model: a visual representation of conceptual classed or real-situation objects in a domain. 问题领域的概念类以及真实对象的可视化表示</p></li><li><p>在这个例子中，领域模型为: Player, Dice, DiceGame</p><blockquote><p>领域模型图本质上是没有方法的类图</p></blockquote></li></ul></li><li><p>定义交互图</p><ul><li>Assignment of responsibilities among objects</li></ul></li><li><p>定义设计类图</p><ul><li>Software classes with methods according to responsibilities and attributes according to visibility</li></ul></li></ul><blockquote><p>LRG: lower representational gap 低表示差异</p><ul><li>领域模型和设计类图有很大的相似之处，在领域模型中找到的概念类在设计的时候可以直接拿来用</li><li>分析的结果是设计的输入</li></ul></blockquote><h3 id="面向对象分析法"><a href="#面向对象分析法" class="headerlink" title="面向对象分析法"></a><strong>面向对象分析法</strong></h3><p>在软件工程，分析是一种过程，把<strong>用户需求</strong>转变为<strong>系统需求</strong></p><p>大的、复杂系统的开发，有两种主要的分析方法</p><ul><li>Function-oriented analysis 面向功能的分析<ul><li>concentrating on the decomposition of complex functions to simply ones. </li></ul></li><li>Object-oriented analysis 面向对象分析<ul><li>identifying objects and the relationship between objects. </li></ul></li></ul><blockquote><ul><li>在抽象层面，面向功能的分析法用得多一点</li><li>在模块层面，面向对象分析法用得多一点</li></ul></blockquote><h5 id="面向对象分析主要步骤"><a href="#面向对象分析主要步骤" class="headerlink" title="面向对象分析主要步骤"></a><strong>面向对象分析主要步骤</strong></h5><ul><li>Identifying objects 识别对象</li><li>Organizing the objects 组织对象<ul><li>classifying the objects identified, so similar objects can later be defined in the<br>same class.</li></ul></li><li>Identifying relationships between objects 定义对象之间的关系<ul><li>this helps to determine inputs and outputs of an object.</li></ul></li><li>Defining operations of the objects 定义对象的操作<ul><li>the way of processing data within an object. (Also known as ‘responsibility assignment’)</li></ul></li><li><p>Defining objects internally 定义对象内部细节</p><h5 id="Three-ways-to-do-Object-Oriented-Analysis-如何寻找概念类"><a href="#Three-ways-to-do-Object-Oriented-Analysis-如何寻找概念类" class="headerlink" title="Three ways to do Object Oriented Analysis 如何寻找概念类"></a><strong>Three ways to do Object Oriented Analysis 如何寻找概念类</strong></h5></li><li><p>Conceptual model (Larman) 概念模型，又称”名词法”</p></li><li>Analysis model with stereotypes (Jacobson) 分析模型法<ul><li>从Boundaries, entities, control 三方面来寻找</li></ul></li><li>CRC cards (Beck, Cunningham) CRC (Class Responsibility Cooperation) 法</li></ul><h5 id="名词法定义概念类"><a href="#名词法定义概念类" class="headerlink" title="名词法定义概念类"></a><strong>名词法定义概念类</strong></h5><ul><li>重用或者修改已有的模型 Reuse or modify existing models.<ul><li>This is the first, best, and usually easiest approach</li></ul></li><li>借助行业、公司内部法的“概念类列表 ” <strong>Concept Category List</strong></li><li>在需求描述中查寻名词 (短语) Finding Concepts with <strong>Noun Phrase Identification</strong>.</li></ul><blockquote><p>A central distinction between object oriented and structures analysis: </p><ul><li><strong>前者根据对象划分系统，而后者根据功能 division by concepts (objects) rather than division by functions. </strong></li></ul></blockquote><h5 id="分析模型法-Analysis-model-with-stereotypes-构造型"><a href="#分析模型法-Analysis-model-with-stereotypes-构造型" class="headerlink" title="分析模型法 Analysis model with stereotypes(构造型) "></a><strong>分析模型法 Analysis model with stereotypes(构造型) </strong></h5><ul><li>用于描述系统规格说明</li><li>一个健壮、稳定的模型，必须与<strong>实现环境无关</strong></li><li>实现环境的任何变化，不会影响到系统的逻辑结构</li><li>分析模型能够关注到系统的信息、行为、展示（输入/出）等特性</li><li>表示符号<ul><li><img src="/2019/02/17/Object-Oriented-3/1.png" alt=""></li></ul></li></ul><p>The model is defined in information - behaviour - presentation space. </p><p><img src="/2019/02/17/Object-Oriented-3/2.png" alt=""></p><p>上述提到的Entity, Boundary/Interface, Control 在空间的位置如下图：</p><p><img src="/2019/02/17/Object-Oriented-3/3.png" alt=""></p><blockquote><ul><li>Presentation 相当于一个展示的窗口，和外面的事物打交道 (接口)</li></ul></blockquote><h3 id="面向对象设计-初步"><a href="#面向对象设计-初步" class="headerlink" title="面向对象设计(初步)"></a><strong>面向对象设计(初步)</strong></h3><h5 id="一般原则"><a href="#一般原则" class="headerlink" title="一般原则"></a><strong>一般原则</strong></h5><ul><li>An object-oriented system is <strong>composed of objects sending messages to other objects</strong></li><li>The quality of the overall design depends on which object is doing what<ul><li>比喻：“人尽其责、各有所长”</li></ul></li><li>That is, the quality depends on how we assign responsibilities to the objects</li></ul><h5 id="职责驱动的设计-RDD-Responsibility-Driven-Design"><a href="#职责驱动的设计-RDD-Responsibility-Driven-Design" class="headerlink" title="职责驱动的设计 (RDD Responsibility Driven Design)"></a><strong>职责驱动的设计 (RDD Responsibility Driven Design)</strong></h5><ul><li><p>Think of objects in terms of what they do or know (the human worker metaphor!) 设计时考虑对象做什么、或者知道什么</p></li><li><p>An object’s obligation or contract that it offers to other objects 一个对象对其他对象承担的义务或者合约</p></li><li><p>A responsibility is really a behavior the other classes depend on 职责是一个对象的行为，而其他的对象依赖这种行为</p></li></ul><blockquote><p>以职责为角度，提取对象的操作，属性</p></blockquote><h5 id="职责的定义"><a href="#职责的定义" class="headerlink" title="职责的定义"></a><strong>职责的定义</strong></h5><ul><li>认知职责 Knowing<ul><li>about private encapsulated data</li><li>about related objects</li><li>about things it can derive or calculate</li></ul></li><li>行为职责 Doing<ul><li>doing something itself</li><li>initiating action in other objects</li><li>controlling and coordinating activities in other objects</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Software Engineering </category>
          
          <category> Object-Oriented </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object-Oriented </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Objected-Oriented Analysis and Design-2</title>
      <link href="/2019/02/17/Object-Oriented-2/"/>
      <url>/2019/02/17/Object-Oriented-2/</url>
      
        <content type="html"><![CDATA[<p>本文为华东师范大学开设的面向对象分析与设计慕课课程第二周笔记，主要介绍了UML统一建模语言。</p><h3 id="UML-Unified-Modeling-Language-统一建模语言"><a href="#UML-Unified-Modeling-Language-统一建模语言" class="headerlink" title="UML (Unified Modeling Language) 统一建模语言"></a><strong>UML (Unified Modeling Language) 统一建模语言</strong></h3><p>由于客户和编程人员对需求理解的差异，且完整地理解一个复杂的系统很困难，所以需要建模。建模是为了能够更好地理解正在开发的系统。</p><a id="more"></a><h5 id="建模"><a href="#建模" class="headerlink" title="建模"></a><strong>建模</strong></h5><ul><li>所谓建模就是把不太理解的东西和一些已经较为理解、且十分类似的东西做比较，从而对这些不太理解的东西产生更深刻的理解</li></ul><h5 id="模型"><a href="#模型" class="headerlink" title="模型"></a><strong>模型</strong></h5><ul><li>建模产生的结果就是模型，模型是对现实的简化、对事物的一种抽象</li><li>模型可以帮助人们更好地了解事物的本质，抓住问题的要害</li><li>在模型中，人们总是剔除那些与问题无关的、非本质的东西，从而使模型与真实的实体相比更加简单、易于把握</li></ul><h5 id="建模的目的"><a href="#建模的目的" class="headerlink" title="建模的目的"></a><strong>建模的目的</strong></h5><ul><li>帮助我们按照需要对系统进行<strong>可视化</strong></li><li>允许我们详细<strong>说明系统的结构和行为</strong></li><li>给出了一个指导我们<strong>构造系统</strong>的模板</li><li>对我们所做出的决策进行<strong>文档化</strong></li></ul><h5 id="建模的四项基本原理"><a href="#建模的四项基本原理" class="headerlink" title="建模的四项基本原理"></a><strong>建模的四项基本原理</strong></h5><ul><li><p>选择要创建什么模型，不同的模型会有不同的启发</p></li><li><p>每一种模型可以在不同的精度级别上表示</p></li><li><p>A model is an abstraction of the real world. Good models are still connected with reality. 最好的模型都是与现实相关联的</p><ul><li>模型都是对现实的简化，但是简化不能掩盖掉任何重要的细节</li></ul></li><li><p>单个模型是不充分的, 对每一个重要的系统最好用一组几乎独立的模型去处理。</p></li></ul><h5 id="UML统一建模语言"><a href="#UML统一建模语言" class="headerlink" title="UML统一建模语言"></a><strong>UML统一建模语言</strong></h5><ul><li>“事实上的”工业标准，相当于软件工程师的工具包</li><li><strong>UML的构造块 (记住它们的常用符号)</strong><ul><li>事物（结构事物，行为事物，分组事物，注释事物）</li><li>关系（依赖，关联，泛化，实现）</li><li>图（<strong>类图</strong>，对象图，<strong>顺序图</strong>，通信图，构建图，<strong>活动图</strong>，包图，<strong>用例图</strong>，<strong>状态图</strong>，部署图）</li></ul></li><li>UML公有机制<ul><li>详述</li><li><strong>修饰</strong></li><li>通用划分</li><li>扩展机制<ul><li><strong>构造型 (用于自定义)</strong>，标记值，约束</li></ul></li></ul></li></ul><h3 id="UseCase-Diagram-用例图"><a href="#UseCase-Diagram-用例图" class="headerlink" title="UseCase Diagram 用例图"></a><strong>UseCase Diagram 用例图</strong></h3><p>用例图的建模元素有边界，参与者，用例，关系</p><h5 id="Actor-参与者"><a href="#Actor-参与者" class="headerlink" title="Actor 参与者"></a><strong>Actor 参与者</strong></h5><ul><li>代表位于系统之外并和系统进行交互的一类事物（人、物、<strong>其他软件子系统</strong>等）</li><li>通过它，可以对软件系统与外界发生的交互进行分析和描述</li><li>通过它，可以了解客户希望软件系统提供哪些功能</li></ul><blockquote><h5 id="如何确定参与者"><a href="#如何确定参与者" class="headerlink" title="如何确定参与者"></a><strong>如何确定参与者</strong></h5><ul><li>谁使用系统？Who or what uses the system</li><li>谁安装系统、维护系统？Who installs the system? Who maintains the system</li><li>谁启动系统、关闭系统？Who starts and stops the system</li><li>谁从系统中获取信息，谁提供信息给系统？Who gets and provides information to the system</li><li>在系统交互中，谁扮演了什么角色？What roles do they play in the interaction</li><li>系统会与哪些其他系统相关联？What other systems interact with this system</li><li>内/外部定时器 Does anything happen at a fixed time?</li></ul></blockquote><p>需要注意的是，Actor是某个系统时，应使用构造型《actor》</p><h5 id="UseCase-用例"><a href="#UseCase-用例" class="headerlink" title="UseCase 用例"></a><strong>UseCase 用例</strong></h5><ul><li>系统为响应参与者引发的一个事件而执行的一系列的处理/动作，而这些处理应该为参与者产生一种有价值的结果，这些动作不但应包含<strong>正常情况</strong>的各种动作序列，而且应包含对<strong>非正常情况</strong>时软件系统的动作序列的描述。<strong>用例名称一般用短小精悍的“动名词”</strong></li></ul><blockquote><h5 id="寻找用例"><a href="#寻找用例" class="headerlink" title="寻找用例"></a><strong>寻找用例</strong></h5><ul><li>参与者希望系统提供什么功能 Start with actors, then identify what they want to do What functions will the actor want from the system ?</li><li>系统是否存储和检索信息</li><li>当系统改变状态时，是否通知参与者 Are any actors notified when the system changes ?</li><li>是否存在影响系统的外部事件，是哪个参与者通知系统这些外部事件 Are there external events that notify the system ?</li><li>哪个参与者触发了活动？Which actors trigger activity ?</li></ul></blockquote><h5 id="用例图中的关系"><a href="#用例图中的关系" class="headerlink" title="用例图中的关系"></a><strong>用例图中的关系</strong></h5><ul><li>参与者与用例之间<ul><li>关联关系</li></ul></li><li>参与者与参与者之间<ul><li>泛化关系</li></ul></li><li>用例之间的关系<ul><li>泛化关系<ul><li>如下订单和网上下订单</li></ul></li><li>包含关系 include <ul><li>如取钱用例的输入密码，因为你做任何事情都需要输入密码</li></ul></li><li>扩展关系 extend <ul><li>如取钱用例的打印单据，你可以选择在取钱以后是否打印单据</li></ul></li></ul></li></ul><blockquote><p><strong>用例注意事项</strong></p><ul><li>用例步骤最好不要有涉及到具体界面的描述，这样的话，适用面会比较广。我们学习分析、设计，目的是应对所要开发的系统的不断演变、需求的不断更改。</li><li>合适的粒度</li><li>用例图中，用例与用例之间，只有三种关系： 泛化、包含、扩展。千万不要随随便便地在用例与用例之间画一条直线 (关联关系) ！这样做的话，就变成“糖葫芦串”了。</li></ul></blockquote><h5 id="UseCase-description-用例描述"><a href="#UseCase-description-用例描述" class="headerlink" title="UseCase description 用例描述"></a><strong>UseCase description 用例描述</strong></h5><p>一般由一个主事件流和多个异常事件流描述</p><ul><li>主事件流：一切正常时的动作序列</li><li>异常事件流或可选事件流流：主事件流的每一步都有可能出现异常，此处描述异常情况的处理</li></ul><h3 id="Activity-Diagram-活动图"><a href="#Activity-Diagram-活动图" class="headerlink" title="Activity Diagram 活动图"></a><strong>Activity Diagram 活动图</strong></h3><p>活动图描述了在一个过程中，顺序的/并行的<strong>活动</strong>及其之间的关系，一般用于业务过程，复杂算法建模。</p><p>活动图是<strong>顶点</strong>和<strong>弧</strong>的集合，包括活动节点，动作，流，对象值，注解和约束等</p><h5 id="开始、结束、对象"><a href="#开始、结束、对象" class="headerlink" title="开始、结束、对象"></a><strong>开始、结束、对象</strong></h5><p><img src="/2019/02/17/Object-Oriented-2/1.png" alt=""></p><h5 id="活动节点"><a href="#活动节点" class="headerlink" title="活动节点"></a><strong>活动节点</strong></h5><ul><li>一个活动是一个过程中进行的非原子的执行单元</li><li>活动的执行最终延伸为一些独立<strong>动作 (Action) </strong>的执行</li></ul><h5 id="分支"><a href="#分支" class="headerlink" title="分支"></a><strong>分支</strong></h5><ul><li>一个分支可以有一个<strong>进入流</strong>和<strong>多个离去流</strong></li><li>在每个离去流上必须设置一个监护条件<ul><li>条件放在方括号里</li><li>条件不能重叠，以免二义性</li><li>可以有 [else] 分支</li></ul></li><li>两个控制路径可以重新合并，无需监护条件</li></ul><h5 id="Forking-and-Joining-分岔和汇合"><a href="#Forking-and-Joining-分岔和汇合" class="headerlink" title="Forking and Joining 分岔和汇合"></a><strong>Forking and Joining 分岔和汇合</strong></h5><ul><li>分岔表示把一个单独的控制流分成两个或多个并发的控制流</li><li>汇合表示两个或多个并发控制流的同步发生，一个汇合可以有两个或多个进入转移和一个输出转移在UML中，用同步棒来说明并行控制流的分岔和汇合</li></ul><h5 id="Swimlanes-泳道"><a href="#Swimlanes-泳道" class="headerlink" title="Swimlanes 泳道"></a><strong>Swimlanes 泳道</strong></h5><ul><li>将一个活动图中的活动分组，每一组表示一个特定的类别、人或部门，他们负责完成组内的活动</li><li>每个组被称为一个泳道</li><li>每个活动严格地属于一个用到，同步棒可以跨越泳道</li></ul><p><strong>活动图与用例模型互为补充，主要用于需求分析阶段</strong></p><h3 id="Class-Diagram-类图"><a href="#Class-Diagram-类图" class="headerlink" title="Class Diagram 类图"></a><strong>Class Diagram 类图</strong></h3><h5 id="Class-类"><a href="#Class-类" class="headerlink" title="Class 类"></a><strong>Class 类</strong></h5><ul><li><strong>具有相同属性、操作、方法、关系或者行为的一组对象的描述符</strong></li><li>类是真实世界事物的抽象</li><li>问题领域的类：在对系统建模时，将会涉及到如何识别业务系统中的事物，这些事物构成了整个业务系统。在UML中，把所有的这些<strong>事物</strong>都建模为<strong>类</strong> (class)</li></ul><h5 id="Object-对象"><a href="#Object-对象" class="headerlink" title="Object 对象"></a><strong>Object 对象</strong></h5><ul><li>当这些事物存在于真实世界中时，它们是类的实例，并被称为对象</li><li>同一个类的各对象具有相同的属性，<strong>但属性的取值可以不同</strong>，提供相同的操作、有相同的语义</li></ul><h5 id="类图中的UML元素"><a href="#类图中的UML元素" class="headerlink" title="类图中的UML元素"></a><strong>类图中的UML元素</strong></h5><ul><li>类之间的关系：关联关系，依赖关系，泛化关系，实现关系<ul><li>关联的修饰<ul><li>名称及其方向</li><li>关联关系的多重性</li><li>角色</li></ul></li></ul></li><li>类：内容包括名称，属性，操作，职责</li><li>关联类，Association class is an association that is also a class, and consists of the class, association and the dashed line</li></ul><h3 id="Sequence-Diagram-顺序图"><a href="#Sequence-Diagram-顺序图" class="headerlink" title="Sequence Diagram 顺序图"></a><strong>Sequence Diagram 顺序图</strong></h3><p>一种详细描述对象之间以及对象与参与者之间交互的图，它由一组相互协作的对象或参与者实例以及它们之间发送的消息组成，<strong>强调消息之间的顺序</strong>，可用于动态验证模型的可行性。顺序图验证的某一功能，属于某个用例描述的功能中的一部分 (称为用例实现)。</p><h5 id="顺序图的构成"><a href="#顺序图的构成" class="headerlink" title="顺序图的构成"></a><strong>顺序图的构成</strong></h5><p>参与者，对象生命线，执行规约，消息</p><h5 id="对象生命线"><a href="#对象生命线" class="headerlink" title="对象生命线"></a><strong>对象生命线</strong></h5><ul><li>表示对象在一段时间的存在</li></ul><h5 id="执行规约-或称控制焦点"><a href="#执行规约-或称控制焦点" class="headerlink" title="执行规约 (或称控制焦点)"></a><strong>执行规约 (或称控制焦点)</strong></h5><ul><li>执行规约 (execution specification) 是一个对象执行一个操作的时期</li></ul><h5 id="消息"><a href="#消息" class="headerlink" title="消息"></a><strong>消息</strong></h5><ul><li>对象间的协作与交流表现为一个对象以某种方式启动另一个对象的活动，这种交流在<br>UML里被定义为消息</li><li>同步消息：如有对象A与对象B，A给B发送同步消息，要求对象B处理好消息并且返回结果，对象A才会继续往下</li><li>异步消息：如有对象A与对象B，A给B发送同步消息，这时候A是将消息发送到B的队列里面，接着A继续做自己的事情，等到有空的时候，就去看看自己的队列中有没有消息，如果有就把这个请求拿出来处理，如果没有就继续做自己的事情。B也是如此，当B闲下来后发现A发来了消息，那么它就会处理并返回给A结果，A在它空闲时查看队列就会得到返回结果，过程如下图：<ul><li><img src="/2019/02/17/Object-Oriented-2/2.png" alt=""></li></ul></li></ul><h5 id="结构化控制"><a href="#结构化控制" class="headerlink" title="结构化控制"></a><strong>结构化控制</strong></h5><p>在顺序图中，除了按顺序排列消息外，还应表示对消息进行选择、循环和并行处理</p><blockquote><p><strong>类图与顺序图、类图与用例模型之间的关系</strong></p><ul><li>用例是基础，是软件的需求</li><li>类图是在用例的基础上做的设计，是设计方案</li><li>顺序图是对设计方案的可行性进行验证的手段</li></ul></blockquote><h3 id="Communication-Diagram-通信图"><a href="#Communication-Diagram-通信图" class="headerlink" title="Communication Diagram 通信图"></a><strong>Communication Diagram 通信图</strong></h3><p>顺序图与通信图本质上是一样的，在语义上是等价的。但建模的角度不同，<strong>前者反映了对象之间协作的时间顺序，后者反映了对象之间协作的结构关系</strong>，强调了对象之间的交流。</p><h5 id="通信图的构成"><a href="#通信图的构成" class="headerlink" title="通信图的构成"></a><strong>通信图的构成</strong></h5><p>对象，链接，在链接上的消息</p><h3 id="State-Diagram-状态图"><a href="#State-Diagram-状态图" class="headerlink" title="State Diagram 状态图"></a><strong>State Diagram 状态图</strong></h3><p>描述了<strong>单个对象</strong>在其生命周期内响应事件所经历的状态序列以及<strong>动态行为</strong></p><h5 id="State-machine状态机"><a href="#State-machine状态机" class="headerlink" title="State machine状态机"></a><strong>State machine状态机</strong></h5><ul><li>是一种行为，说明对象在它的生命期中, 响应事件所经历的<strong>状态序列</strong>以及它们<strong>对每个事件的响应</strong></li></ul><h5 id="State-Diagram-状态图-1"><a href="#State-Diagram-状态图-1" class="headerlink" title="State Diagram 状态图"></a><strong>State Diagram 状态图</strong></h5><ul><li>状态机可以用状态图来可视化，状态图显示了一个状态机，它强调从状态到状态的控制流</li></ul><h5 id="State-状态"><a href="#State-状态" class="headerlink" title="State 状态"></a><strong>State 状态</strong></h5><ul><li>是对象的生命期中的一个条件或状况，在此期间，对象可以响应事件、执行某活动等</li><li>状态由以下几个部分组成<ul><li>名称</li><li>进入/退出动作 (entry/exit action)</li><li>内部迁移 (internal transition)</li><li>子状态</li><li>延迟事件 (deferred event)</li></ul></li></ul><h5 id="Event-事件"><a href="#Event-事件" class="headerlink" title="Event 事件"></a><strong>Event 事件</strong></h5><ul><li>是对一个在时间和空间上占有一定位置的有意义的事情的描述<ul><li><strong>在状态机的语境中，一个事件是一个激励的发生，它能够触发一个状态迁移</strong></li></ul></li><li>UML对四种事件进行建模<ul><li>change event 参量变化</li><li>signal 信号 (异步)</li><li>call 调用 (同步)</li><li>时间事件 </li></ul></li></ul><h5 id="Transition-迁移"><a href="#Transition-迁移" class="headerlink" title="Transition 迁移"></a><strong>Transition 迁移</strong></h5><ul><li>在状态A，发生事件并满足一定条件，转到状态B。一个迁移由5部分组成：<ul><li>source state 源状态</li><li>event trigger 事件触发器</li><li>guard condition 触发条件</li><li>effect 效应 (或称迁移动作)</li><li>目标状态</li></ul></li><li>特殊的迁移<ul><li>self transition 自身迁移</li><li>internal transition 内部迁移</li></ul></li></ul><blockquote><p><strong>状态图建模注意事项</strong></p><ul><li>不允许孤立的状态存在</li><li>不允许只进不出的状态迁移</li><li>不允许只出不进的状态迁移</li><li><strong>不允许没有事件发生的迁移</strong></li></ul></blockquote><h5 id="状态图、交互图-顺序图与通信图-和活动图的比较"><a href="#状态图、交互图-顺序图与通信图-和活动图的比较" class="headerlink" title="状态图、交互图 (顺序图与通信图) 和活动图的比较"></a><strong>状态图、交互图 (顺序图与通信图) 和活动图的比较</strong></h5><ul><li>交互<ul><li>对共同工作的<strong>对象群体</strong>的行为建模</li><li><strong>动态行为</strong></li></ul></li><li>状态机<ul><li>对<strong>单个对象</strong>的行为建模<ul><li>有时，可以对单个“完整系统”的行为建模</li><li>说明对象在它的生命期中响应事件所经历的状态序列以及对那些事件的响应</li></ul></li><li><strong>动态行为</strong>建模</li></ul></li><li>活动图<ul><li><strong>强调从活动到活动的控制流，多个业务角色，在需求分析时使用</strong></li><li><strong>状态图是强调对象潜在的状态和这些状态之间的迁移</strong></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Software Engineering </category>
          
          <category> Object-Oriented </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object-Oriented </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Objected-Oriented Analysis and Design-1</title>
      <link href="/2019/02/16/Object-Oriented-1/"/>
      <url>/2019/02/16/Object-Oriented-1/</url>
      
        <content type="html"><![CDATA[<p>本文为华东师范大学开设的面向对象分析与设计慕课课程第一周笔记，主要讲了面向对象方法学，面向对象的起源以及面向对象的相关概念与特征。</p><h3 id="面向对象分析与设计"><a href="#面向对象分析与设计" class="headerlink" title="面向对象分析与设计"></a><strong>面向对象分析与设计</strong></h3><p>面向对象之父——艾伦.C.凯</p><blockquote><p>启发来源: </p><ul><li>生物学上，细胞信息的传递方式</li><li>传送数据时，将数据以及对数据的处理方法一起打包</li></ul></blockquote><a id="more"></a><h3 id="类与对象"><a href="#类与对象" class="headerlink" title="类与对象"></a><strong>类与对象</strong></h3><h5 id="Class-类"><a href="#Class-类" class="headerlink" title="Class 类"></a><strong>Class 类</strong></h5><p>A <strong>class</strong> is a description (描述符) of a set of <strong>objects</strong> that share the same attributes, operations, relationships, and semantics 类是共享相同属性、操作、方法、关系或行为的一组对象的<strong>描述符</strong><br></p><h5 id="Object-对象"><a href="#Object-对象" class="headerlink" title="Object 对象"></a><strong>Object 对象</strong></h5><p>An object is an <strong>instance</strong> created from a class 一个对象是根据一个类创建的一个实例 <br></p><blockquote><p>An instance’s <strong>behaviour</strong> and <strong>information structure</strong> is defined in the class.<br>Its current <strong>state</strong> (values of instance variables) is determined by operations performed on it.</p></blockquote><h5 id="Message-消息"><a href="#Message-消息" class="headerlink" title="Message 消息"></a><strong>Message 消息</strong></h5><p>对象之间的一种交流手段 <br></p><blockquote><p>All objects of a paricular type can receive the same messages.</p></blockquote><h3 id="面向对象的思考方式"><a href="#面向对象的思考方式" class="headerlink" title="面向对象的思考方式"></a><strong>面向对象的思考方式</strong></h3><h5 id="面向对象"><a href="#面向对象" class="headerlink" title="面向对象"></a><strong>面向对象</strong></h5><p>适合解决不确定的事件、创新性的事件</p><h5 id="面向过程"><a href="#面向过程" class="headerlink" title="面向过程"></a><strong>面向过程</strong></h5><p>处理已知的事实、重要的条件都已知的场景</p><h3 id="面向对象的核心特征"><a href="#面向对象的核心特征" class="headerlink" title="面向对象的核心特征"></a><strong>面向对象的核心特征</strong></h3><h5 id="Encapsulation-封装"><a href="#Encapsulation-封装" class="headerlink" title="Encapsulation 封装"></a><strong>Encapsulation 封装</strong></h5><ul><li>Encapsulation is the process of hiding the implementation details of an object 隐藏了对象的实现细节</li><li>The internal state is usually not accessible by other objects 内部的状态不为其他对象所访问</li><li>The only access to manipulate the object data is through its interface 对象的数据只能通过接口进行访问</li><li>Encapsulation allows objects to be viewed as ‘black boxes’ 封装使得对象可以被看成一个“黑盒子”</li><li>It protects an object’s internal state from being corrupted by other objects. 保护数据 </li><li>Also, other objects are protected from changes in the objectimplementation. 一个对象实现方法的改变，不影响其他相关对象</li><li>Communication is achieved through an ‘interface’ 对象间通过“接口”进行通信</li></ul><blockquote><p>为什么封装？</p><ul><li>保护隐私</li><li>保护数据安全</li><li>隔离复杂度</li></ul><p>封装原则</p><ul><li>An object should only reveal the interfaces needed to interact with it. Details not pertinent to the use of the object should be hidden from other objects.  只公开必须公开的属性</li><li>使用 Getters and Setters 来实现信息隐藏</li></ul></blockquote><h5 id="Inheritance-继承"><a href="#Inheritance-继承" class="headerlink" title="Inheritance 继承"></a><strong>Inheritance 继承</strong></h5><ul><li>A class gets the state and behavior of another class and adds additional state and behavior 一个类从其他的类里面获取它的状态和行为，同时加上自己的一些额外的状态和行为</li></ul><h5 id="Ploymorphism-多态"><a href="#Ploymorphism-多态" class="headerlink" title="Ploymorphism 多态"></a><strong>Ploymorphism 多态</strong></h5><ul><li>When one class inherits from another, then polymorphism allows a subclass to stand in for the superclass. 当一个类从另一个类继承而来，多态使得子类可以代替父类</li><li>The sender of a stimulus doesn’t need to know the receiver’s class. 消息发送方不需要知道消息接收方属于哪个子类</li><li>Different receivers can interpret the message in their own way. <strong>同一类族</strong>的接收者可以按自己的方式处理消息 </li><li><strong>使用指向父类的指针或者引用，能够调用子类的对象</strong>，这是多态的核心思想，也是设计模式的基础</li></ul><h5 id="Aggregation-amp-Composition-聚合-amp-组合"><a href="#Aggregation-amp-Composition-聚合-amp-组合" class="headerlink" title="Aggregation &amp; Composition 聚合&amp;组合"></a><strong>Aggregation &amp; Composition 聚合&amp;组合</strong></h5><ul><li>Aggregation describes a “has a” relationship. One object is a part of another object. XX有XX这种关系</li></ul><blockquote><ul><li>Aggregation relationships are transitive. 聚合有传递性<ul><li>if A contains B an B contains C, then A contains C</li></ul></li><li>Aggregation relationships are asymmetric. 聚合是不对称的<ul><li>If A contains B, then B does not contain A</li></ul></li></ul></blockquote><ul><li>A variant of aggregation is <strong>composition</strong> which adds the property of existence dependency. 组合是聚合的一个变种<ul><li>组合特别强调<strong>整体控制着部分的生命</strong>，比如手指和手掌</li></ul></li><li>但有些时候，聚合的组合的关系不是很明确，含糊不清的话一般用聚合</li></ul><h5 id="Interface-amp-Implementation-接口-amp-实现"><a href="#Interface-amp-Implementation-接口-amp-实现" class="headerlink" title="Interface &amp; Implementation 接口&amp;实现"></a><strong>Interface &amp; Implementation 接口&amp;实现</strong></h5><ul><li>Interface describes how users of the class interact with the class 描述一个类的用户如何与这个类交互<ul><li>使得当对其中某一个类进行局部修改的时候，不影响其他的类</li></ul></li><li>实现即完成接口所定义的功能</li></ul><h5 id="Abstraction-抽象"><a href="#Abstraction-抽象" class="headerlink" title="Abstraction 抽象"></a><strong>Abstraction 抽象</strong></h5><ul><li>A process allowing to focus on most important aspects while ignoring less important details. 简单地说，抽象就是专注于最重要的部分而忽略掉那些不太重要部分的过程</li><li><strong>抽象是面向对象领域发现类的主要方法</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> Software Engineering </category>
          
          <category> Object-Oriented </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object-Oriented </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Neural Networks-2</title>
      <link href="/2019/02/08/Deep-Neural-Networks-2/"/>
      <url>/2019/02/08/Deep-Neural-Networks-2/</url>
      
        <content type="html"><![CDATA[<p>本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第四周深层神经网络的相关课程及作业。</p><h3 id="Building-blocks-of-deep-neural-networks"><a href="#Building-blocks-of-deep-neural-networks" class="headerlink" title="Building blocks of deep neural networks"></a><strong>Building blocks of deep neural networks</strong></h3><p>本节，我们将用网络块来深入理解正向传播和反向传播的过程，由于之前部分已经详细解释了正向传播和反向传播，这里不再详述，本节只是起到补充说明加深理解的作用。</p><a id="more"></a><p><img src="/2019/02/08/Deep-Neural-Networks-2/1.png" alt=""></p><p>上图展示了神经网络正向传播和反向传播中数据的传递的整个过程。顺便提一下，在正向传播中，我们需要缓存 $z^{[l]}$ ，因为在反向传播中会用到。</p><h3 id="Forward-and-backward-propagation"><a href="#Forward-and-backward-propagation" class="headerlink" title="Forward and backward propagation"></a><strong>Forward and backward propagation</strong></h3><p>总结一下，如何在深层神经网络中实现前向传播和反向传播</p><h5 id="Forward-propagation-for-layer-l"><a href="#Forward-propagation-for-layer-l" class="headerlink" title="Forward propagation for layer $l$"></a>Forward propagation for layer $l$</h5><blockquote><p>Input $a^{[l-1]}$</p><p>Output $a^{[l]}$, cache $(z^{[l]})$</p><p>Step：</p><script type="math/tex; mode=display">z^{[l]} = W^{[l]} \cdot a^{[l-1]}+b^{[l]}</script><script type="math/tex; mode=display">a^{[l]} = g^{[l]}(z^{[l]})</script><p>After vectorization：</p><script type="math/tex; mode=display">Z^{[l]} = W^{[l]}A^{[l-1]}+b^{[l]}</script><script type="math/tex; mode=display">A^{[l]} = g^{[l]}(Z^{[l]})</script></blockquote><h5 id="Backward-propagation-for-layer-l"><a href="#Backward-propagation-for-layer-l" class="headerlink" title="Backward propagation for layer $l$"></a>Backward propagation for layer $l$</h5><blockquote><p>Input $da^{[l]}$</p><p>Output $da^{[l-1]}, dW^{[l]}, db^{[l]}$</p><p>Step：</p><script type="math/tex; mode=display">dz^{[l]} = da^{[l]} * g^{[l]'}(z^{[l]})</script><script type="math/tex; mode=display">dW^{[l]} = dz^{[l]} \cdot a^{[l-1]}</script><script type="math/tex; mode=display">db^{[l]} = dz^{[l]}</script><script type="math/tex; mode=display">da^{[l-1]} = W^{[l]T} \cdot dz^{[l]}</script><p>After vectorization：</p><script type="math/tex; mode=display">dZ^{[l]} = dA^{[l]} * g^{[l]'}(Z^{[l]})</script><script type="math/tex; mode=display">dW^{[l]} = \frac{1}{m}dZ^{[l]}A^{[l-1]T}</script><script type="math/tex; mode=display">db^{[l]} = \frac{1}{m}np.sum(dZ^{[l]},axis=1,keepdims=True)</script><script type="math/tex; mode=display">dA^{[l-1]} = W^{[l]T} \cdot dZ^{[l]}</script></blockquote><h3 id="Parameters-vs-Hyperparameters"><a href="#Parameters-vs-Hyperparameters" class="headerlink" title="Parameters vs. Hyperparameters"></a><strong>Parameters vs. Hyperparameters</strong></h3><p>在神经网络中，参数有：$W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]} …​$ ，超参数有：学习率 (learning rate) $\alpha​$ ，梯度下降迭代次数 (iteration) ，隐藏层 (hidden layer) 个数， 隐藏层神经元个数 $n^{[1]}, n^{[2]} …​$ ，激活函数的选择等等。这些超参数控制了最后参数的变化，因此称为超参数，在接下来的课程会深入探讨。</p><p>选择最优的超参数常常是困难的，它需要靠我们的经验，以及一次次的尝试，从而获得更好的参数来训练出更优的模型。</p><h3 id="Homework-Building-your-Deep-Neural-Network"><a href="#Homework-Building-your-Deep-Neural-Network" class="headerlink" title="Homework-Building your Deep Neural Network"></a><strong>Homework-Building your Deep Neural Network</strong></h3><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> h5py</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">from</span> testCases_v3 <span class="hljs-keyword">import</span> *</span><br><span class="line"><span class="hljs-keyword">from</span> dnn_utils_v2 <span class="hljs-keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># initialize parameters 初始化一个 2-layer 神经网络</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize_parameters</span><span class="hljs-params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Argument:</span></span><br><span class="line"><span class="hljs-string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="hljs-string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="hljs-string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="hljs-string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="hljs-string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="hljs-string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="hljs-string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="hljs-number">1</span>)</span><br><span class="line"></span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * <span class="hljs-number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="hljs-number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * <span class="hljs-number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="hljs-number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">assert</span> (W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="hljs-keyword">assert</span> (b1.shape == (n_h, <span class="hljs-number">1</span>))</span><br><span class="line">    <span class="hljs-keyword">assert</span> (W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="hljs-keyword">assert</span> (b2.shape == (n_y, <span class="hljs-number">1</span>))</span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="hljs-string">"W1"</span>: W1,</span><br><span class="line">                  <span class="hljs-string">"b1"</span>: b1,</span><br><span class="line">                  <span class="hljs-string">"W2"</span>: W2,</span><br><span class="line">                  <span class="hljs-string">"b2"</span>: b2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># initialize parameters 初始化一个 l-layer 神经网络</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize_parameters_deep</span><span class="hljs-params">(layer_dims)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    layer_dims -- python array (list) containing the dimensions of each layer in our network</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="hljs-string">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="hljs-string">                    bl -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="hljs-number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layer_dims)  <span class="hljs-comment"># number of layers in the network</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, L):</span><br><span class="line">        parameters[<span class="hljs-string">'W'</span> + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - <span class="hljs-number">1</span>]) * <span class="hljs-number">0.01</span></span><br><span class="line">        parameters[<span class="hljs-string">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class="hljs-number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">assert</span> (parameters[<span class="hljs-string">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l - <span class="hljs-number">1</span>]))</span><br><span class="line">        <span class="hljs-keyword">assert</span> (parameters[<span class="hljs-string">'b'</span> + str(l)].shape == (layer_dims[l], <span class="hljs-number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 前向传播，仅仅计算 Z</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">linear_forward</span><span class="hljs-params">(A, W, b)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implement the linear part of a layer's forward propagation.</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    A -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="hljs-string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="hljs-string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    Z -- the input of the activation function, also called pre-activation parameter</span></span><br><span class="line"><span class="hljs-string">    cache -- a python dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    Z = np.dot(W, A) + b</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">assert</span> (Z.shape == (W.shape[<span class="hljs-number">0</span>], A.shape[<span class="hljs-number">1</span>]))</span><br><span class="line">    cache = (A, W, b)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> Z, cache</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 对于某一层前向传播的整个过程</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">linear_activation_forward</span><span class="hljs-params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="hljs-string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="hljs-string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="hljs-string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    A -- the output of the activation function, also called the post-activation value</span></span><br><span class="line"><span class="hljs-string">    cache -- a python dictionary containing "linear_cache" and "activation_cache";</span></span><br><span class="line"><span class="hljs-string">             stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">if</span> activation == <span class="hljs-string">"sigmoid"</span>:</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">elif</span> activation == <span class="hljs-string">"relu"</span>:</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">assert</span> (A.shape == (W.shape[<span class="hljs-number">0</span>], A_prev.shape[<span class="hljs-number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> A, cache</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 整个模型的前向传播整个过程，对于 l-layer 神经网络</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">L_model_forward</span><span class="hljs-params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    X -- data, numpy array of shape (input size, number of examples)</span></span><br><span class="line"><span class="hljs-string">    parameters -- output of initialize_parameters_deep()</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    AL -- last post-activation value</span></span><br><span class="line"><span class="hljs-string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="hljs-string">                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</span></span><br><span class="line"><span class="hljs-string">                the cache of linear_sigmoid_forward() (there is one, indexed L-1)</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    A = X</span><br><span class="line">    L = len(parameters) // <span class="hljs-number">2</span>  <span class="hljs-comment"># number of layers in the neural network 因为同时有w,b两个参数，所以除2</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 循环调用，从第1层到第L-1层</span></span><br><span class="line">    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, L):</span><br><span class="line">        A_prev = A</span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="hljs-string">'W'</span> + str(l)], parameters[<span class="hljs-string">'b'</span> + str(l)],</span><br><span class="line">                                             activation=<span class="hljs-string">"relu"</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 第L层使用sigmoid函数</span></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="hljs-string">'W'</span> + str(L)], parameters[<span class="hljs-string">'b'</span> + str(L)], activation=<span class="hljs-string">"sigmoid"</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">assert</span> (AL.shape == (<span class="hljs-number">1</span>, X.shape[<span class="hljs-number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> AL, caches</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 计算成本函数</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_cost</span><span class="hljs-params">(AL, Y)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implement the cost function defined by equation (7).</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span><br><span class="line"><span class="hljs-string">    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    cost -- cross-entropy cost</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    m = Y.shape[<span class="hljs-number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 注意这里np.log(AL).T需要转置,因为Y和AL都是列向量</span></span><br><span class="line">    cost = (<span class="hljs-number">-1.</span> / m) * (np.dot(Y, np.log(AL).T) - np.dot(<span class="hljs-number">1</span> - Y, np.log(<span class="hljs-number">1</span> - AL).T))</span><br><span class="line"></span><br><span class="line">    cost = np.squeeze(cost)  <span class="hljs-comment"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span></span><br><span class="line">    <span class="hljs-keyword">assert</span> (cost.shape == ())</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 反向传播，仅仅是对于线性函数z = w.T * x + b</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">linear_backward</span><span class="hljs-params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implement the linear portion of backward propagation for a single layer (layer l)</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span><br><span class="line"><span class="hljs-string">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="hljs-string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="hljs-string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="hljs-number">1</span>]</span><br><span class="line"></span><br><span class="line">    dW = <span class="hljs-number">1.</span> / m * np.dot(dZ, A_prev.T)</span><br><span class="line">    db = <span class="hljs-number">1.</span> / m * np.sum(dZ, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-keyword">True</span>)</span><br><span class="line">    dA_prev = np.dot(W.T, dZ)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="hljs-keyword">assert</span> (dW.shape == W.shape)</span><br><span class="line">    <span class="hljs-keyword">assert</span> (db.shape == b.shape)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> dA_prev, dW, db</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 对于每一层整个反向传播过程，其中对于dZ = dA * g'(Z)的计算已经有函数relu_backward,sigmoid_backward帮你实现了</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">linear_activation_backward</span><span class="hljs-params">(dA, cache, activation)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    dA -- post-activation gradient for current layer l</span></span><br><span class="line"><span class="hljs-string">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="hljs-string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="hljs-string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="hljs-string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">if</span> activation == <span class="hljs-string">"relu"</span>:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">elif</span> activation == <span class="hljs-string">"sigmoid"</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> dA_prev, dW, db</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 整个模型的反向传播过程，对于 l-layer 神经网络</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">L_model_backward</span><span class="hljs-params">(AL, Y, caches)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    AL -- probability vector, output of the forward propagation (L_model_forward())</span></span><br><span class="line"><span class="hljs-string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat)</span></span><br><span class="line"><span class="hljs-string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="hljs-string">                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)</span></span><br><span class="line"><span class="hljs-string">                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    grads -- A dictionary with the gradients</span></span><br><span class="line"><span class="hljs-string">             grads["dA" + str(l)] = ...</span></span><br><span class="line"><span class="hljs-string">             grads["dW" + str(l)] = ...</span></span><br><span class="line"><span class="hljs-string">             grads["db" + str(l)] = ...</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = len(caches)  <span class="hljs-comment"># the number of layers</span></span><br><span class="line">    m = AL.shape[<span class="hljs-number">1</span>]</span><br><span class="line">    Y = Y.reshape(AL.shape)  <span class="hljs-comment"># after this line, Y is the same shape as AL</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Initializing the backpropagation 对于dAL的计算公式已经给出了</span></span><br><span class="line">    dAL = - (np.divide(Y, AL) - np.divide(<span class="hljs-number">1</span> - Y, <span class="hljs-number">1</span> - AL))</span><br><span class="line"></span><br><span class="line">    current_cache = caches[L - <span class="hljs-number">1</span>]</span><br><span class="line">    grads[<span class="hljs-string">"dA"</span> + str(L)], grads[<span class="hljs-string">"dW"</span> + str(L)], grads[<span class="hljs-string">"db"</span> + str(L)] = linear_activation_backward(dAL, current_cache,</span><br><span class="line">                                                                                                  activation=<span class="hljs-string">"sigmoid"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> reversed(range(L - <span class="hljs-number">1</span>)):</span><br><span class="line">        current_cache = caches[l]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="hljs-string">"dA"</span> + str(l + <span class="hljs-number">2</span>)], current_cache,</span><br><span class="line">                                                                    activation=<span class="hljs-string">"relu"</span>)</span><br><span class="line">        grads[<span class="hljs-string">"dA"</span> + str(l + <span class="hljs-number">1</span>)] = dA_prev_temp</span><br><span class="line">        grads[<span class="hljs-string">"dW"</span> + str(l + <span class="hljs-number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="hljs-string">"db"</span> + str(l + <span class="hljs-number">1</span>)] = db_temp</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> grads</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 更新参数</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_parameters</span><span class="hljs-params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Update parameters using gradient descent</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    parameters -- python dictionary containing your parameters</span></span><br><span class="line"><span class="hljs-string">    grads -- python dictionary containing your gradients, output of L_model_backward</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    parameters -- python dictionary containing your updated parameters</span></span><br><span class="line"><span class="hljs-string">                  parameters["W" + str(l)] = ...</span></span><br><span class="line"><span class="hljs-string">                  parameters["b" + str(l)] = ...</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="hljs-number">2</span>  <span class="hljs-comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Update rule for each parameter. Use a for loop.</span></span><br><span class="line">    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> range(L):</span><br><span class="line">        parameters[<span class="hljs-string">"W"</span> + str(l + <span class="hljs-number">1</span>)] = parameters[<span class="hljs-string">"W"</span> + str(l + <span class="hljs-number">1</span>)] - learning_rate * grads[<span class="hljs-string">"dW"</span> + str(l + <span class="hljs-number">1</span>)]</span><br><span class="line">        parameters[<span class="hljs-string">"b"</span> + str(l + <span class="hljs-number">1</span>)] = parameters[<span class="hljs-string">"b"</span> + str(l + <span class="hljs-number">1</span>)] - learning_rate * grads[<span class="hljs-string">"db"</span> + str(l + <span class="hljs-number">1</span>)]</span><br><span class="line">    <span class="hljs-keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span></span><br><span class="line">    plt.rcParams[<span class="hljs-string">'figure.figsize'</span>] = (<span class="hljs-number">5.0</span>, <span class="hljs-number">4.0</span>) <span class="hljs-comment"># set default size of plots</span></span><br><span class="line">    plt.rcParams[<span class="hljs-string">'image.interpolation'</span>] = <span class="hljs-string">'nearest'</span></span><br><span class="line">    plt.rcParams[<span class="hljs-string">'image.cmap'</span>] = <span class="hljs-string">'gray'</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="hljs-number">1</span>)</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure><h3 id="Homework-Deep-Neural-Network-Application"><a href="#Homework-Deep-Neural-Network-Application" class="headerlink" title="Homework-Deep Neural Network Application"></a><strong>Homework-Deep Neural Network Application</strong></h3><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> time</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> h5py</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">import</span> scipy</span><br><span class="line"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image</span><br><span class="line"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> ndimage</span><br><span class="line"><span class="hljs-keyword">from</span> dnn_app_utils_v2 <span class="hljs-keyword">import</span> *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 两层神经网络</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">two_layer_model</span><span class="hljs-params">(X, Y, layers_dims, learning_rate=<span class="hljs-number">0.0075</span>, num_iterations=<span class="hljs-number">3000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    X -- input data, of shape (n_x, number of examples)</span></span><br><span class="line"><span class="hljs-string">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="hljs-string">    layers_dims -- dimensions of the layers (n_x, n_h, n_y)</span></span><br><span class="line"><span class="hljs-string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="hljs-string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="hljs-string">    print_cost -- If set to True, this will print the cost every 100 iterations</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    parameters -- a dictionary containing W1, W2, b1, and b2</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="hljs-number">1</span>)</span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []  <span class="hljs-comment"># to keep track of the cost</span></span><br><span class="line">    m = X.shape[<span class="hljs-number">1</span>]  <span class="hljs-comment"># number of examples</span></span><br><span class="line">    (n_x, n_h, n_y) = layers_dims</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Initialize parameters dictionary, by calling one of the functions you'd previously implemented</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Get W1, b1, W2 and b2 from the dictionary parameters.</span></span><br><span class="line">    W1 = parameters[<span class="hljs-string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="hljs-string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="hljs-string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="hljs-string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Inputs: "X, W1, b1". Output: "A1, cache1, A2, cache2".</span></span><br><span class="line">        A1, cache1 = linear_activation_forward(X, W1, b1, activation=<span class="hljs-string">"relu"</span>)</span><br><span class="line">        A2, cache2 = linear_activation_forward(A1, W2, b2, activation=<span class="hljs-string">"sigmoid"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(A2, Y)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Initializing backward propagation</span></span><br><span class="line">        dA2 = - (np.divide(Y, A2) - np.divide(<span class="hljs-number">1</span> - Y, <span class="hljs-number">1</span> - A2))</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Backward propagation. Inputs: "dA2, cache2, cache1". Outputs: "dA1, dW2, db2; also dA0 (not used), dW1, db1".</span></span><br><span class="line">        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation=<span class="hljs-string">"sigmoid"</span>)</span><br><span class="line">        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation=<span class="hljs-string">"relu"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2</span></span><br><span class="line">        grads[<span class="hljs-string">'dW1'</span>] = dW1</span><br><span class="line">        grads[<span class="hljs-string">'db1'</span>] = db1</span><br><span class="line">        grads[<span class="hljs-string">'dW2'</span>] = dW2</span><br><span class="line">        grads[<span class="hljs-string">'db2'</span>] = db2</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Retrieve W1, b1, W2, b2 from parameters</span></span><br><span class="line">        W1 = parameters[<span class="hljs-string">"W1"</span>]</span><br><span class="line">        b1 = parameters[<span class="hljs-string">"b1"</span>]</span><br><span class="line">        W2 = parameters[<span class="hljs-string">"W2"</span>]</span><br><span class="line">        b2 = parameters[<span class="hljs-string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Print the cost every 100 training example</span></span><br><span class="line">        <span class="hljs-keyword">if</span> print_cost <span class="hljs-keyword">and</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:</span><br><span class="line">            print(<span class="hljs-string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, np.squeeze(cost)))</span><br><span class="line">        <span class="hljs-keyword">if</span> print_cost <span class="hljs-keyword">and</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># plot the cost</span></span><br><span class="line"></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="hljs-string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="hljs-string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="hljs-string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 5层神经网络</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">L_layer_model</span><span class="hljs-params">(X, Y, layers_dims, learning_rate=<span class="hljs-number">0.0075</span>, num_iterations=<span class="hljs-number">3000</span>, print_cost=False)</span>:</span>  <span class="hljs-comment"># lr was 0.009</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class="line"><span class="hljs-string">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="hljs-string">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span></span><br><span class="line"><span class="hljs-string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="hljs-string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="hljs-string">    print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="hljs-number">1</span>)</span><br><span class="line">    costs = []  <span class="hljs-comment"># keep track of cost</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Parameters initialization.</span></span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        AL, caches = L_model_forward(X, parameters)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Compute cost.</span></span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Backward propagation.</span></span><br><span class="line">        grads = L_model_backward(AL, Y, caches)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Print the cost every 100 training example</span></span><br><span class="line">        <span class="hljs-keyword">if</span> print_cost <span class="hljs-keyword">and</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:</span><br><span class="line">            print(<span class="hljs-string">"Cost after iteration %i: %f"</span> % (i, cost))</span><br><span class="line">        <span class="hljs-keyword">if</span> print_cost <span class="hljs-keyword">and</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># plot the cost</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="hljs-string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="hljs-string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="hljs-string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span></span><br><span class="line">    plt.rcParams[<span class="hljs-string">'figure.figsize'</span>] = (<span class="hljs-number">5.0</span>, <span class="hljs-number">4.0</span>)  <span class="hljs-comment"># set default size of plots</span></span><br><span class="line">    plt.rcParams[<span class="hljs-string">'image.interpolation'</span>] = <span class="hljs-string">'nearest'</span></span><br><span class="line">    plt.rcParams[<span class="hljs-string">'image.cmap'</span>] = <span class="hljs-string">'gray'</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="hljs-number">1</span>)</span><br><span class="line"></span><br><span class="line">    train_x_orig, train_y, test_x_orig, test_y, classes = load_data()</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Explore your dataset</span></span><br><span class="line">    m_train = train_x_orig.shape[<span class="hljs-number">0</span>]</span><br><span class="line">    num_px = train_x_orig.shape[<span class="hljs-number">1</span>]</span><br><span class="line">    m_test = test_x_orig.shape[<span class="hljs-number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Reshape the training and test examples</span></span><br><span class="line">    train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class="hljs-number">0</span>],</span><br><span class="line">                                           <span class="hljs-number">-1</span>).T  <span class="hljs-comment"># The "-1" makes reshape flatten the remaining dimensions</span></span><br><span class="line">    test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class="hljs-number">0</span>], <span class="hljs-number">-1</span>).T</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Standardize data to have feature values between 0 and 1.</span></span><br><span class="line">    train_x = train_x_flatten / <span class="hljs-number">255.</span></span><br><span class="line">    test_x = test_x_flatten / <span class="hljs-number">255.</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    两层神经网络</span></span><br><span class="line"><span class="hljs-string">    # CONSTANTS DEFINING THE MODEL</span></span><br><span class="line"><span class="hljs-string">    n_x = 12288  # num_px * num_px * 3</span></span><br><span class="line"><span class="hljs-string">    n_h = 7</span></span><br><span class="line"><span class="hljs-string">    n_y = 1</span></span><br><span class="line"><span class="hljs-string">    layers_dims = (n_x, n_h, n_y)</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    parameters = two_layer_model(train_x, train_y, layers_dims=(n_x, n_h, n_y), num_iterations=2500, print_cost=True)</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    五层神经网络</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line">    <span class="hljs-comment">### CONSTANTS ###</span></span><br><span class="line">    layers_dims = [<span class="hljs-number">12288</span>, <span class="hljs-number">20</span>, <span class="hljs-number">7</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>]  <span class="hljs-comment"># 5-layer model</span></span><br><span class="line"></span><br><span class="line">    parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations=<span class="hljs-number">2500</span>, print_cost=<span class="hljs-keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 训练集正确率</span></span><br><span class="line">    predictions_train = predict(train_x, train_y, parameters)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 测试集正确率</span></span><br><span class="line">    predictions_test = predict(test_x, test_y, parameters)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 自定义图片</span></span><br><span class="line">    my_image = <span class="hljs-string">"a.jpg"</span>  <span class="hljs-comment"># change this to the name of your image file</span></span><br><span class="line">    my_label_y = [<span class="hljs-number">1</span>]  <span class="hljs-comment"># the true class of your image (1 -&gt; cat, 0 -&gt; non-cat)</span></span><br><span class="line"></span><br><span class="line">    fname = <span class="hljs-string">"images/"</span> + my_image</span><br><span class="line">    image = np.array(ndimage.imread(fname, flatten=<span class="hljs-keyword">False</span>))</span><br><span class="line">    my_image = scipy.misc.imresize(image, size=(num_px, num_px)).reshape((num_px * num_px * <span class="hljs-number">3</span>, <span class="hljs-number">1</span>))</span><br><span class="line">    my_predicted_image = predict(my_image, my_label_y, parameters)</span><br><span class="line"></span><br><span class="line">    plt.imshow(image)</span><br><span class="line">    print(<span class="hljs-string">"y = "</span> + str(np.squeeze(my_predicted_image)) + <span class="hljs-string">", your L-layer model predicts a \""</span> + classes[</span><br><span class="line">        int(np.squeeze(my_predicted_image)),].decode(<span class="hljs-string">"utf-8"</span>) + <span class="hljs-string">"\" picture."</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> deeplearning.ai </category>
          
          <category> Deep learning &amp; NN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Neural Networks-1</title>
      <link href="/2019/02/08/Deep-Neural-Networks-1/"/>
      <url>/2019/02/08/Deep-Neural-Networks-1/</url>
      
        <content type="html"><![CDATA[<p>本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第四周深层神经网络的相关课程。</p><h3 id="Deep-L-layer-Neural-network"><a href="#Deep-L-layer-Neural-network" class="headerlink" title="Deep L-layer Neural network"></a><strong>Deep L-layer Neural network</strong></h3><p>主要复习了之前 Logistic Regression 和单隐藏层的神经网络，并推广到多隐藏层，同时也介绍了深层神经网络的一些符号约定，基本遵循了之前的规则，所以这里不再详述。</p><a id="more"></a><h3 id="Forward-propagation-in-a-deep-network"><a href="#Forward-propagation-in-a-deep-network" class="headerlink" title="Forward propagation in a deep network"></a><strong>Forward propagation in a deep network</strong></h3><p>以 $4$ 层的神经网络为例，我们来推导一下对于单个样本的正向传播过程：</p><p><img src="/2019/02/08/Deep-Neural-Networks-1/1.png" alt=""></p><p>第一层：</p><script type="math/tex; mode=display">z^{[1]} = w^{[1]}x + b^{[1]} = w^{[1]}a^{[0]} + b^{[1]}</script><script type="math/tex; mode=display">a^{[1]} = g^{[1]}(z^{[1]})</script><p>推广到第 $l$ 层：</p><script type="math/tex; mode=display">z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}</script><script type="math/tex; mode=display">a^{[l]} = g^{[l]}(z^{[l]})</script><p>第 $l$ 层向量化版本：</p><script type="math/tex; mode=display">Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}</script><script type="math/tex; mode=display">A^{[l]} = g^{[l]}(Z^{[l]})</script><p>这里有一点需要注意，对于深层神经网络正向传播，它类似一个循环的过程，即从 $1$ to $l$ 的循环，对每一层分别计算正向传播的过程。你可能会想到前面所说的向量化来优化，但这里避免不了显式的 $for$ 循环。</p><h3 id="Getting-your-matrix-dimensions-right"><a href="#Getting-your-matrix-dimensions-right" class="headerlink" title="Getting your matrix dimensions right"></a><strong>Getting your matrix dimensions right</strong></h3><p>在实现深层神经网络的过程中，想要降低 bug 的出现概率，你必须非常仔细地检查矩阵维数是否正确。本节就来讨论如何确定矩阵的维数。</p><p>以下图的神经网络为例：</p><p><img src="/2019/02/08/Deep-Neural-Networks-1/2.png" alt=""></p><p>不难看出 $l=5, n^{[0]}=n_x=2, n^{[1]}=3, n^{[2]}=5, n^{[3]}=4, n^{[4]}=2, n^{[5]}=1$，如果我们想要进行正向传播，那么首先需要计算</p><script type="math/tex; mode=display">z^{[1]} = W^{[1]}x + b^{[1]}</script><p>这里的 $z^{[1]}​$ 的维度为 $(n^{[1]},1)​$ ，$x​$ 的维度为 $(n^{[0]},1)​$ ，根据矩阵乘法的规则，可以得到 $w^{[1]}​$ 的维度必为 $(n^{[1]},n^{[0]})​$ ，同样可以得到 $b^{[1]}​$ 的维度为 $(n^{[1]},1)​$ 。</p><p>总结一下， $W^{[l]}​$ 的维度为 $(n^{[l]},n^{[l-1]})​$ ，$b^{[1]}​$ 的维度为 $(n^{[l]},1)​$ 。顺便提一下，$dw^{[l]},db^{[l]}​$ 的维度与 $W^{[l]},b^{[l]}​$ 相同。</p><p>接着，我们需要计算</p><script type="math/tex; mode=display">a^{[1]} = g^{[1]}(z^{[1]})</script><p>可以得到，$a^{[l]}$ 的维度与 $z^{[l]}$ 相同，同样为 $(n^{[l]},1)$。</p><blockquote><p>总结一下：</p><script type="math/tex; mode=display">W^{[l]} :(n^{[l]},n^{[l-1]})</script><script type="math/tex; mode=display">z^{[l]},a^{[l]},b^{[l]} :(n^{[l]},1)</script><p>$dw,db$ 的维度与 $W,b$ 相同</p></blockquote><p>接下来，我们看一下向量化后，各矩阵的维数，同样需要计算</p><script type="math/tex; mode=display">Z^{[1]} = W^{[1]}X + b^{[1]}</script><p>我们将 $m$ 个样本横向堆叠，那么 $Z^{[1]}$ 的维度为 $(n^{[1]},m)$ ，$X$ 的维度为 $(n^{[0]},m)$ ，$W^{[1]}$ 的维度为 $(n^{[1]},n^{[0]})$ ，$b^{[1]}$ 的维度为 $(n^{[0]},1)$ ，由于 Broadcasting ，所以实际运算中，会将 $b^{[1]}$ 复制 $m$ 次，形成维度为 $(n^{[0]},m)$ 的矩阵。</p><blockquote><p>在向量化后，各矩阵维度如下：</p><script type="math/tex; mode=display">W^{[l]} :(n^{[l]},n^{[l-1]})</script><script type="math/tex; mode=display">Z^{[l]},A^{[l]} :(n^{[l]},m)</script><script type="math/tex; mode=display">b^{[l]} :(n^{[l]},1)</script><p>同样，$dZ,dA$ 与 $Z,A​$ 维度一样</p></blockquote><h3 id="Why-deep-representations"><a href="#Why-deep-representations" class="headerlink" title="Why deep representations"></a><strong>Why deep representations</strong></h3><p>本节从几个例子来探讨了为什么深度神经网络如此强大。</p><p><strong>人脸识别</strong></p><p><img src="/2019/02/08/Deep-Neural-Networks-1/3.png" alt=""></p><p>神经网络第一层，你可以把它当成一个边缘探测器 (edge detector) 从原始图片中识别人脸的边缘，下方图片中的一个小方块就是一个隐藏单元，代表着边缘的方向，从而识别边缘。神经网络第二层就将前一层的边缘信息进行组合，组合成面部的不同部分，比如：眼睛，鼻子等等。最后再将这些局部特征放在一起，比如：鼻子，眼睛，下巴，就可以识别整张人脸。可以看出，随着神经网络由浅入深，所获得的信息也是从部分到整体。更深入的部分会在卷积神经网络中讨论。</p><p><strong>语音识别</strong></p><p>和人脸识别有些相似，也是通过这种从简单到复杂，从部分到整体的方式，在前几层先学习一些简单的特征，再后面几层进行组合，最后去识别更复杂的东西。以语音识别为例，第一层你可能会试着探测一些低层次的音频波形特征，比如：音调，白噪音等等。然后将这些波形组合在一起，就能去探测声音的基本单元，即音位 (phonemes) ，比如 cat 发音的 /k/ /æ/ /t/ 分别为三个音位，有了基本的声音单元后，我们就可以识别单词，从而识别词组，再到整个句子。在前几层学习一些简单的特征，再后面几层进行组合，去识别更复杂的东西。</p>]]></content>
      
      
      <categories>
          
          <category> deeplearning.ai </category>
          
          <category> Deep learning &amp; NN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shallow Neural Networks-2</title>
      <link href="/2019/02/01/Shallow-Neural-Networks-2/"/>
      <url>/2019/02/01/Shallow-Neural-Networks-2/</url>
      
        <content type="html"><![CDATA[<p>本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第三周浅层神经网络的相关课程及相关作业。</p><h3 id="Why-do-you-need-non-linear-activation-function"><a href="#Why-do-you-need-non-linear-activation-function" class="headerlink" title="Why do you need non-linear activation function"></a><strong>Why do you need non-linear activation function</strong></h3><p>为什么神经网络需要非线性的激活函数？不能使用线性的激活函数，比如 $g(z) = z$ 吗？</p><a id="more"></a><p>假设我们使用线性的激活函数 $g(z) = z$ ，那么有：</p><script type="math/tex; mode=display">a^{[1]} = z^{[1]} = W^{[1]}x + b^{[1]}</script><script type="math/tex; mode=display">a^{[2]} = z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}</script><p>把 $a^{[1]}$ 带入，则有</p><script type="math/tex; mode=display">a^{[2]} = W^{[2]}(W^{[1]}x + b^{[1]}) + b^{[2]} = (W^{[2]}W{[1]})x + (W^{[2]}b^{[1]}+b{[2]}) = W'x+b'</script><p>我们可以得到，$a^{[2]}$ 仍是输入 $x$ 的线性组合。也就是说，使用线性函数的神经网络仅仅只是把输入 $x$ 线性组合再输出。即便是包含许多隐藏层的神经网络，如果使用的是线性的激活函数，不管多少层，得到的输出依然是 $x$ 的线性组合，也就意味着隐藏层根本没有什么作用。所以，隐藏层激活函数必须是非线性的，否则将失去意义。</p><p>只有一个地方你可能会使用线性激活函数，在机器学习的回归问题中，$y$ 是一个实数，比如你像预测房地产的价格，那么 $y$ 是一个实数，而不是像二分类问题那样要么 $0$ 要么 $1$ ，这种情况下，在输出层你可能会使用线性激活函数，但隐藏层不会使用线性激活函数。</p><h3 id="Derivatives-of-activation-functions"><a href="#Derivatives-of-activation-functions" class="headerlink" title="Derivatives of activation functions"></a><strong>Derivatives of activation functions</strong></h3><p>在反向传播的过程中，我们需要计算激活函数的导数，那么我们来看一下上述这些激活函数的导数。</p><ul><li>Sigmoid 函数<script type="math/tex; mode=display">g(z) = \frac{1}{1+e^{-z}}</script></li></ul><script type="math/tex; mode=display">g'(z) = g(z)(1-g(z)) = a(1-a)</script><ul><li>tanh 函数</li></ul><script type="math/tex; mode=display">g(z) = \frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}</script><script type="math/tex; mode=display">g'(z) = 1-(g(z))^{2} = 1-a^2</script><ul><li>ReLU 函数<script type="math/tex; mode=display">g(z) = max(0,z)</script></li></ul><script type="math/tex; mode=display">g'(z) = \begin{cases}0,\quad z < 0 \\1,\quad z \geq  0\end{cases}</script><ul><li>Leaky ReLU 函数<script type="math/tex; mode=display">g(z) = max(0.01z,z)</script></li></ul><script type="math/tex; mode=display">g'(z) = \begin{cases}0.01,\quad z < 0 \\\1,\quad \quad z \geq  0\end{cases}</script><h3 id="Gradient-descent-for-neural-networks"><a href="#Gradient-descent-for-neural-networks" class="headerlink" title="Gradient descent for neural networks"></a><strong>Gradient descent for neural networks</strong></h3><p>好了，有了以上的铺垫，我们终于可以实现在单隐藏层神经网络上的梯度下降算法了。</p><p>由于是单隐藏层神经网络，那么我们有参数 $W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}​$ 。我们一般用 $n_x = n^{[0]}​$ 来表示输入层特征的个数，用 $n^{[1]}​$ 表示隐藏层节点个数，用 $n^{[2]}=1​$ 表示输出层节点个数。 其中，$W^{[1]}​$ 的维度为 $(n^{[1]}, n^{[0]})​$ ，$b^{[1]}​$ 的维度为 $(n^{[1]}, 1)​$ ，$W^{[2]}​$ 的维度为 $(n^{[2]}, n^{[1]})​$ ，$b^{[2]}​$ 的维度为 ​$(n^{[2]}, 1)​$ 。</p><p>假设我们在做二元分类，那么 Cost function 为：</p><script type="math/tex; mode=display">J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \frac{1}{m}\sum_{i=1}^{m}L(\hat{y}, y)</script><p>整个训练神经网络的过程为：</p><blockquote><p>Repeat{</p><p>initialize parameters $W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]} $</p><p>compute predicts $\hat{y}^{(i)}, i \in [1,m]​$</p><p>compute $dW^{[1]}, db^{[1]}, W^{[2]}, b^{[2]}$</p><p>update $W^{[1]} = W^{[1]} - \alpha dW^{[1]}, b^{[1]} = b^{[1]} - \alpha db^{[1]} …​$  </p><p>}</p></blockquote><p>在训练神经网络时，随机初始化参数很重要，并不是单纯的全部初始化为 $0​$ ，我们将在后续详细讨论。</p><p>在之前，我们讨论了如果计算 predict (预测值) ，以及如何向量化实现整个过程，所以现在的关键在于，如何计算这些偏导项 $dW^{[1]}, db^{[1]}​$ … </p><p>神经网络正向传播的过程为：</p><script type="math/tex; mode=display">Z^{[1]} = W^{[1]}X + b^{[1]}</script><script type="math/tex; mode=display">A^{[1]} = g(Z^{[1]})</script><script type="math/tex; mode=display">Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}</script><script type="math/tex; mode=display">A^{[2]} = g(Z^{[2]}) = \sigma(Z^{[2]})</script><p>反向传播的过程为：</p><script type="math/tex; mode=display">dZ^{[2]} = A^{[2]}-Y</script><script type="math/tex; mode=display">dW^{[2]} = \frac{1}{m}dZ^{[2]}A^{[1]T}</script><script type="math/tex; mode=display">db^{[2]} = \frac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True)</script><blockquote><p> 这个过程其实和之前的 Logistic Regression 很相似。需要注意的是，这里$db^{[2]}$ 的计算直接用了 python 语句，np.sum函数的参数 axis 代表在哪个维度上求和，keepdims为了保持 $db^{[2]}$ 的形状为 $(n^{[2]},1)$ 而不是奇怪的 $(n^{[2]}, )$ </p></blockquote><script type="math/tex; mode=display">dZ^{[2]} = W^{[2]T}dZ^{[2]} * g^{[1]'}(Z^{[1]})</script><script type="math/tex; mode=display">dW^{[1]} = \frac{1}{m}dZ^{[1]}X^{T}</script><script type="math/tex; mode=display">db^{[1]}=\frac{1}{m}np.sum(dZ^{[1]},axis = 1,keepdims = True)</script><blockquote><p>需要注意的是 $dZ^{[2]} = W^{[2]T}dZ^{[2]} *g^{[1]’}(Z^{[1]})$ 这里是对应元素相乘，$W^{[2]T}dZ^{[2]}$ 的形状为 $(n^{[1]}, m)$ ，$g’^{[1]}(Z^{[1]})$ 的形状也为 $(n^{[1]}, m)$ 。</p></blockquote><p>关于详细的推导过程，会在接下来的部分详细说明。</p><h3 id="Backpropagation-intuition"><a href="#Backpropagation-intuition" class="headerlink" title="Backpropagation intuition"></a><strong>Backpropagation intuition</strong></h3><p>先回顾一下之前我们在实现 Logistic Regression 时是如何推导的，可以参考之前的博客 <a href="https://xiaohanhan1019.github.io/2019/01/21/Basic-of-Neural-Networks-1/">Logistic Regression Gradient Descent</a>.</p><p>由于现在多了一层隐藏层，整个反向传播过程会更复杂一点。</p><p>首先，先考虑单个样本的情况，先画出计算图，如下图：</p><p><img src="/2019/02/01/Shallow-Neural-Networks-2/1.png" alt=""></p><p>根据导数链式法则，可以计算出：</p><script type="math/tex; mode=display">dz^{[2]} = \frac{\partial L}{\partial z^{[2]}} = \frac{\partial L}{\partial a^{[2]}} \cdot \frac{\partial a^{[2]}}{\partial z^{[2]}}= a^{[2]}-y</script><script type="math/tex; mode=display">dW^{[2]}= \frac{\partial L}{\partial W^{[2]}} = \frac{\partial L}{\partial z^{[2]}} \cdot \frac{\partial z^{[2]}}{\partial W^{[2]}} = dz^{[2]} \cdot \frac{\partial z^{[2]}}{\partial W^{[2]}} = dz^{[2]}a^{[1]T}</script><blockquote><p>注意，这里 $dW^{[2]}$ 的形状为 $(n^{[2]}, n^{[1]})$ ， $n^{[1]}$ 为隐藏层节点个数，而 $a^{[1]}$ 形状为 $(n^{[1]}, n^{[2]})$ ，故这里需要 $a^{[1]}$ 转置，即 $a^{[1]T}$ 。</p></blockquote><script type="math/tex; mode=display">db^{[2]}= \frac{\partial L}{\partial b^{[2]}} = \frac{\partial L}{\partial z^{[2]}} \cdot \frac{\partial z^{[2]}}{\partial b^{[2]}} = dz^{[2]} \cdot \frac{\partial z^{[2]}}{\partial b^{[2]}} = dz^{[2]}</script><script type="math/tex; mode=display">dz^{[1]} = \frac{\partial L}{\partial z^{[1]}} = \frac{\partial L}{\partial a^{[1]}} \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}} = \frac{\partial L}{\partial z^{[2]}} \cdot \frac{\partial z^{[2]}}{\partial a^{[1]}} \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}} = dz^{[2]} \cdot \frac{\partial z^{[2]}}{\partial a^{[1]}} \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}}</script><p>即，</p><script type="math/tex; mode=display">dz^{[1]} = W^{[2]T}dz^{[2]} * g^{[1]'}(z^{[1]})</script><blockquote><p>同样，这里的 $W^{[2]T}$ 也需要转置，具体为什么，我也不是很明白，但可以从矩阵乘法的规则来判断其形状是否正确。需要注意的是，这里的乘法 $*$ 为对应元素相乘，而不是一般意义上的矩阵乘法。在实现过程中，你必须要确保矩阵的形状相匹配，这里 $W^{[2]T}$ 形状为 $(n^{[1]}, n^{[2]})$ ，$dz^{[2]}$ 的形状为 $(n^{[2]}, 1)$ ，$z^{[1]}$ 的形状为 $(n^{[1]}, 1)$</p></blockquote><script type="math/tex; mode=display">dW^{[1]} = dz^{[1]} \cdot \frac{\partial z^{[1]}}{\partial W^{[1]}} = dz^{[1]}x^{T} = dz^{[1]}a^{[0]T}</script><script type="math/tex; mode=display">db^{[1]} = dz^{[1]} \cdot \frac{\partial z^{[1]}}{\partial b^{[1]}} = dz^{[1]}</script><p>到这里为止，我们就推导完了反向传播的6个公式。接下来，我们需要将其推广到 $m$ 个训练样本的向量化实现上，得到结果如下，：</p><p><img src="/2019/02/01/Shallow-Neural-Networks-2/2.png" alt=""></p><h3 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a><strong>Random Initialization</strong></h3><p>神经网络中的参数 $W$ 是不能和 Logistic Regression 那样全部初始化为 $0$ 的，我们来分析一下原因。</p><p>假设我们有这样一个神经网络，如下图所示：</p><p><img src="/2019/02/01/Shallow-Neural-Networks-2/3.png" alt=""></p><p>假设，我们将 $W^{[1]}, W^{[2]}$ 都初始化为零矩阵，那么经过正向传播以后，我们会得到 $a^{[1]}_1 = a^{[1]}_{2}$ ，那么根据对称性，在反向传播后会有 $dz^{[1]}_{1} = dz^{[1]}_{2}$ ， $dW^{[1]}_{1}=dW^{[1]}_{2}$ ，无论你执行多少次梯度下降算法，隐藏层的每个节点都在做相同的操作。这样的话，最后我们获得的 $W^{[1]}, W^{[2]}$ 每行元素都相同，也就是说所有隐藏层中的节点都可以用一个节点来代替，多余的节点没有任何意义，这不是我们想要的。 另外，参数 $b$ 可以全部初始化为 $0$ ，不会发生上面提到的问题。</p><p>所以，我们必须随机初始化所有的参数。python 语句如下：</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W1 = np.random.randn((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))*<span class="hljs-number">0.01</span></span><br><span class="line">b1 = np.zero((<span class="hljs-number">2</span>,<span class="hljs-number">1</span>))</span><br><span class="line">W1 = np.random.randn((<span class="hljs-number">1</span>,<span class="hljs-number">2</span>))*<span class="hljs-number">0.01</span></span><br><span class="line">b1 = <span class="hljs-number">0</span></span><br></pre></td></tr></table></figure><p>你可能为由疑问，为什么这里要 $*0.01$ ，为什么是 $0.01$ 而不是其他的数字？事实上，我们倾向于把矩阵初始化为非常非常小的随机值。因为如果你用 tanh 函数或者 sigmoid 函数作为激活函数， $W$ 比较小，那正向传播后得到的 $z$ 也会比较小，经过激活函数后所得到的 $a$ 也会接近于 $0$ ，而在 $0$ 附近，激活函数的斜率比较大，能大大地提高梯度下降算法的更新速度，即学习的速度。如果你使用的激活函数为 ReLU 或是 Leaky ReLU 则没有这个问题。</p><p>有时候，会有比 $0.01$ 更好用的常数，但如果你只是训练一个单隐层神经网络，或是一个相对较浅的神经网络，没有太多隐藏层，使用 $0.01$ 没有太大问题。但是当你训练一个很深的神经网络时，你可能需要尝试一下别的常数，关于常数的详细内容会在后续部分提到。</p><h3 id="Homework"><a href="#Homework" class="headerlink" title="Homework"></a><strong>Homework</strong></h3><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># Package imports</span></span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">from</span> testCases_v2 <span class="hljs-keyword">import</span> *</span><br><span class="line"><span class="hljs-keyword">import</span> sklearn</span><br><span class="line"><span class="hljs-keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="hljs-keyword">import</span> sklearn.linear_model</span><br><span class="line"><span class="hljs-keyword">from</span> planar_utils <span class="hljs-keyword">import</span> plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">layer_sizes</span><span class="hljs-params">(X, Y)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    X -- input dataset of shape (input size, number of examples)</span></span><br><span class="line"><span class="hljs-string">    Y -- labels of shape (output size, number of examples)</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    n_x -- the size of the input layer</span></span><br><span class="line"><span class="hljs-string">    n_h -- the size of the hidden layer</span></span><br><span class="line"><span class="hljs-string">    n_y -- the size of the output layer</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line">    n_x = X.shape[<span class="hljs-number">0</span>]  <span class="hljs-comment"># size of input layer</span></span><br><span class="line">    n_h = <span class="hljs-number">4</span></span><br><span class="line">    n_y = Y.shape[<span class="hljs-number">0</span>]  <span class="hljs-comment"># size of output layer</span></span><br><span class="line">    <span class="hljs-keyword">return</span> (n_x, n_h, n_y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize_parameters</span><span class="hljs-params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Argument:</span></span><br><span class="line"><span class="hljs-string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="hljs-string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="hljs-string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    params -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="hljs-string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="hljs-string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="hljs-string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="hljs-string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="hljs-number">2</span>)  <span class="hljs-comment"># we set up a seed so that your output matches ours although the initialization is random.</span></span><br><span class="line"></span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * <span class="hljs-number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="hljs-number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * <span class="hljs-number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="hljs-number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">assert</span> (W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="hljs-keyword">assert</span> (b1.shape == (n_h, <span class="hljs-number">1</span>))</span><br><span class="line">    <span class="hljs-keyword">assert</span> (W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="hljs-keyword">assert</span> (b2.shape == (n_y, <span class="hljs-number">1</span>))</span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="hljs-string">"W1"</span>: W1,</span><br><span class="line">                  <span class="hljs-string">"b1"</span>: b1,</span><br><span class="line">                  <span class="hljs-string">"W2"</span>: W2,</span><br><span class="line">                  <span class="hljs-string">"b2"</span>: b2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward_propagation</span><span class="hljs-params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Argument:</span></span><br><span class="line"><span class="hljs-string">    X -- input data of size (n_x, m)</span></span><br><span class="line"><span class="hljs-string">    parameters -- python dictionary containing your parameters (output of initialization function)</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    A2 -- The sigmoid output of the second activation</span></span><br><span class="line"><span class="hljs-string">    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line">    <span class="hljs-comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    W1 = parameters[<span class="hljs-string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="hljs-string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="hljs-string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="hljs-string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Implement Forward Propagation to calculate A2 (probabilities)</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">assert</span> (A2.shape == (<span class="hljs-number">1</span>, X.shape[<span class="hljs-number">1</span>]))</span><br><span class="line"></span><br><span class="line">    cache = &#123;<span class="hljs-string">"Z1"</span>: Z1,</span><br><span class="line">             <span class="hljs-string">"A1"</span>: A1,</span><br><span class="line">             <span class="hljs-string">"Z2"</span>: Z2,</span><br><span class="line">             <span class="hljs-string">"A2"</span>: A2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> A2, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_cost</span><span class="hljs-params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Computes the cross-entropy cost given in equation (13)</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)</span></span><br><span class="line"><span class="hljs-string">    Y -- "true" labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="hljs-string">    parameters -- python dictionary containing your parameters W1, b1, W2 and b2</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    cost -- cross-entropy cost given equation (13)</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    m = Y.shape[<span class="hljs-number">1</span>]  <span class="hljs-comment"># number of example</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Compute the cross-entropy cost</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(<span class="hljs-number">1</span> - A2), <span class="hljs-number">1</span> - Y)</span><br><span class="line">    cost = -np.sum(logprobs) / m</span><br><span class="line"></span><br><span class="line">    cost = np.squeeze(cost)  <span class="hljs-comment"># makes sure cost is the dimension we expect.</span></span><br><span class="line">    <span class="hljs-comment"># E.g., turns [[17]] into 17</span></span><br><span class="line">    <span class="hljs-keyword">assert</span> (isinstance(cost, float))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward_propagation</span><span class="hljs-params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implement the backward propagation using the instructions above.</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    parameters -- python dictionary containing our parameters</span></span><br><span class="line"><span class="hljs-string">    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".</span></span><br><span class="line"><span class="hljs-string">    X -- input data of shape (2, number of examples)</span></span><br><span class="line"><span class="hljs-string">    Y -- "true" labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    grads -- python dictionary containing your gradients with respect to different parameters</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line">    m = X.shape[<span class="hljs-number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># First, retrieve W1 and W2 from the dictionary "parameters".</span></span><br><span class="line">    W1 = parameters[<span class="hljs-string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="hljs-string">"W2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Retrieve also A1 and A2 from dictionary "cache".</span></span><br><span class="line">    A1 = cache[<span class="hljs-string">"A1"</span>]</span><br><span class="line">    A2 = cache[<span class="hljs-string">"A2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Backward propagation: calculate dW1, db1, dW2, db2.</span></span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = np.dot(dZ2, A1.T) / m</span><br><span class="line">    db2 = np.sum(dZ2, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-keyword">True</span>) / m</span><br><span class="line">    dZ1 = np.dot(W2.T, dZ2) * (<span class="hljs-number">1</span> - np.power(A1, <span class="hljs-number">2</span>))</span><br><span class="line">    dW1 = np.dot(dZ1, X.T) / m</span><br><span class="line">    db1 = np.sum(dZ1, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-keyword">True</span>) / m</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="hljs-string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="hljs-string">"db1"</span>: db1,</span><br><span class="line">             <span class="hljs-string">"dW2"</span>: dW2,</span><br><span class="line">             <span class="hljs-string">"db2"</span>: db2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> grads</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_parameters</span><span class="hljs-params">(parameters, grads, learning_rate=<span class="hljs-number">1.2</span>)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Updates parameters using the gradient descent update rule given above</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    parameters -- python dictionary containing your parameters</span></span><br><span class="line"><span class="hljs-string">    grads -- python dictionary containing your gradients</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    parameters -- python dictionary containing your updated parameters</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line">    <span class="hljs-comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    W1 = parameters[<span class="hljs-string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="hljs-string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="hljs-string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="hljs-string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Retrieve each gradient from the dictionary "grads"</span></span><br><span class="line">    dW1 = grads[<span class="hljs-string">"dW1"</span>]</span><br><span class="line">    db1 = grads[<span class="hljs-string">"db1"</span>]</span><br><span class="line">    dW2 = grads[<span class="hljs-string">"dW2"</span>]</span><br><span class="line">    db2 = grads[<span class="hljs-string">"db2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Update rule for each parameter</span></span><br><span class="line">    W1 = W1 - learning_rate * dW1</span><br><span class="line">    b1 = b1 - learning_rate * db1</span><br><span class="line">    W2 = W2 - learning_rate * dW2</span><br><span class="line">    b2 = b2 - learning_rate * db2</span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="hljs-string">"W1"</span>: W1,</span><br><span class="line">                  <span class="hljs-string">"b1"</span>: b1,</span><br><span class="line">                  <span class="hljs-string">"W2"</span>: W2,</span><br><span class="line">                  <span class="hljs-string">"b2"</span>: b2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">nn_model</span><span class="hljs-params">(X, Y, n_h, num_iterations=<span class="hljs-number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    X -- dataset of shape (2, number of examples)</span></span><br><span class="line"><span class="hljs-string">    Y -- labels of shape (1, number of examples)</span></span><br><span class="line"><span class="hljs-string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="hljs-string">    num_iterations -- Number of iterations in gradient descent loop</span></span><br><span class="line"><span class="hljs-string">    print_cost -- if True, print the cost every 1000 iterations</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="hljs-number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="hljs-number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="hljs-number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: "n_x, n_h, n_y". Outputs = "W1, b1, W2, b2, parameters".</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    W1 = parameters[<span class="hljs-string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="hljs-string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="hljs-string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="hljs-string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache".</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Cost function. Inputs: "A2, Y, parameters". Outputs: "cost".</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads".</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".</span></span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Print the cost every 1000 iterations</span></span><br><span class="line">        <span class="hljs-keyword">if</span> print_cost <span class="hljs-keyword">and</span> i % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:</span><br><span class="line">            print(<span class="hljs-string">"Cost after iteration %i: %f"</span> % (i, cost))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(parameters, X)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Using the learned parameters, predicts a class for each example in X</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    parameters -- python dictionary containing your parameters</span></span><br><span class="line"><span class="hljs-string">    X -- input data of size (n_x, m)</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Returns</span></span><br><span class="line"><span class="hljs-string">    predictions -- vector of predictions of our model (red: 0 / blue: 1)</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.</span></span><br><span class="line">    A2, cache = forward_propagation(X, parameters)</span><br><span class="line">    predictions = (A2 &gt; <span class="hljs-number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span></span><br><span class="line">    <span class="hljs-comment"># random seed</span></span><br><span class="line">    np.random.seed(<span class="hljs-number">1</span>)  <span class="hljs-comment"># set a seed so that the results are consistent</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Load data</span></span><br><span class="line">    X, Y = load_planar_dataset()</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Visualize the data</span></span><br><span class="line">    <span class="hljs-comment"># 原来的代码会报错，同样planar_utils.py 21行也需要修改</span></span><br><span class="line">    plt.scatter(X[<span class="hljs-number">0</span>, :], X[<span class="hljs-number">1</span>, :], c=Y.reshape(X[<span class="hljs-number">0</span>, :].shape), cmap=plt.cm.Spectral)</span><br><span class="line">    <span class="hljs-comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># shape of dataset</span></span><br><span class="line">    shape_X = X.shape</span><br><span class="line">    shape_Y = Y.shape</span><br><span class="line">    m = X.shape[<span class="hljs-number">1</span>]  <span class="hljs-comment"># training set size</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Build a model with a n_h-dimensional hidden layer</span></span><br><span class="line">    parameters = nn_model(X, Y, n_h=<span class="hljs-number">4</span>, num_iterations=<span class="hljs-number">10000</span>, print_cost=<span class="hljs-keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Plot the decision boundary</span></span><br><span class="line">    plot_decision_boundary(<span class="hljs-keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">    plt.title(<span class="hljs-string">"Decision Boundary for hidden layer size "</span> + str(<span class="hljs-number">4</span>))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Print accuracy</span></span><br><span class="line">    predictions = predict(parameters, X)</span><br><span class="line">    print(<span class="hljs-string">'Accuracy: %d'</span> % float(</span><br><span class="line">        (np.dot(Y, predictions.T) + np.dot(<span class="hljs-number">1</span> - Y, <span class="hljs-number">1</span> - predictions.T)) / float(Y.size) * <span class="hljs-number">100</span>) + <span class="hljs-string">'%'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># # Train the logistic regression classifier</span></span><br><span class="line">    <span class="hljs-comment"># clf = sklearn.linear_model.LogisticRegressionCV()</span></span><br><span class="line">    <span class="hljs-comment"># clf.fit(X.T, Y.T)</span></span><br><span class="line">    <span class="hljs-comment">#</span></span><br><span class="line">    <span class="hljs-comment"># # Plot the decision boundary for logistic regression</span></span><br><span class="line">    <span class="hljs-comment"># plot_decision_boundary(lambda x: clf.predict(x), X, Y)</span></span><br><span class="line">    <span class="hljs-comment"># plt.title("Logistic Regression")</span></span><br><span class="line">    <span class="hljs-comment"># plt.show()</span></span><br><span class="line">    <span class="hljs-comment">#</span></span><br><span class="line">    <span class="hljs-comment"># # Print accuracy</span></span><br><span class="line">    <span class="hljs-comment"># LR_predictions = clf.predict(X.T)</span></span><br><span class="line">    <span class="hljs-comment"># print('Accuracy of logistic regression: %d ' % float(</span></span><br><span class="line">    <span class="hljs-comment">#     (np.dot(Y, LR_predictions) + np.dot(1 - Y, 1 - LR_predictions)) / float(Y.size) * 100) +</span></span><br><span class="line">    <span class="hljs-comment">#       '% ' + "(percentage of correctly labelled datapoints)")</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> deeplearning.ai </category>
          
          <category> Deep learning &amp; NN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shallow Neural Networks-1</title>
      <link href="/2019/02/01/Shallow-Neural-Networks-1/"/>
      <url>/2019/02/01/Shallow-Neural-Networks-1/</url>
      
        <content type="html"><![CDATA[<p>本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第三周浅层神经网络的相关课程。</p><h3 id="Neural-Network-Overview"><a href="#Neural-Network-Overview" class="headerlink" title="Neural Network Overview"></a><strong>Neural Network Overview</strong></h3><p>本周，你将学会如何实现神经网络。上周，我们讨论了对数几率回归 (logistic regression) ，并且使用计算图 (computation graph) 的方式了解了梯度下降算法的正向传播和反向传播的两个过程，如下图所示 :<br></p><a id="more"></a><p><img src="/2019/02/01/Shallow-Neural-Networks-1/1.png" alt=""></p><p>而神经网络 (Neural Network) 是这个样子，如下图 :</p><p><img src="/2019/02/01/Shallow-Neural-Networks-1/2.png" alt=""></p><p>我们可以把很多 sigmoid 单元堆叠起来来构成一个神经网络。在之前所学的对数几率回归中，每一个节点对应着两个计算步骤：首先计算 $z=w^{T}x + b​$ ，然后计算 $a=\sigma(z)​$ 。在这个神经网络中，三个竖排堆叠的节点就对应着这两部分的计算，那个单独的节点也对应着另一个类似的 $z, a​$ 的计算。 <br></p><p>在神经网络中，所用的符号也会有些不一样。我们还是用 $x$ 来表示输入特征，用 $W^{[1]}, b^{[1]}$ 来表示参数，这样你就可以计算出 $z^{[1]} = W^{[1]}x + b^{[1]}$ 。这里右上角的 $[1]$ 代表着节点所属的层，你可以认为层数从 $0$ 开始算起，如上图中的 $x_1, x_2, x_3$ 就代表着第 $0$ 层（也称为输入层），三个竖排的节点就属于第 $1$ 层（也称为隐藏层），单独的那个节点属于第 $2$ 层（也称为输出层）。需要注意的是，这与之前用来标注第 $i$ 个训练样本 $(x^{(i)}, y^{(i)})$ 不同，这里用的是方括号。<br></p><p>那么，在这个神经网络模型中，正向传播就分为两层。</p><ul><li>从输入层到隐藏层：在使用类似对数几率回归的方法计算了 $z^{[1]}$ 之后，再计算 $a^{[1]}=\sigma(z^{[1]})​$ 。</li><li>从隐藏层到输出层：使用相同的方法计算 $z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$ 。当然，这里的参数  $W^{[2]}, b^{[2]}$ 与 $W^{[1]}, b^{[1]}$ 不同，且第 $1$ 层的输出 $a^{[1]}$ 作为第 $2$ 层的输入 $x$ ，接着同样计算 $a^{[2]}=\sigma(z^{[2]})$ ，得到的 $a^{[2]}​$ 就是整个神经网络的输出。</li></ul><p>同样，还需要通过反向传播计算 $da^{[2]}, dz^{[2]}$ 等等，这些将会在后面详细讨论。</p><h3 id="One-hidden-layer-Neural-Network"><a href="#One-hidden-layer-Neural-Network" class="headerlink" title="One hidden layer Neural Network"></a><strong>One hidden layer Neural Network</strong></h3><p>下图是一张单隐藏层的神经网络，也称为双层神经网络 (2 layer NN) 。我们把最左边的 $x1, x2, x3$ 称为输入层 （Input Layer) ，中间称为隐藏层 (Hidden Layer) ，最右边只有一个节点的称为输出层 (Output Layer) ，负责输出预测值 $\hat{y}$ 。在计算神经网络的层数时，不算入输入层。</p><p><img src="/2019/02/01/Shallow-Neural-Networks-1/3.png" alt=""></p><p>由于在训练过程中，我们看不到这些中间节点的真正数值，不像输入，输出层那样，所以称为隐藏层。</p><p>之前，我们用 $x$ 来表示输入，其实它还有一种表示方式 $a^{[0]}$ ，这个 $a$ 有 activation (激活) 的意思，意味这它把不同层的值传递给下一层，起到了激活的作用。用上标 $[i]$ 表示在第 $i$ 层，用下标 $j$ 表示这层中第 $j$ 个节点，如 $a^{[1]}_{2}$ 即表示第 $1$ 层的第 $2$ 个节点。那么上图中隐藏层的4个节点可以写成矩阵的形式：</p><script type="math/tex; mode=display">a^{[1]} =\left( \begin{array}{c}a^{[1]}_{1} \\\a^{[1]}_{2} \\\a^{[1]}_{3} \\\a^{[1]}_{4}\end{array} \right)</script><h3 id="Computing-a-Neural-Network’s-Output"><a href="#Computing-a-Neural-Network’s-Output" class="headerlink" title="Computing a Neural Network’s Output"></a><strong>Computing a Neural Network’s Output</strong></h3><p>接下来，我们来看神经网络的输出是如何计算出来的。我们可以把神经网络的计算看作对数几率回归的多次重复计算。</p><p>我们先来回顾一下对数几率回归的计算过程，如下图：</p><p><img src="/2019/02/01/Shallow-Neural-Networks-1/4.png" alt=""></p><p>这里的圆圈代表了，对数几率回归的两个步骤。我们先隐去其他节点，如右图，那么它就和对数几率回归非常相似，我们可以计算出 $z^{[1]}_{1} = w^{[1]T}_{1}x+b^{[1]}_{1}$ ， $a^{[1]}_{1} = \sigma(z^{[1]}_{1})$ ，上标代表层数，下标表示这层上的第几个节点。</p><p>以此类推，我们可以写出：</p><p><img src="/2019/02/01/Shallow-Neural-Networks-1/5.png" alt=""></p><p>回想起之前所讲的向量化，如果我们想让程序高效的运行，就必须将其向量化。</p><p>我们首先先将 $w$ 向量化，由于有4个对数几率回归单元，而每一个回归单元都有其对应的参数向量 $w$ ，且每一个回归单元都有输入 $x_1, x_2, x_3​$ ，所以我们可以得到：</p><script type="math/tex; mode=display">W^{[1]} =\left( \begin{array}{c}w^{[1]T}_{1} \\\w^{[1]T}_{2} \\\w^{[1]T}_{3} \\\w^{[1]T}_{4}\end{array} \right)_{(4 \times 3)}</script><p>那么，我们可以得到如下式子：</p><script type="math/tex; mode=display">Z^{[1]} =\left( \begin{array}{c}w^{[1]T}_{1} \\\w^{[1]T}_{2} \\\w^{[1]T}_{3} \\\w^{[1]T}_{4}\end{array} \right)\cdot\left( \begin{array}{c}x_1 \\\x_2 \\\x_3\end{array} \right)+\left( \begin{array}{c}b^{[1]}_{1} \\\b^{[1]}_{2} \\\b^{[1]}_{3} \\\b^{[1]}_{4}\end{array} \right)=\left( \begin{array}{c}w^{[1]T}_{1}x+b^{[1]}_{1} \\\w^{[1]T}_{2}x+b^{[1]}_{2} \\\w^{[1]T}_{3}x+b^{[1]}_{3} \\\w^{[1]T}_{4}x+b^{[1]}_{4}\end{array} \right)=\left( \begin{array}{c}z^{[1]}_{1} \\\z^{[1]}_{2} \\\z^{[1]}_{3} \\\z^{[1]}_{4}\end{array} \right)</script><script type="math/tex; mode=display">a^{[1]} =\left( \begin{array}{c}a^{[1]}_{1} \\\a^{[1]}_{2} \\\a^{[1]}_{3} \\\a^{[1]}_{4}\end{array} \right)=\sigma(Z^{[1]})=\sigma(W^{[1]}x + b^{[1]})</script><p>在本神经网络中，你就应该计算 (为了更好理解，右下角标注了矩阵的形状) ：</p><script type="math/tex; mode=display">z^{[1]}_{(4 \times 1)} = W^{[1]}_{(4 \times 3)}x_{(3 \times 1)}+ b^{[1]}_{(4 \times 1)}</script><script type="math/tex; mode=display">a^{[1]}_{4 \times 1} = \sigma(z^{[1]}_{(4 \times 1)})</script><p>记得我们之前说过，可以用 $a^{[0]}​$表示 $x​$ ，所以 $z^{[1]}​$ 有可以写成 $z^{[1]} = W^{[1]}a^{[0]}+ b^{[1]}​$ ，那么可以用同样的方法推导出：</p><script type="math/tex; mode=display">z^{[2]}_{(1 \times 1)} = W^{[2]}_{(1 \times 4)}a^{[1]}_{(4 \times 1)}+ b^{[2]}_{(1 \times 1)}</script><script type="math/tex; mode=display">a^{[2]}_{1 \times 1} = \sigma(z^{[2]}_{(1 \times 1)})</script><p>所以在代码中，我们只需实现上述的4行代码。然而这是对单个样本的，即对于输入的特征向量 $x$ ，可以计算出 $\hat{y} = a^{[2]}​$ ，对于整个训练集的向量化将在接下来的部分介绍。</p><h3 id="Vectorizing-across-multiple-examples"><a href="#Vectorizing-across-multiple-examples" class="headerlink" title="Vectorizing across multiple examples"></a><strong>Vectorizing across multiple examples</strong></h3><p>接下来，我们实现对整个训练集的向量化。假设你有 $m​$ 个训练样本，理解了上述理论，那么我们可以通过输入 $x^{(i)}​$ 计算得出 $ \hat{y}^{(i)} = a^{[2](i)}​$ 其中 $i \in [1,m]​$ 。</p><p>还记得我们之前把输入的特征向量 $x^{(1)}, x^{(2)}, …, x^{(m)}​$ 横向堆叠起来，得到了一个 $(n_x \times m)​$ 的矩阵 $X​$ ，即</p><script type="math/tex; mode=display">\mathbf{X} =\left( \begin{array}{c}\vdots & \vdots & \ldots & \vdots \\\\x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\\vdots & \vdots & \ldots & \vdots\end{array} \right)_{(n_x \times m)}</script><p>同样的，我们把 $x^{(1)}, x^{(2)}, …, x^{(m)}​$ 横向堆叠起来，那么我们需要计算的式子变为：</p><script type="math/tex; mode=display">Z^{[1]} = W^{[1]}X + b^{[1]}</script><script type="math/tex; mode=display">A^{[1]} = \sigma(Z^{[1]})</script><script type="math/tex; mode=display">Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}</script><script type="math/tex; mode=display">A^{[2]} = \sigma(Z^{[2]})</script><p>其中，$Z^{[1]}​$ 的形状为 $(4 \times m)​$ ，你可以这样理解，每一个样本对应着矩阵的一列，所以有 $m​$ 列，隐藏层中的每一个节点对应着矩阵的一行，所以 $Z^{[1]}​$ 有 $4​$ 行，规律同样适用于 $X, A​$。</p><h3 id="Activation-functions"><a href="#Activation-functions" class="headerlink" title="Activation functions"></a><strong>Activation functions</strong></h3><p>到目前为止，我们一直选择 sigmoid 函数作为 activation function (激活函数) ，但有时使用其他函数效果要好得多，它们各自有不同的特点，下面我们来介绍几个不同的激活函数 $g(x)$：</p><p><img src="/2019/02/01/Shallow-Neural-Networks-1/6.png" alt=""></p><ul><li>tanh (双曲正切函数)，实际上是 sigmoid 函数向下平移后再经过拉伸得到的。对于隐藏单元，如果你选择 tanh 作为激活函数，它的表现几乎总是比 sigmoid 函数要好，因为 tanh 函数的输出介于 $(-1,1)$ 之间，激活函数的平均值更接近于 $0$ ，而不是 $0.5$ ，这让下一层的学习更方便一点。所以之后我们几乎不再使用 sigmoid 作为激活函数了，但有一个例外，即选择输出层的激活函数的时候，因为二分类问题的输出为 $\{0,1\}$ ，你更希望 $\hat{y}$ 介于 $0,1$ 之间，所以一般会选择 sigmoid 函数。</li><li>所以之前所举的例子中，你可以使用 tanh 函数作为隐藏层的激活函数，而选择 sigmoid 函数作为输出层的激活函数。同样的，可以使用上标来表示每一层的激活函数，如：$g^{[1]}(x) = tanh(z), g^{[2]} = \sigma(z)​$ </li><li>然而，不管是 tanh 还是 sigmoid 函数，都有一个缺点，如果 $z​$ 特别大或者特别小，那么在这一点的函数导数会很小，因此会拖慢梯度下降算法。为了弥补这个缺点，就出现了 ReLU (rectified linear unit) 函数，该函数有如下特点：当 $z​$ 为正，导数为 $1​$，当 $z​$ 为负，导数为 $0​$ ，当 $z​$ 为 $0​$ 时，导数不存在，但在实际使用中， $z​$ 几乎不会等于 $0​$ ，当然你可以在程序中直接把在 $z=0​$ 点的导数赋为 $0​$ 或 $1​$ 。</li><li>但 ReLU 也有一个缺点，当 $z$ 为负时，导数恒等于 $0$ 。所以就有了 Leaky ReLU 函数，通常表现比 ReLU 函数更好，但实际使用中频率没那么高。</li><li>ReLU 和 Leaky ReLU 的优势在于，对于 $z$ 的许多取值，激活函数的导数和 $0$ 差的很远，这也就意味着，在实践中你的神经网络的学习速度会快很多。</li><li>总结一下，在我们一般选择 sigmoid 作为输出层的激活函数，而选择 ReLU 作为其他层的激活函数，ReLU 如今被人们广泛使用。</li></ul>]]></content>
      
      
      <categories>
          
          <category> deeplearning.ai </category>
          
          <category> Deep learning &amp; NN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Basic of Neural Networks-2</title>
      <link href="/2019/01/26/Basic-of%20Neural-Networks-2/"/>
      <url>/2019/01/26/Basic-of%20Neural-Networks-2/</url>
      
        <content type="html"><![CDATA[<p>本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第二周神经网络基础相关课程及第二周作业。</p><h3 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a><strong>Vectorization</strong></h3><p>Vectorization (向量化) 的意义在于：消除代码中显式的调用for循环。在深度学习领域中，你常常需要训练大数据集，所以程序运行的效率非常重要，否则需要等待很长时间才能得到结果。<br></p><a id="more"></a><p>在对数几率回归中，你需要计算 $z = w^{T}x + b$ ，其中 $w, x$ 都是 $n_x$ 维向量</p><blockquote><p>如果你是用非向量化的实现，即传统的矩阵相乘算法伪代码如下：<br>$ z = 0 $<br>$ for  \quad i  \quad in  \quad range(n_x) : $<br>$ \quad z+= w[i] * x[i] $<br>$z+=b ​$</p><p>若使用向量化的实现，Python代码如下：<br><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z = np.dot(w,x) + b</span><br></pre></td></tr></table></figure></p><p>即清晰又高效</p></blockquote><p>可以测试一下这两个代码在效率方面的差距，大约差了300倍。可以试想一下，如果你的代码1分钟出结果，和5个小时才出结果，那可差太远了。<br></p><p>为了加快深度学习的运算速度，可以使用GPU (Graphic Processing Unit) 。事实上，GPU 和 CPU 都有并行指令 (Parallelization Instructions) ，同时也叫作 SIMD (Single Instruction Multiple Data)，即单指令流多数据流，是一种采用一个控制器来控制多个处理器，同时对一组数据 (又称“数据向量”) 中的每一个分别执行相同的操作从而实现空间上的并行性的技术。numpy 是 Python 数据分析及科学计算的基础库，它有许多内置 (Built-in) 函数，主要用于数组的计算，充分利用了并行化，使得运算速度大大提高。在深度学习的领域，一般来说，能不用显式的调用for循环就不用。<br></p><p>这样，我们就可以使用 Vectorization 来优化梯度下降算法，先去掉内层对 feature (特征 $w1, w2 …$) 的循环 ：</p><blockquote><p>$J=0; db=0; dw = np.zeros(n_x,1)$<br>$for \quad i = 1 \quad to \quad m $<br>$\quad z^{(i)} = w^{T}x^{(i)}+b$<br>$\quad a^{(i)} = \sigma(z^{(i)})$<br>$\quad J += -(y^{(i)} \log a^{(i)} + (1-y^{(i)}) \log(1-a^{(i)}))$<br>$\quad dz^{(i)} = a^{(i)}-y^{(i)}$<br>$\quad dw+=x^{(i)}dz^{(i)} \quad \quad $ //vectorization<br>$\quad db  += dz^{(i)}$<br>$J /= m$<br>$dw /= m$<br>$db /= m​$</p></blockquote><p>然后，我们再去掉对 $m$ 个训练样本的外层循环，分别从正向传播和反向传播两方面来分析：</p><blockquote><h5 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a><strong>正向传播</strong></h5><p>回顾一下对数几率回归的正向传播步骤，如果你有 $m$ 个训练样本</p><p>那么对第一个样本进行预测，你需要计算<br>$ \quad z^{(1)} = w^{T}x^{(1)} + b$<br>$ \quad a^{(1)} = \sigma(z^{(1)})​$</p><p>然后继续对第二个样本进行预测<br>$ \quad z^{(2)} = w^{T}x^{(2)} + b$<br>$ \quad a^{(2)} = \sigma(z^{(2)})$</p><p>然后继续对第三个，第四个，…，直到第 $m$ 个</p><p>回忆一下之前在二分分类部分所讲到的用更紧凑的符号 $X​$ 表示整个训练集，即大小为 $(n_x,m)​$ 的矩阵 : </p><script type="math/tex; mode=display">\mathbf{X} =\left( \begin{array}{c}\vdots & \vdots & \ldots & \vdots \\\\x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\\vdots & \vdots & \ldots & \vdots\end{array} \right)</script><p>那么计算 $z^{(1)}, z^{(2)}, … , z^{(m)}$ 的步骤如下 :<br>首先先构造一个 $(1,m)$ 的矩阵 $[z^{(1)}, z^{(2)}, … , z^{(m)}]$ ，则 </p><script type="math/tex; mode=display">Z = [z^{(1)}, z^{(2)}, ... , z^{(m)}] = w^{T}X + [b, b , ... , b] = [w^{T}x^{(1)} + b, w^{T}x^{(2)} + b] , ... , w^{T}x^{(m)} + b]</script><p>在 Python 中一句代码即可完成上述过程<br><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T,X) + b</span><br></pre></td></tr></table></figure></p><p>你可能会有疑问，明明这里的 $b$ 只是一个实数 (或者说是一个 $b_{(1,1)}$ 的矩阵) ，为什么可以和矩阵 $Z_{(1,m)}$ 相加？事实上，当做 $Z+b$ 这个操作时，Python 会自动把矩阵 $b_{(1,1)}$ 自动扩展为 $b_{(1,m)}$ 这样的一个行向量，在 Python 中这称为 Broadcasting (广播) ，现在你只要看得懂就好，接下来会更详细地说明它。</p><p>同理我们可以得到</p><script type="math/tex; mode=display">A = [a^{(1)}, a^{(2)}, ... , a^{(m)}] = \sigma (Z)</script><p>同样也只需一句 Python 代码</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A = sigmoid(Z)</span><br></pre></td></tr></table></figure><h5 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a><strong>反向传播</strong></h5><p>接着，我们来看如何用向量化优化反向传播，计算梯度</p><p>同样，你需要计算<br>$dz^{(1)} = a^{(1)} - y^{(1)}​$<br>$dz^{(2)} = a^{(2)} - y^{(2)}​$<br>$…​$<br>一直到第 $m​$ 个</p><p>即计算 $dZ = [dz^{(1)} , dz^{(2)} , … , dz^{(m)}]$</p><p>之前我们已经得到 $A = [a^{(1)}, a^{(2)}, … , a^{(m)}] = \sigma (Z)​$<br>我们再定义输出标签 $Y = [y^{(1)}, y^{(2)}, … , y^{(m)}]​$</p><p>那么，</p><script type="math/tex; mode=display">dZ = A-Y = [a^{(1)}-y^{(1)} , a^{(2)}-y^{(2)} , ... , a^{(m)}-y^{(m)}]</script><p>有了 $dZ$ 我们就可以计算 $dw, db​$<br>根据之前的公式，有 </p><script type="math/tex; mode=display">db = \frac{1}{m} \sum_{i=1}^{m}dz^{(i)}</script><script type="math/tex; mode=display">dw = \frac{1}{m}X \cdot dZ^{T} = \frac{1}{m}\left( \begin{array}{c}\vdots & \vdots & \ldots & \vdots \\\\x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\\vdots & \vdots & \ldots & \vdots\end{array} \right)\left( \begin{array}{c}dz^{(1)} \\\\\vdots   \\\\dz^{(m)}\end{array} \right)= \frac{1}{m}[x^{(1)}dz^{(1)} + ...+ x^{(m)}dz^{(m)}]</script><p>对应的 Python 代码即为<br><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db = np.sum(dZ)/m</span><br><span class="line">dw = np.dot(x,dZ.T)/m</span><br></pre></td></tr></table></figure></p><p>最后，我们可以得到向量化后的梯度下降算法<br><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line">Z = np.dot(w.T,X) + b</span><br><span class="line">A = sigmoid(Z)</span><br><span class="line">dw = np.dot(x,dZ.T)/m</span><br><span class="line">db = np.sum(dZ)/m</span><br><span class="line">w = w - alpha * dw</span><br><span class="line">b = b - alpha * db</span><br></pre></td></tr></table></figure></p><p>你可能会有疑问，为什么这里不需要再计算 Cost function (代价函数) $J$ 了，笔者认为 $J$ 只是对数几率回归模型所需要的损失函数，借助它我们才能计算出 $dw, db$ ，从而进行迭代。在后续的作业中，计算 $J​$ 可以帮助我们对模型进一步分析。<br>这样，我们就完成了对数几率回归的梯度下降的一次迭代，但如果你需要多次执行迭代的操作，只能显式的调用for循环。</p></blockquote><h3 id="Broadcasting"><a href="#Broadcasting" class="headerlink" title="Broadcasting"></a><strong>Broadcasting</strong></h3><p>Broadcasting (广播) 机制主要是为了方便不同 shape 的 array (可以理解为不同形状的矩阵) 进行数学运算</p><ul><li>当我们将向量和一个常量做加减乘除操作时，比如对数几率回归中的 $w^{T}x+b$ ，会对向量中的每一格元素都和常量做一次操作，或者你可以理解为把这个数复制 $m \times n$ 次，使其变为一个形状相同的 $(m,n)$ 矩阵，如 :</li></ul><script type="math/tex; mode=display">\begin{bmatrix}1 \\ 2 \\ 3 \\ 4\end{bmatrix} + 100 => \begin{bmatrix}1 \\ 2 \\ 3 \\ 4\end{bmatrix} + \begin{bmatrix}100 \\ 100 \\ 100 \\ 100\end{bmatrix} = \begin{bmatrix}101 \\ 102 \\ 103 \\ 104\end{bmatrix}</script><ul><li>一个 $(m,n)$ 矩阵和一个 $(1,n)$ 矩阵相加 (减乘除)，会将这个 $(1,n)$ 矩阵复制 $m$ 次，使其变为 $(m,n)$ 矩阵然后相加，如 :</li></ul><script type="math/tex; mode=display">\begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix} 100 & 200 & 300\end{bmatrix} => \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix} 100 & 200 & 300 \\ 100 & 200 & 300\end{bmatrix} = \begin{bmatrix}101 & 202 & 303 \\ 104 & 205 & 306\end{bmatrix}</script><ul><li>同样地，一个 $(m,n)$ 矩阵和一个 $(m,1)$ 矩阵相加 (减乘除)，会将这个 $(m,1)$ 矩阵复制 $n$ 次，使其变为 $(m,n)$ 矩阵然后相加</li></ul><script type="math/tex; mode=display">\begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix}100 \\ 200\end{bmatrix} => \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix} 100 & 100 & 100 \\ 200 & 200 & 200\end{bmatrix} = \begin{bmatrix}101 & 102 & 103 \\ 204 & 205 & 206\end{bmatrix}</script><p>通俗的讲，numpy 会通过复制的方法，使两个不同形状的矩阵变得一致，再执行相关操作。值得一提的是，为了保证运算按照我们的想法进行，使用 reshape() 函数是一个较好的习惯。</p><h3 id="Homework"><a href="#Homework" class="headerlink" title="Homework"></a><strong>Homework</strong></h3><p>附上所有代码：</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># LogisticRegression.py</span></span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">import</span> h5py</span><br><span class="line"><span class="hljs-keyword">import</span> scipy</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image</span><br><span class="line"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> ndimage</span><br><span class="line"><span class="hljs-keyword">from</span> lr_utils <span class="hljs-keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Sigmoid 函数</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(z)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Compute the sigmoid of z</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    z -- A scalar or numpy array of any size.</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Return:</span></span><br><span class="line"><span class="hljs-string">    s -- sigmoid(z)</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    s = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-z))</span><br><span class="line">    <span class="hljs-keyword">return</span> s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 初始化 w,b</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize_with_zeros</span><span class="hljs-params">(dim)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Argument:</span></span><br><span class="line"><span class="hljs-string">    dim -- size of the w vector we want (or number of parameters in this case)</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    w -- initialized vector of shape (dim, 1)</span></span><br><span class="line"><span class="hljs-string">    b -- initialized scalar (corresponds to the bias)</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    w = np.zeros((dim, <span class="hljs-number">1</span>))  <span class="hljs-comment"># (dim, 1) 是shape参数，代表初始化一个dim*1的矩阵</span></span><br><span class="line">    b = <span class="hljs-number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">assert</span> (w.shape == (dim, <span class="hljs-number">1</span>))</span><br><span class="line">    <span class="hljs-keyword">assert</span> (isinstance(b, float) <span class="hljs-keyword">or</span> isinstance(b, int))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> w, b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># propagate 正向与反向传播</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">propagate</span><span class="hljs-params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Implement the cost function and its gradient for the propagation explained above</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="hljs-string">    b -- bias, a scalar</span></span><br><span class="line"><span class="hljs-string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="hljs-string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">    Return:</span></span><br><span class="line"><span class="hljs-string">    cost -- negative log-likelihood cost for logistic regression</span></span><br><span class="line"><span class="hljs-string">    dw -- gradient of the loss with respect to w, thus same shape as w</span></span><br><span class="line"><span class="hljs-string">    db -- gradient of the loss with respect to b, thus same shape as b</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Tips:</span></span><br><span class="line"><span class="hljs-string">    - Write your code step by step for the propagation. np.log(), np.dot()</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="hljs-number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># FORWARD PROPAGATION (FROM X TO COST)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T, X) + b)  <span class="hljs-comment"># compute activation</span></span><br><span class="line">    cost = <span class="hljs-number">-1</span> / m * (np.dot(Y,np.log(A).T) + np.dot(<span class="hljs-number">1</span> - Y,np.log(<span class="hljs-number">1</span> - A).T))  <span class="hljs-comment"># compute cost</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># BACKWARD PROPAGATION (TO FIND GRAD)</span></span><br><span class="line">    dw = <span class="hljs-number">1</span> / m * np.dot(X, (A - Y).T)</span><br><span class="line">    db = <span class="hljs-number">1</span> / m * np.sum(A - Y)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">assert</span> (dw.shape == w.shape)</span><br><span class="line">    <span class="hljs-keyword">assert</span> (db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost)  <span class="hljs-comment"># 删除shape为1的维度，比如cost=[[1]]，则经过np.squeeze处理后cost=[1]</span></span><br><span class="line">    <span class="hljs-keyword">assert</span> (cost.shape == ())</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="hljs-string">"dw"</span>: dw, <span class="hljs-string">"db"</span>: db&#125;</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> grads, cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 梯度下降</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">optimize</span><span class="hljs-params">(w, b, X, Y, num_iterations, learning_rate, print_cost=False)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    This function optimizes w and b by running a gradient descent algorithm</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="hljs-string">    b -- bias, a scalar</span></span><br><span class="line"><span class="hljs-string">    X -- data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="hljs-string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="hljs-string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="hljs-string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="hljs-string">    print_cost -- True to print the loss every 100 steps</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    params -- dictionary containing the weights w and bias b</span></span><br><span class="line"><span class="hljs-string">    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function</span></span><br><span class="line"><span class="hljs-string">    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Tips:</span></span><br><span class="line"><span class="hljs-string">    You basically need to write down two steps and iterate through them:</span></span><br><span class="line"><span class="hljs-string">        1) Calculate the cost and the gradient for the current parameters. Use propagate().</span></span><br><span class="line"><span class="hljs-string">        2) Update the parameters using gradient descent rule for w and b.</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Cost and gradient calculation</span></span><br><span class="line">        grads, cost = propagate(w, b, X, Y)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Retrieve derivatives from grads</span></span><br><span class="line">        dw = grads[<span class="hljs-string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="hljs-string">"db"</span>]</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># update rule</span></span><br><span class="line">        w = w - dw * learning_rate</span><br><span class="line">        b = b - db * learning_rate</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Record the costs</span></span><br><span class="line">        <span class="hljs-keyword">if</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Print the cost every 100 training examples</span></span><br><span class="line">        <span class="hljs-keyword">if</span> print_cost <span class="hljs-keyword">and</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:</span><br><span class="line">            print(<span class="hljs-string">"Cost after iteration %i: %f"</span> % (i, cost))</span><br><span class="line"></span><br><span class="line">    params = &#123;<span class="hljs-string">"w"</span>: w, <span class="hljs-string">"b"</span>: b&#125;</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="hljs-string">"dw"</span>: dw, <span class="hljs-string">"db"</span>: db&#125;</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> params, grads, costs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 利用logistic regression判断Y的标签值</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(w, b, X)</span>:</span></span><br><span class="line">    <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="hljs-string">    b -- bias, a scalar</span></span><br><span class="line"><span class="hljs-string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X</span></span><br><span class="line"><span class="hljs-string">    '''</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="hljs-number">1</span>]</span><br><span class="line">    Y_prediction = np.zeros((<span class="hljs-number">1</span>, m))</span><br><span class="line">    w = w.reshape(X.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Compute vector "A" predicting the probabilities of a cat being present in the picture</span></span><br><span class="line">    A = sigmoid(np.dot(w.T, X) + b)  <span class="hljs-comment"># A.shape = (1,m)</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(A.shape[<span class="hljs-number">1</span>]):</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Convert probabilities A[0,i] to actual predictions p[0,i]</span></span><br><span class="line">        <span class="hljs-keyword">if</span> A[<span class="hljs-number">0</span>, i] &gt; <span class="hljs-number">0.5</span>:</span><br><span class="line">            Y_prediction[<span class="hljs-number">0</span>, i] = <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="hljs-number">0</span>, i] = <span class="hljs-number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">assert</span> (Y_prediction.shape == (<span class="hljs-number">1</span>, m))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> Y_prediction</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 构建整个模型</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">model</span><span class="hljs-params">(X_train,</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-params">          Y_train,</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-params">          X_test,</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-params">          Y_test,</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-params">          num_iterations=<span class="hljs-number">2000</span>,</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-params">          learning_rate=<span class="hljs-number">0.5</span>,</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-params">          print_cost=False)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Builds the logistic regression model by calling the function you've implemented previously</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Arguments:</span></span><br><span class="line"><span class="hljs-string">    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span></span><br><span class="line"><span class="hljs-string">    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)</span></span><br><span class="line"><span class="hljs-string">    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span></span><br><span class="line"><span class="hljs-string">    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)</span></span><br><span class="line"><span class="hljs-string">    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters</span></span><br><span class="line"><span class="hljs-string">    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()</span></span><br><span class="line"><span class="hljs-string">    print_cost -- Set to true to print the cost every 100 iterations</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    d -- dictionary containing information about the model.</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># initialize parameters with zeros</span></span><br><span class="line">    w, b = initialize_with_zeros(X_train.shape[<span class="hljs-number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Gradient descent</span></span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations,</span><br><span class="line">                                        learning_rate, print_cost)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Retrieve parameters w and b from dictionary "parameters"</span></span><br><span class="line">    w = parameters[<span class="hljs-string">"w"</span>]</span><br><span class="line">    b = parameters[<span class="hljs-string">"b"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Predict test/train set examples</span></span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line">    Y_prediction_train = predict(w, b, X_train)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># Print train/test Errors</span></span><br><span class="line">    print(<span class="hljs-string">"train accuracy: &#123;&#125; %"</span>.format(</span><br><span class="line">        <span class="hljs-number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="hljs-number">100</span>))</span><br><span class="line">    print(<span class="hljs-string">"test accuracy: &#123;&#125; %"</span>.format(</span><br><span class="line">        <span class="hljs-number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="hljs-number">100</span>))</span><br><span class="line"></span><br><span class="line">    d = &#123;</span><br><span class="line">        <span class="hljs-string">"costs"</span>: costs,</span><br><span class="line">        <span class="hljs-string">"Y_prediction_test"</span>: Y_prediction_test,</span><br><span class="line">        <span class="hljs-string">"Y_prediction_train"</span>: Y_prediction_train,</span><br><span class="line">        <span class="hljs-string">"w"</span>: w,</span><br><span class="line">        <span class="hljs-string">"b"</span>: b,</span><br><span class="line">        <span class="hljs-string">"learning_rate"</span>: learning_rate,</span><br><span class="line">        <span class="hljs-string">"num_iterations"</span>: num_iterations</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span></span><br><span class="line">    train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset(</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    m_train = train_set_x_orig.shape[<span class="hljs-number">0</span>]</span><br><span class="line">    m_test = test_set_x_orig.shape[<span class="hljs-number">0</span>]</span><br><span class="line">    num_px = train_set_x_orig.shape[<span class="hljs-number">2</span>]</span><br><span class="line"></span><br><span class="line">    train_set_x_flatten = train_set_x_orig.reshape(m_train, <span class="hljs-number">-1</span>).T</span><br><span class="line">    test_set_x_flatten = test_set_x_orig.reshape(m_test, <span class="hljs-number">-1</span>).T</span><br><span class="line">    train_set_x = train_set_x_flatten / <span class="hljs-number">255.</span></span><br><span class="line">    test_set_x = test_set_x_flatten / <span class="hljs-number">255.</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># train model</span></span><br><span class="line">    d = model(</span><br><span class="line">        train_set_x,</span><br><span class="line">        train_set_y,</span><br><span class="line">        test_set_x,</span><br><span class="line">        test_set_y,</span><br><span class="line">        num_iterations=<span class="hljs-number">2000</span>,</span><br><span class="line">        learning_rate=<span class="hljs-number">0.005</span>,</span><br><span class="line">        print_cost=<span class="hljs-keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># 判断单张图片是否有猫</span></span><br><span class="line">    my_image = <span class="hljs-string">"a.jpg"</span>  <span class="hljs-comment"># change this to the name of your image file</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># We preprocess the image to fit your algorithm.</span></span><br><span class="line">    fname = <span class="hljs-string">"images/"</span> + my_image</span><br><span class="line">    image = np.array(ndimage.imread(fname, flatten=<span class="hljs-keyword">False</span>))</span><br><span class="line">    my_image = scipy.misc.imresize(image, size=(num_px, num_px)).reshape((<span class="hljs-number">1</span>, num_px * num_px * <span class="hljs-number">3</span>)).T</span><br><span class="line">    my_predicted_image = predict(d[<span class="hljs-string">"w"</span>], d[<span class="hljs-string">"b"</span>], my_image)</span><br><span class="line"></span><br><span class="line">    print(<span class="hljs-string">"y = "</span> + str(np.squeeze(my_predicted_image)) + <span class="hljs-string">", your algorithm predicts a \""</span> + classes[</span><br><span class="line">        int(np.squeeze(my_predicted_image)),].decode(<span class="hljs-string">"utf-8"</span>) + <span class="hljs-string">"\" picture."</span>)</span><br><span class="line"></span><br><span class="line">    plt.imshow(image)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> deeplearning.ai </category>
          
          <category> Deep learning &amp; NN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Basic of Neural Networks-1</title>
      <link href="/2019/01/21/Basic-of-Neural-Networks-1/"/>
      <url>/2019/01/21/Basic-of-Neural-Networks-1/</url>
      
        <content type="html"><![CDATA[<p>本文为 Andrew Ng 深度学习课程第一部分神经网络和深度学习的笔记，对应第二周神经网络基础的相关课程。</p><h3 id="Binary-Classification"><a href="#Binary-Classification" class="headerlink" title="Binary Classification"></a><strong>Binary Classification</strong></h3><p>在二分分类问题中，目标是训练出一个分类器，以特征向量<code>x (feature vector)</code>为输入，以<code>y (output label)</code>为输出，<code>y</code>一般只有 ${0,1}​$ 两个离散值。以图像识别问题为例，判断图片中是否由猫存在，0代表noncat，1代表cat<br></p><a id="more"></a><p>通常，我们用 $(x,y)​$ 来表示一个单独的样本，其中<code>x(feature vector)</code>是$n_x​$维的向量 ( $n_x​$ 为样本特征个数，即决定输出的因素) ，<code>y(output label)</code>为输出，取值为 $y\in\{0,1\}​$<br><br>则m个训练样本 (training example) 可表示为</p><script type="math/tex; mode=display">\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\}</script><p>用$ m=m_{train} $表示训练样本的个数<br></p><p>最后，我们可以用更紧凑的符号 $X$ 表示整个训练集，$X$ 由训练集中的 $x^{(1)}$，$x^{(2)}$，…，$x^{(m)}$ 作为列向量组成，$X\in{\Bbb R}^{n_x*m}$，即 X.shape = $(n_x,m)$</p><script type="math/tex; mode=display">\mathbf{X} =\left( \begin{array}{c}\vdots & \vdots & \ldots & \vdots \\\\x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\\vdots & \vdots & \ldots & \vdots\end{array} \right)</script><p>同时，把<code>y</code>也放入列中，用 $Y$ 来表示，$Y\in{\Bbb R}^{1*m}$，即 Y.shape = $(1,m)​$</p><script type="math/tex; mode=display">\mathbf{Y} =\left( \begin{array}{c}y^{(1)} & y^{(2)} & \ldots & y^{(m)} \\end{array} \right)</script><h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a><strong>Logistic Regression</strong></h3><p>参照了周志华的西瓜书，把 Logisitic Regression 翻译为对数纪律回归，简称为对率回归。对数几率回归是一种解决二分分类问题的机器学习方法，用于预测某种实物的可能性。<br></p><blockquote><p>Given x, you want $\hat{y} = P(y=1 \mid x)​$. </p><p>In other words, if x is a picture, as talked about above, you want $\hat{y}​$ to tell you the chance that there is a cat in the picture.</p></blockquote><p>根据输入 $x$ 和参数 $w, b$，计算出 $\hat{y}$ ，下面介绍了两种方法 :</p><blockquote><p>Parameter : $w\in{\Bbb R}^{n_x}, b\in{\Bbb R}​$<br>Output : $\hat{y}​$</p><blockquote><p>One way : $\hat{y} = w^{T}x + b​$ (Linear regression) </p><ul><li>Not good for binary classification</li><li>Because you want $\hat{y}​$ to be the chance that $y​$ equals to one. In this situation  $\hat{y}​$ can be much bigger than 1 or negative.</li></ul><p>The other way : $\hat{y} = \sigma(w^{T}x + b)$ (Logistic Regression) </p><ul><li>$\sigma(z) =  \frac{1}{1+e^{-z}} ​$</li><li>通过$\sigma(z)$函数，可以将输出限定在$[0,1]$之间</li></ul></blockquote></blockquote><h3 id="Logistic-Regression-Cost-Function"><a href="#Logistic-Regression-Cost-Function" class="headerlink" title="Logistic Regression Cost Function"></a><strong>Logistic Regression Cost Function</strong></h3><p>给出$\{(x^{(1)},y^{(1)})…,(x^{(m)},y^{(m)})\}​$，希望通过训练集，找到参数 $w, b​$ 使得 $\hat{y}^{(i)} \approx  y^{(i)}​$ 。所以，我们需要定义一个<code>loss function</code>，通过这个<code>loss function</code>来衡量你的预测输出值 $\hat{y}​$ 与 $y​$ 的实际值由多接近 <br></p><p>对于m个训练样本，我们通常用上标 $(i)​$ 来指明数据与第 $i​$ 个样本有关。<br></p><p>通常，我们这样定义Loss function (损失函数) :</p><script type="math/tex; mode=display">L(\hat{y},y) = \frac{1}{2}(\hat{y} - y)^2</script><p>但在对数几率回归中一般不使用，因为它是non-convex (非凸的) ，将来使用<code>梯度下降算法 (Gradient Descent)</code>时无法找到全局最优值 <br></p><p>在对数几率回归中，我们使用的损失函数为 : </p><script type="math/tex; mode=display">L(\hat{y},y) = -(y \log\hat{y} + (1-y) \log(1-\hat{y}))</script><blockquote><p>If y = 1 : $L(\hat{y},y) = -\log(\hat{y})​$, you want $\hat{y}​$ to be large</p><p>if y = 0 : $L(\hat{y},y) = -\log(1-\hat{y})​$, you want  $\hat{y}​$ to be small</p><p>所以，这个损失函数和 $L(\hat{y},y) = \frac{1}{2}(\hat{y} - y)^2$ 类似，都希望 $L$ 越小越好</p></blockquote><p>上述的<code>Loss function</code>衡量了单个训练样本的表现，对于m个样本，我们定义<code>Cost function</code> (代价函数) ，它衡量了全体训练样本的表现</p><script type="math/tex; mode=display">J(w,b) = \frac{1}{m} \sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)}) = -\frac{1}{m} \sum_{i=1}^{m}y^{(i)} \log\hat{y}^{(i)} + (1-y^{(i)}) \log(1-\hat{y}^{(i)})</script><p><code>Loss function</code>只适用于单个训练样本，<code>Cost function</code>是基于参数的总代价。所以，在训练对数几率回归模型时，我们要找到合适的参数 $w, b$ 使得<code>Cost function</code>尽可能的小<br></p><h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a><strong>Gradient Descent</strong></h3><p>我们将使用<code>梯度下降 (Gradient Descent) 算法</code>来找出合适的参数 $w,b$，使得<code>Cost function</code> 即 $J(w,b)$ 最小 <br></p><ul><li><img src="/2019/01/21/Basic-of-Neural-Networks-1/gradient_descent.png" alt=""></li></ul><p>最上方的小红点为初始点，对于对数几率回归，一般使用0来初始化，随机初始化也有效，但通常不这么做 <br></p><blockquote><p>梯度下降过程：</p><ul><li>从初始点开始，朝最陡的下坡方向走一步</li><li>重复上述过程，不断修正 $w, b​$ 使得 $J(w,b)​$ 接近全局最优值 (global opitmal)</li></ul></blockquote><p>代码表述为：</p><blockquote><p>Repeat {<br>$w := w - \alpha \frac{\partial J(w,b)}{\partial w}​$ &nbsp; &nbsp;&nbsp;&nbsp;在代码中 $\frac{\partial J(w,b)}{\partial w}​$ 记作”dw”<br>$b := b - \alpha \frac{\partial J(w,b)}{\partial b}​$ &nbsp; &nbsp;&nbsp;&nbsp;在代码中 $\frac{\partial J(w,b)}{\partial b}​$ 记作”db”<br>}</p></blockquote><h3 id="Computation-Graph"><a href="#Computation-Graph" class="headerlink" title="Computation Graph"></a><strong>Computation Graph</strong></h3><p>神经网络的训练包含了两个过程：</p><ul><li>正向传播 (Forward Propagation)，从输入经过一层层神经网络，最后得到 $\hat{y}$ ，从而计算代价函数 $J$</li><li>反向传播 (Back Propagation)，根据损失函数 $L(\hat{y},y)$ 来反方向的计算每一层参数的偏导数，从而更新参数</li></ul><p>下面我们用计算图 (Computation Graph) 来理解这个过程</p><p><img src="/2019/01/21/Basic-of-Neural-Networks-1/computation_graph.png" alt=""></p><p>从左向右，可以计算出 $J​$ 的值，对应着神经网络中输入经过计算得到代价函数 $J(w,b)​$ 值的过程<br></p><p>从右向左，根据求导的链式法则，可以得到：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial a} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial a} = 3 \cdot 1 = 3</script><script type="math/tex; mode=display">\frac{\partial J}{\partial b} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial u} \cdot \frac{\partial u}{\partial b} = 3 \cdot 1 \cdot c = 6</script><script type="math/tex; mode=display">\frac{\partial J}{\partial c} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial u} \cdot \frac{\partial u}{\partial c} = 3 \cdot 1 \cdot b = 9</script><p>在反向传播中，一般我们只关心最终输出值 (在这个例子中是 $J$ ) ，需要计算 $J$ 对于某个变量 (记作var) 的导数，即 $\frac {dJ}{dvar}​$，在Python代码中简写为<code>dvar</code> <br></p><h3 id="Logistic-Regression-Gradient-Descent"><a href="#Logistic-Regression-Gradient-Descent" class="headerlink" title="Logistic Regression Gradient Descent"></a><strong>Logistic Regression Gradient Descent</strong></h3><p>现在，我们来实现对数几率回归梯度下降算法，只考虑单个样本的情况 :</p><blockquote><p>$z = w^{T}x + b$</p><p>$\hat{y} = a = \sigma({z})​$</p><p>$L(\hat{y},y) = -(y \log\hat{y} + (1-y) \log(1-\hat{y}))​$</p></blockquote><p>假设样本只有两个，分别为 $x1, x2$，则计算图如下 :</p><p><img src="/2019/01/21/Basic-of-Neural-Networks-1/logistic_regression_gradient_descent.png" alt=""></p><p>在对数几率回归中，我们需要做的是，改变参数 $w, b$ 的值，来最小化损失函数，即需要计算出 $dw, dz$ <br></p><p>向后传播计算损失函数 $L​$ 的偏导数步骤如下：</p><blockquote><ul><li>$da = \frac {\partial L(a,y)}{\partial a} = -\frac {y}{a} + \frac{1-y}{1-a}​$</li><li>$dz = \frac {\partial L}{\partial z} = \frac {\partial L}{\partial a} \cdot \frac {da}{dz}= (-\frac {y}{a} + \frac{1-y}{1-a}) \cdot a \cdot (1-a) = a - y​$</li><li>$dw_1 =  \frac {\partial L}{\partial w_1} = \frac {\partial L}{\partial z} \cdot \frac { \partial z}{\partial w_1} = x_1 \cdot dz ​$</li><li>$dw_2 =  \frac {\partial L}{\partial w_2} = \frac {\partial L}{\partial z} \cdot \frac { \partial z}{\partial w_2} = x_2 \cdot dz ​$</li><li>$db =  \frac {\partial L}{\partial b} = \frac {\partial L}{\partial z} \cdot \frac { \partial z}{\partial b} = dz​$</li></ul></blockquote><p>所以，在对数几率回归梯度下降算法中你需要做的是</p><blockquote><ul><li>$ dz = a - y$</li><li>$dw_1 = x_1 \cdot dz ​$</li><li>$dw_2 = x_2 \cdot dz ​$</li><li>$db = dz​$</li><li>更新$w_1​$,  $w_1 = w_1 - \alpha dw_1​$</li><li>更新$w_2$,  $w_2 = w_2 - \alpha dw_2$</li><li>更新$b​$,  $b = b - \alpha db​$</li></ul></blockquote><h3 id="Gradient-descent-on-m-examples"><a href="#Gradient-descent-on-m-examples" class="headerlink" title="Gradient descent on $m$ examples"></a><strong>Gradient descent on $m$ examples</strong></h3><p>之前只实现了单个样本的梯度下降算法，现在我们将梯度下降算法应用到整个训练集<br></p><blockquote><p>$J(w,b) = \frac{1}{m} \sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)}) $</p><p>$a^{(i)} = \hat{y}^{(i)} = \sigma(z^{(i)}) = \sigma(w^{T}x^{(i)}+b)​$</p><p>$\frac {\partial}{\partial w_1}J(w,b) = \frac {1}{m} \sum_{i=1}^{m} \frac {\partial}{\partial w_1}L(a^{(i)},y^{(i)}) = \frac {1}{m} \sum_{i=1}^{m}dw_1^{(i)}$ </p><ul><li>$dw_1^{(i)}​$按照之前单个样本的情况计算</li></ul></blockquote><p>伪代码如下 :</p><blockquote><p>$J=0; dw_1=0; dw_2=0; db=0;$<br>$for \quad i = 1 \quad to \quad m $<br>$\quad z^{(i)} = w^{T}x^{(i)}+b$<br>$\quad a^{(i)} = \sigma(z^{(i)})$<br>$\quad J += -(y^{(i)} \log a^{(i)} + (1-y^{(i)}) \log(1-a^{(i)}))$<br>$\quad dz^{(i)} = a^{(i)}-y^{(i)}$<br>$\quad dw_1 += x_1^{(i)}dz^{(i)}$<br>$\quad dw_2 += x_2^{(i)}dz^{(i)} \qquad$<br>$\quad…\quad\quad\quad\quad\quad\quad$ //这里应该是一个循环，这里 $n_x = 2$<br>$\quad db  += dz^{(i)}$<br>​$J /= m$<br>$dw_1 /= m$<br>$dw_2 /= m$<br>$db /= m$</p><p>$w_1 = w_1 - \alpha dw_1​$<br>$w_2 = w_2 - \alpha dw_2​$<br>$b = b - \alpha db​$</p></blockquote><p>但这种方法，有两个循环，一个是最外层的循环，循环 $m$ 个训练样本，另一个是 $dw_1, dw_2$ (feature) 的循环，在这个例子中 $n_x = 2$。随着训练集越来越大，应该尽量避免使用for循环，而使用向量化技术 (vectorization)</p>]]></content>
      
      
      <categories>
          
          <category> deeplearning.ai </category>
          
          <category> Deep learning &amp; NN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java常量池</title>
      <link href="/2019/01/17/Java-Constant-Pool/"/>
      <url>/2019/01/17/Java-Constant-Pool/</url>
      
        <content type="html"><![CDATA[<h3 id="常量池"><a href="#常量池" class="headerlink" title="常量池"></a>常量池</h3><ul><li>相同的值只存储一份，节省内存，共享访问，提高运行效率</li></ul><a id="more"></a><h3 id="基本类型的包装类"><a href="#基本类型的包装类" class="headerlink" title="基本类型的包装类"></a>基本类型的包装类</h3><ul><li><code>Boolean</code> <code>Byte</code> <code>Short</code> <code>Integer</code> <code>Long</code> <code>Character</code> <code>Float</code> <code>Double</code> 八种基本类型的包装类</li><li>常量值范围<ul><li><code>Boolean</code>：true, false</li><li><code>Byte</code> <code>Character</code> : \u0000 - \u007f</li><li><code>Short</code> <code>Integer</code> <code>Long</code> : -128 - 127</li><li><code>Float</code> <code>Double</code> : 无常量池</li></ul></li></ul><h3 id="与equals"><a href="#与equals" class="headerlink" title="==与equals()"></a>==与equals()</h3><ul><li>对于基本数据类型，<code>==</code>比较他们的数值</li><li>对于对象，<code>==</code>比较两个对象在内存中的存放地址，可以通过重写<code>equals()</code>来比较两个对象的内容是否相等</li></ul><h3 id="字符串常量"><a href="#字符串常量" class="headerlink" title="字符串常量"></a>字符串常量</h3><ul><li>Java为常量字符串建立了常量池缓存机制<figure class="highlight java hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">String s1 = <span class="hljs-string">"abc"</span>;</span><br><span class="line">String s2 = <span class="hljs-string">"ab"</span> + <span class="hljs-string">"c"</span>;</span><br><span class="line">String s3 = <span class="hljs-string">"a"</span> + <span class="hljs-string">"b"</span> + <span class="hljs-string">"c"</span>; <span class="hljs-comment">//都是常量，是确定的，编译器将优化</span></span><br><span class="line">System.out.println(s1==s2); <span class="hljs-comment">//true</span></span><br><span class="line">System.out.println(s1==s3); <span class="hljs-comment">//true</span></span><br><span class="line">System.out.println(s2==s3); <span class="hljs-comment">//true</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="基本类型的包装类和字符串的两种创建方式"><a href="#基本类型的包装类和字符串的两种创建方式" class="headerlink" title="基本类型的包装类和字符串的两种创建方式"></a>基本类型的包装类和字符串的两种创建方式</h3><ul><li>字面值赋值，放在栈内存<strong>（将被常量化）</strong><ul><li><code>Integer a = 1;</code> </li><li><code>String b = &quot;abc&quot;;</code></li></ul></li><li>new对象进行创建，放在堆内存<strong>（不会常量化）</strong><ul><li><code>Integer c = new Integer(1);</code></li><li><code>String d = new String(&quot;abc&quot;);</code></li></ul></li><li>栈内存读取速度快，容量小</li><li>堆内存读取速度慢，容量大，可以通俗的理解为Java认为new出来的对象所占内存较大（不确定，而字面值是确定的），所以需要放在堆内存</li></ul><h3 id="Integer常量池的例子"><a href="#Integer常量池的例子" class="headerlink" title="Integer常量池的例子"></a>Integer常量池的例子</h3><figure class="highlight java hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">int</span> i1 = <span class="hljs-number">10</span>;</span><br><span class="line">Integer i2 = <span class="hljs-number">10</span>; <span class="hljs-comment">//自动装箱，10本来只是int，是基本类型，而我们需要把它变成一个对象，相当于包装了一层</span></span><br><span class="line">System.out.println(i1==i2) <span class="hljs-comment">//true</span></span><br><span class="line"><span class="hljs-comment">//自动拆箱 基本类型和包装类进行比较，包装类自动拆箱</span></span><br><span class="line"></span><br><span class="line">Integer i3 = <span class="hljs-keyword">new</span> Integer(<span class="hljs-number">10</span>);</span><br><span class="line">System.out.println(i1==i3) <span class="hljs-comment">//true 同理，包装类自动拆箱</span></span><br><span class="line">System.out.println(i2==i3) <span class="hljs-comment">//false i2,i3都是对象，而i2是常量，在常量池，i3是new出来的对象，在堆内存中 </span></span><br><span class="line"></span><br><span class="line">Integer i4 = <span class="hljs-keyword">new</span> Integer(<span class="hljs-number">5</span>);</span><br><span class="line">Integer i5 = <span class="hljs-keyword">new</span> Integer(<span class="hljs-number">5</span>);</span><br><span class="line">System.out.println(i1 == (i4+i5)); <span class="hljs-comment">//true</span></span><br><span class="line">System.out.println(i1 == (i4+i5)); <span class="hljs-comment">//true</span></span><br><span class="line">System.out.println(i1 == (i4+i5)); <span class="hljs-comment">//true</span></span><br><span class="line"><span class="hljs-comment">//i4+i5的操作将会使i4,i5自动拆箱为基本类型并运算得到10，而根据之前所提到的，基本类型和包装类进行比较，包装类自动拆箱，所以都为true</span></span><br><span class="line"></span><br><span class="line">Integer i6 = i4 + i5;</span><br><span class="line">System.out.println(i1==i6); <span class="hljs-comment">//true，同理i4+i5的操作使i4,i5自动拆箱，得到10，相当于Integer i6 = 10;</span></span><br><span class="line">System.out.println(i3==i6); <span class="hljs-comment">//false</span></span><br></pre></td></tr></table></figure><h3 id="String常量池的例子"><a href="#String常量池的例子" class="headerlink" title="String常量池的例子"></a>String常量池的例子</h3><p>字符串常量池存在于方法区，方法区包含的都是在整个程序中唯一的元素，如static变量</p><ul><li><p>一个简单的例子</p><figure class="highlight java hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">String s1 = <span class="hljs-string">"abc"</span>;</span><br><span class="line">String s2 = <span class="hljs-string">"abc"</span>;</span><br><span class="line">String s3 = <span class="hljs-keyword">new</span> String(<span class="hljs-string">"abc"</span>);</span><br><span class="line">String s4 = <span class="hljs-keyword">new</span> String(<span class="hljs-string">"abc"</span>);</span><br><span class="line">System.out.println(s1==s2); <span class="hljs-comment">//true 都是常量池</span></span><br><span class="line">System.out.println(s1==s3); <span class="hljs-comment">//false 一个是栈内存，一个是堆内存</span></span><br><span class="line">System.out.println(s3==s4); <span class="hljs-comment">//false 都是堆内存，但是不同对象</span></span><br></pre></td></tr></table></figure></li><li><p>图解：(<code>&quot;</code>由<code>&#39;</code>代替)</p><pre class="mermaid">graph LR;subgraph 方法区  s['abc']endsubgraph 堆  A["s3 = new String('abc')"]  B["s4 = new String('abc')"]endsubgraph 栈  s1  s2  s3  s4ends1-->ss2-->sA-->sB-->ss3-->As4-->B</pre></li><li>更为复杂的例子<figure class="highlight java hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">String s5 = <span class="hljs-string">"abcdef"</span>;</span><br><span class="line">String s6 = s1 + <span class="hljs-string">"def"</span>; <span class="hljs-comment">//涉及到变量（不确定的），编译器不会优化</span></span><br><span class="line">String s7 = <span class="hljs-string">"abc"</span> + <span class="hljs-string">"def"</span>; <span class="hljs-comment">//都是常量，编译器会优化成abcdef</span></span><br><span class="line">String s8 = <span class="hljs-string">"abc"</span> + <span class="hljs-keyword">new</span> String(<span class="hljs-string">"def"</span>); <span class="hljs-comment">//涉及到new对象，编译器不优化</span></span><br><span class="line">System.out.println(s6==s7); <span class="hljs-comment">//false</span></span><br><span class="line">System.out.println(s6==s8); <span class="hljs-comment">//false</span></span><br><span class="line">System.out.println(s7==s8); <span class="hljs-comment">//false</span></span><br><span class="line">System.out.println(s5==s7); <span class="hljs-comment">//true</span></span><br><span class="line"></span><br><span class="line">String s9 = s3 + <span class="hljs-string">"def"</span>; <span class="hljs-comment">//由于s3是new的，涉及到new对象，编译器不优化</span></span><br><span class="line">System.out.println(s7==s9); <span class="hljs-comment">//false</span></span><br><span class="line"><span class="hljs-comment">//对于s5~s9，只有s5,s7是在常量池中，其余都在堆内存上，且地址互不相同</span></span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vim Tutorial</title>
      <link href="/2019/01/14/Vim-Tutorial/"/>
      <url>/2019/01/14/Vim-Tutorial/</url>
      
        <content type="html"><![CDATA[<p>This tutorial includes some basic vim commands and I hope that it will be helpful.</p><a id="more"></a><h3 id="Moving-the-cursor"><a href="#Moving-the-cursor" class="headerlink" title="Moving the cursor"></a>Moving the cursor</h3><ul><li><code>h</code> : left</li><li><code>j</code> : down</li><li><code>k</code> : up</li><li><code>l</code> : right</li></ul><p>It takes time to get used to it.</p><h3 id="Navigation"><a href="#Navigation" class="headerlink" title="Navigation"></a>Navigation</h3><ul><li><code>w</code>: move the cursor one word forward (to the first letter)</li><li><code>b</code> : one word backward (also to the first letter)</li><li><code>e</code> : one word forward (to the last letter)</li><li><code>fx</code> : forward to the letter <code>x</code></li><li><code>(</code> : to the start of the sentence</li><li><code>)</code> : start of the sentence</li><li><code>0</code> : start of line</li><li><code>$</code> : end of line </li><li><code>{</code> : start of paragraph</li><li><code>}</code> : end of paragraph </li><li><code>G</code> : end of file<ul><li><code>ctrl+G</code> : to see the cursor location and file status </li></ul></li><li><code>gg</code> : start of file</li><li><code>xG</code> : to the number <code>x</code> line of file<ul><li><strong>typing a number before a motion repeats it that many times!</strong></li></ul></li></ul><h3 id="Delete"><a href="#Delete" class="headerlink" title="Delete"></a>Delete</h3><ul><li><code>x</code>: delete the character at the cursor</li><li><code>dw</code>: delete all the characters between the cursor and the first letter of the next word<ul><li>e.g. Please de<code>l</code>ete the word. (Assume the cursor is at <code>l</code>)</li><li>After you press dw, the sentence becomes <code>Please dethe word delete</code></li></ul></li><li><code>de</code>: delete all the characters between the cursor and the next space<ul><li>e.g. Please de<code>l</code>ete the word. (Assume the cursor is at <code>l</code>)</li><li>After you press de, the sentence becomes <code>Please de the word delete</code></li></ul></li><li><code>d$</code> : delete to end of line</li><li><code>dd</code> : delete whole line</li><li><code>p</code> : After you delete something, press p to paste things you delete wherever you like.</li></ul><h3 id="Insert"><a href="#Insert" class="headerlink" title="Insert"></a>Insert</h3><ul><li><code>a</code> : insert after the cursor</li><li><code>A</code> : insert after the end of line</li><li><code>i</code> : insert before the cursor</li><li><code>I</code> : insert before the start of line </li><li><code>o</code> : insert in the next line</li><li><code>O</code> : insert in the previous line</li></ul><h3 id="Search"><a href="#Search" class="headerlink" title="Search"></a>Search</h3><ul><li><code>/yourSearchString + &lt;Enter&gt;</code> : search for <code>yourSearchString</code><ul><li><code>n</code> : to search for the same string again (press <code>&lt;Enter&gt;</code> to exit)</li><li><code>N</code> : to search for the same string again, but in opposite direction</li><li><code>ctrl+o</code> : to go back to where you came from</li><li><code>ctrl+i</code> : to go forward</li><li>set option<ul><li><code>:set ic</code> : ignore case</li><li><code>:set noic</code> : disable ignore case</li><li><code>:set hls</code> : highlight the matches</li><li><code>:set nohls</code> : disable highlight matches</li><li><code>:set is</code> : increase search</li><li><code>:set nois</code>: disable increase search</li></ul></li></ul></li><li><code>%</code> : move the cursor to the other matching parenthesis</li></ul><h3 id="Replace"><a href="#Replace" class="headerlink" title="Replace"></a>Replace</h3><ul><li><code>rx</code> : replace the character at cursor with <code>x</code></li><li><code>ce</code> : almost the same as <code>de</code>, but this time will place you in Insert Mode</li><li><code>s/old/new</code> : replace the first occurrence of ‘old’ with ‘new’</li><li><code>s/old/new/g</code> : replace all occurrence of ‘old’ with ‘new’ in one line</li><li><code>#,#/old/new/g</code> : #,# are the line numbers of the range of lines where the replace should be done</li><li><code>%s/old/new/g</code> : replace all occurrence of ‘old’ with ‘new’ in the whole file</li><li><code>%s/old/new/g</code> : replace all occurrence of ‘old’ with ‘new’ in the whole file, with a prompt whether to replace or not</li></ul><h3 id="Undo-amp-Redo"><a href="#Undo-amp-Redo" class="headerlink" title="Undo &amp; Redo"></a>Undo &amp; Redo</h3><ul><li><code>u</code> : undo the last command</li><li><code>U</code> : undo the command excuting on the while line</li><li><code>ctrl+R</code> : redo the command</li></ul><h3 id="Copy-amp-Paste"><a href="#Copy-amp-Paste" class="headerlink" title="Copy &amp; Paste"></a>Copy &amp; Paste</h3><ul><li><code>y</code> : to copy</li><li><code>p</code> : to paste<ul><li>e.g. Start Visual Mode with <code>v</code> and move the cursor to chose whatever you want, type <code>y</code> to copy the highlighted text and type <code>p</code> to paste the text.</li></ul></li></ul><h3 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h3><ul><li><code>.</code> : repeat the last command</li><li><code>&lt;start position&gt;&lt;command&gt;&lt;end position&gt;</code> : many commands follow this pattern<ul><li>e.g. <code>0y$</code> means copy the whole line<ul><li><code>0</code> move the cursor to the start of line</li><li><code>y</code> copy</li><li><code>$</code> move the cursor to the end of line</li></ul></li></ul></li><li><code>ctrl+n</code> : auto complete</li></ul>]]></content>
      
      
      <categories>
          
          <category> vim </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vim </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
