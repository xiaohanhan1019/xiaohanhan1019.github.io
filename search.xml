<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Neural Network and Deep Learning (Week Two)]]></title>
    <url>%2F2019%2F01%2F26%2FDeep-Learning-Week-Two%2F</url>
    <content type="text"><![CDATA[Vectorization (向量化)Vectorization (向量化) 的意义在于：消除代码中显式的调用for循环。在深度学习领域中，你常常需要训练大数据集，所以程序运行的效率非常重要，否则需要等待很长时间才能得到结果。 在对数几率回归中，你需要计算 $z = w^{T}x + b$ ，其中 $w, x$ 都是 $n_x$ 维向量 如果你是用非向量化的实现，即传统的矩阵相乘算法伪代码如下：$ z = 0 $$ for \quad i \quad in \quad range(n_x) : $$ \quad z+= w[i] * x[i] $$z+=b ​$ 若使用向量化的实现，Python代码如下：1z = np.dot(w,x) + b 即清晰又高效 可以测试一下这两个代码在效率方面的差距，大约差了300倍。可以试想一下，如果你的代码1分钟出结果，和5个小时才出结果，那可差太远了。 为了加快深度学习的运算速度，可以使用GPU (Graphic Processing Unit) 。事实上，GPU 和 CPU 都有并行指令 (Parallelization Instructions) ，同时也叫作 SIMD (Single Instruction Multiple Data)，即单指令流多数据流，是一种采用一个控制器来控制多个处理器，同时对一组数据 (又称“数据向量”) 中的每一个分别执行相同的操作从而实现空间上的并行性的技术。numpy 是 Python 数据分析及科学计算的基础库，它有许多内置 (Built-in) 函数，主要用于数组的计算，充分利用了并行化，使得运算速度大大提高。在深度学习的领域，一般来说，能不用显式的调用for循环就不用。 这样，我们就可以使用 Vectorization 来优化梯度下降算法，先去掉内层对 feature (特征 $w1, w2 …$) 的循环 ： $J=0; db=0; dw = np.zeros(n_x,1)$$for \quad i = 1 \quad to \quad m $$\quad z^{(i)} = w^{T}x^{(i)}+b$$\quad a^{(i)} = \sigma(z^{(i)})$$\quad J += -(y^{(i)} \log a^{(i)} + (1-y^{(i)}) \log(1-a^{(i)}))$$\quad dz^{(i)} = a^{(i)}-y^{(i)}$$\quad dw+=x^{(i)}dz^{(i)} \quad \quad $ //vectorization$\quad db += dz^{(i)}$$J /= m$$dw /= m$$db /= m​$ 然后，我们再去掉对 $m$ 个训练样本的外层循环，分别从正向传播和反向传播两方面来分析： 正向传播回顾一下对数几率回归的正向传播步骤，如果你有 $m$ 个训练样本 那么对第一个样本进行预测，你需要计算$ \quad z^{(1)} = w^{T}x^{(1)} + b$$ \quad a^{(1)} = \sigma(z^{(1)})​$ 然后继续对第二个样本进行预测$ \quad z^{(2)} = w^{T}x^{(2)} + b$$ \quad a^{(2)} = \sigma(z^{(2)})$ 然后继续对第三个，第四个，…，直到第 $m$ 个 回忆一下之前在二分分类部分所讲到的用更紧凑的符号 $X​$ 表示整个训练集，即大小为 $(n_x,m)​$ 的矩阵 : \mathbf{X} = \left( \begin{array}{c} \vdots & \vdots & \ldots & \vdots \\\\ x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\ \vdots & \vdots & \ldots & \vdots \end{array} \right)那么计算 $z^{(1)}, z^{(2)}, … , z^{(m)}$ 的步骤如下 :首先先构造一个 $(1,m)$ 的矩阵 $[z^{(1)}, z^{(2)}, … , z^{(m)}]$ ，则 Z = [z^{(1)}, z^{(2)}, ... , z^{(m)}] = w^{T}X + [b, b , ... , b] = [w^{T}x^{(1)} + b, w^{T}x^{(2)} + b] , ... , w^{T}x^{(m)} + b]在 Python 中一句代码即可完成上述过程1Z = np.dot(w.T,X) + b 你可能会有疑问，明明这里的 $b$ 只是一个实数 (或者说是一个 $b_{(1,1)}$ 的矩阵) ，为什么可以和矩阵 $Z_{(1,m)}$ 相加？事实上，当做 $Z+b$ 这个操作时，Python 会自动把矩阵 $b_{(1,1)}$ 自动扩展为 $b_{(1,m)}$ 这样的一个行向量，在 Python 中这称为 Broadcasting (广播) ，现在你只要看得懂就好，接下来会更详细地说明它。 同理我们可以得到 A = [a^{(1)}, a^{(2)}, ... , a^{(m)}] = \sigma (Z)同样也只需一句 Python 代码 1A = sigmoid(Z) 反向传播接着，我们来看如何用向量化优化反向传播，计算梯度 同样，你需要计算$dz^{(1)} = a^{(1)} - y^{(1)}​$$dz^{(2)} = a^{(2)} - y^{(2)}​$$…​$一直到第 $m​$ 个 即计算 $dZ = [dz^{(1)} , dz^{(2)} , … , dz^{(m)}]$ 之前我们已经得到 $A = [a^{(1)}, a^{(2)}, … , a^{(m)}] = \sigma (Z)​$我们再定义输出标签 $Y = [y^{(1)}, y^{(2)}, … , y^{(m)}]​$ 那么， dZ = A-Y = [a^{(1)}-y^{(1)} , a^{(2)}-y^{(2)} , ... , a^{(m)}-y^{(m)}]有了 $dZ$ 我们就可以计算 $dw, db​$根据之前的公式，有 db = \frac{1}{m} \sum_{i=1}^{m}dz^{(i)} dw = \frac{1}{m}X \cdot dZ^{T} = \frac{1}{m} \left( \begin{array}{c} \vdots & \vdots & \ldots & \vdots \\\\ x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\ \vdots & \vdots & \ldots & \vdots \end{array} \right) \left( \begin{array}{c} dz^{(1)} \\\\ \vdots \\\\ dz^{(m)} \end{array} \right) = \frac{1}{m}[x^{(1)}dz^{(1)} + ...+ x^{(m)}dz^{(m)}]对应的 Python 代码即为12db = np.sum(dZ)/mdw = np.dot(x,dZ.T)/m 最后，我们可以得到向量化后的梯度下降算法1234567import numpy as npZ = np.dot(w.T,X) + bA = sigmoid(Z)dw = np.dot(x,dZ.T)/mdb = np.sum(dZ)/mw = w - alpha * dwb = b - alpha * db 你可能会有疑问，为什么这里不需要再计算 Cost function (代价函数) $J$ 了，笔者认为 $J$ 只是对数几率回归模型所需要的损失函数，借助它我们才能计算出 $dw, db$ ，从而进行迭代。在后续的作业中，计算 $J​$ 可以帮助我们对模型进一步分析。这样，我们就完成了对数几率回归的梯度下降的一次迭代，但如果你需要多次执行迭代的操作，只能显式的调用for循环。 Broadcasting (广播) Broadcasting (广播) 机制主要是为了方便不同 shape 的 array (可以理解为不同形状的矩阵) 进行数学运算 当我们将向量和一个常量做加减乘除操作时，比如对数几率回归中的 $w^{T}x+b$ ，会对向量中的每一格元素都和常量做一次操作，或者你可以理解为把这个数复制 $m \times n$ 次，使其变为一个形状相同的 $(m,n)$ 矩阵，如 : \begin{bmatrix}1 \\ 2 \\ 3 \\ 4\end{bmatrix} + 100 => \begin{bmatrix}1 \\ 2 \\ 3 \\ 4\end{bmatrix} + \begin{bmatrix}100 \\ 100 \\ 100 \\ 100\end{bmatrix} = \begin{bmatrix}101 \\ 102 \\ 103 \\ 104\end{bmatrix} 一个 $(m,n)$ 矩阵和一个 $(1,n)$ 矩阵相加 (减乘除)，会将这个 $(1,n)$ 矩阵复制 $m$ 次，使其变为 $(m,n)$ 矩阵然后相加，如 : \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix} 100 & 200 & 300\end{bmatrix} => \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix} 100 & 200 & 300 \\ 100 & 200 & 300\end{bmatrix} = \begin{bmatrix}101 & 202 & 303 \\ 104 & 205 & 306\end{bmatrix} 同样地，一个 $(m,n)$ 矩阵和一个 $(m,1)$ 矩阵相加 (减乘除)，会将这个 $(m,1)$ 矩阵复制 $n$ 次，使其变为 $(m,n)$ 矩阵然后相加 \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix}100 \\ 200\end{bmatrix} => \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix} + \begin{bmatrix} 100 & 100 & 100 \\ 200 & 200 & 200\end{bmatrix} = \begin{bmatrix}101 & 102 & 103 \\ 204 & 205 & 206\end{bmatrix}通俗的讲，numpy 会通过复制的方法，使两个不同形状的矩阵变得一致，再执行相关操作。值得一提的是，为了保证运算按照我们的想法进行，使用 reshape() 函数是一个较好的习惯。 Homework附上所有代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276# LogisticRegression.pyimport numpy as npimport matplotlib.pyplot as pltimport h5pyimport scipyfrom PIL import Imagefrom scipy import ndimagefrom lr_utils import load_dataset# Sigmoid 函数def sigmoid(z): """ Compute the sigmoid of z Arguments: z -- A scalar or numpy array of any size. Return: s -- sigmoid(z) """ s = 1 / (1 + np.exp(-z)) return s# 初始化 w,bdef initialize_with_zeros(dim): """ This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0. Argument: dim -- size of the w vector we want (or number of parameters in this case) Returns: w -- initialized vector of shape (dim, 1) b -- initialized scalar (corresponds to the bias) """ w = np.zeros((dim, 1)) # (dim, 1) 是shape参数，代表初始化一个dim*1的矩阵 b = 0 assert (w.shape == (dim, 1)) assert (isinstance(b, float) or isinstance(b, int)) return w, b# propagate 正向与反向传播def propagate(w, b, X, Y): """ Implement the cost function and its gradient for the propagation explained above Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples) Return: cost -- negative log-likelihood cost for logistic regression dw -- gradient of the loss with respect to w, thus same shape as w db -- gradient of the loss with respect to b, thus same shape as b Tips: - Write your code step by step for the propagation. np.log(), np.dot() """ m = X.shape[1] # FORWARD PROPAGATION (FROM X TO COST) A = sigmoid(np.dot(w.T, X) + b) # compute activation cost = -1 / m * (np.dot(Y,np.log(A).T) + np.dot(1 - Y,np.log(1 - A).T)) # compute cost # BACKWARD PROPAGATION (TO FIND GRAD) dw = 1 / m * np.dot(X, (A - Y).T) db = 1 / m * np.sum(A - Y) assert (dw.shape == w.shape) assert (db.dtype == float) cost = np.squeeze(cost) # 删除shape为1的维度，比如cost=[[1]]，则经过np.squeeze处理后cost=[1] assert (cost.shape == ()) grads = &#123;"dw": dw, "db": db&#125; return grads, cost# 梯度下降def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False): """ This function optimizes w and b by running a gradient descent algorithm Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of shape (num_px * num_px * 3, number of examples) Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples) num_iterations -- number of iterations of the optimization loop learning_rate -- learning rate of the gradient descent update rule print_cost -- True to print the loss every 100 steps Returns: params -- dictionary containing the weights w and bias b grads -- dictionary containing the gradients of the weights and bias with respect to the cost function costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve. Tips: You basically need to write down two steps and iterate through them: 1) Calculate the cost and the gradient for the current parameters. Use propagate(). 2) Update the parameters using gradient descent rule for w and b. """ costs = [] for i in range(num_iterations): # Cost and gradient calculation grads, cost = propagate(w, b, X, Y) # Retrieve derivatives from grads dw = grads["dw"] db = grads["db"] # update rule w = w - dw * learning_rate b = b - db * learning_rate # Record the costs if i % 100 == 0: costs.append(cost) # Print the cost every 100 training examples if print_cost and i % 100 == 0: print("Cost after iteration %i: %f" % (i, cost)) params = &#123;"w": w, "b": b&#125; grads = &#123;"dw": dw, "db": db&#125; return params, grads, costs# 利用logistic regression判断Y的标签值def predict(w, b, X): ''' Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b) Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Returns: Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X ''' m = X.shape[1] Y_prediction = np.zeros((1, m)) w = w.reshape(X.shape[0], 1) # Compute vector "A" predicting the probabilities of a cat being present in the picture A = sigmoid(np.dot(w.T, X) + b) # A.shape = (1,m) for i in range(A.shape[1]): # Convert probabilities A[0,i] to actual predictions p[0,i] if A[0, i] &gt; 0.5: Y_prediction[0, i] = 1 else: Y_prediction[0, i] = 0 assert (Y_prediction.shape == (1, m)) return Y_prediction# 构建整个模型def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False): """ Builds the logistic regression model by calling the function you've implemented previously Arguments: X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train) Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train) X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test) Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test) num_iterations -- hyperparameter representing the number of iterations to optimize the parameters learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize() print_cost -- Set to true to print the cost every 100 iterations Returns: d -- dictionary containing information about the model. """ # initialize parameters with zeros w, b = initialize_with_zeros(X_train.shape[0]) # Gradient descent parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Retrieve parameters w and b from dictionary "parameters" w = parameters["w"] b = parameters["b"] # Predict test/train set examples Y_prediction_test = predict(w, b, X_test) Y_prediction_train = predict(w, b, X_train) # Print train/test Errors print("train accuracy: &#123;&#125; %".format( 100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100)) print("test accuracy: &#123;&#125; %".format( 100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100)) d = &#123; "costs": costs, "Y_prediction_test": Y_prediction_test, "Y_prediction_train": Y_prediction_train, "w": w, "b": b, "learning_rate": learning_rate, "num_iterations": num_iterations &#125; return ddef main(): train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset( ) m_train = train_set_x_orig.shape[0] m_test = test_set_x_orig.shape[0] num_px = train_set_x_orig.shape[2] train_set_x_flatten = train_set_x_orig.reshape(m_train, -1).T test_set_x_flatten = test_set_x_orig.reshape(m_test, -1).T train_set_x = train_set_x_flatten / 255. test_set_x = test_set_x_flatten / 255. # train model d = model( train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True) # 判断单张图片是否有猫 my_image = "a.jpg" # change this to the name of your image file # We preprocess the image to fit your algorithm. fname = "images/" + my_image image = np.array(ndimage.imread(fname, flatten=False)) my_image = scipy.misc.imresize(image, size=(num_px, num_px)).reshape((1, num_px * num_px * 3)).T my_predicted_image = predict(d["w"], d["b"], my_image) print("y = " + str(np.squeeze(my_predicted_image)) + ", your algorithm predicts a \"" + classes[ int(np.squeeze(my_predicted_image)),].decode("utf-8") + "\" picture.") plt.imshow(image) plt.show()main()]]></content>
      <categories>
        <category>Deep Learning</category>
        <category>Neural Network</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural Network and Deep Learning (Week One)]]></title>
    <url>%2F2019%2F01%2F21%2FDeep-Learning-Week-One%2F</url>
    <content type="text"><![CDATA[Binary Classification (二分分类)在二分分类问题中，目标是训练出一个分类器，以特征向量x (feature vector)为输入，以y (output label)为输出，y一般只有 ${0,1}​$ 两个离散值。以图像识别问题为例，判断图片中是否由猫存在，0代表noncat，1代表cat 通常，我们用 $(x,y)​$ 来表示一个单独的样本，其中x(feature vector)是$n_x​$维的向量 ( $n_x​$ 为样本特征个数，即决定输出的因素) ，y(output label)为输出，取值为 $y\in\{0,1\}​$则m个训练样本 (training example) 可表示为 \{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\}用$ m=m_{train} $表示训练样本的个数 最后，我们可以用更紧凑的符号 $X$ 表示整个训练集，$X$ 由训练集中的 $x^{(1)}$，$x^{(2)}$，…，$x^{(m)}$ 作为列向量组成，$X\in{\Bbb R}^{n_x*m}$，即 X.shape = $(n_x,m)$ \mathbf{X} = \left( \begin{array}{c} \vdots & \vdots & \ldots & \vdots \\\\ x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\\\ \vdots & \vdots & \ldots & \vdots \end{array} \right)同时，把y也放入列中，用 $Y$ 来表示，$Y\in{\Bbb R}^{1*m}$，即 Y.shape = $(1,m)​$ \mathbf{Y} = \left( \begin{array}{c} y^{(1)} & y^{(2)} & \ldots & y^{(m)} \ \end{array} \right)Logistic Regression (对数几率回归)参照了周志华的西瓜书，把 Logisitic Regression 翻译为对数纪律回归，简称为对率回归。对数几率回归是一种解决二分分类问题的机器学习方法，用于预测某种实物的可能性。 Given x, you want $\hat{y} = P(y=1 \mid x)​$. In other words, if x is a picture, as talked about above, you want $\hat{y}​$ to tell you the chance that there is a cat in the picture. 根据输入 $x$ 和参数 $w, b$，计算出 $\hat{y}$ ，下面介绍了两种方法 : Parameter : $w\in{\Bbb R}^{n_x}, b\in{\Bbb R}​$Output : $\hat{y}​$ One way : $\hat{y} = w^{T}x + b​$ (Linear regression) Not good for binary classification Because you want $\hat{y}​$ to be the chance that $y​$ equals to one. In this situation $\hat{y}​$ can be much bigger than 1 or negative. The other way : $\hat{y} = \sigma(w^{T}x + b)$ (Logistic Regression) $\sigma(z) = \frac{1}{1+e^{-z}} ​$ 通过$\sigma(z)$函数，可以将输出限定在$[0,1]$之间 Logistic Regression Cost Function (对数几率回归损失函数)给出$\{(x^{(1)},y^{(1)})…,(x^{(m)},y^{(m)})\}​$，希望通过训练集，找到参数 $w, b​$ 使得 $\hat{y}^{(i)} \approx y^{(i)}​$ 。所以，我们需要定义一个loss function，通过这个loss function来衡量你的预测输出值 $\hat{y}​$ 与 $y​$ 的实际值由多接近 对于m个训练样本，我们通常用上标 $(i)​$ 来指明数据与第 $i​$ 个样本有关。 通常，我们这样定义Loss function (损失函数) : L(\hat{y},y) = \frac{1}{2}(\hat{y} - y)^2但在对数几率回归中一般不使用，因为它是non-convex (非凸的) ，将来使用梯度下降算法 (Gradient Descent)时无法找到全局最优值 在对数几率回归中，我们使用的损失函数为 : L(\hat{y},y) = -(y \log\hat{y} + (1-y) \log(1-\hat{y})) If y = 1 : $L(\hat{y},y) = -\log(\hat{y})​$, you want $\hat{y}​$ to be large if y = 0 : $L(\hat{y},y) = -\log(1-\hat{y})​$, you want $\hat{y}​$ to be small 所以，这个损失函数和 $L(\hat{y},y) = \frac{1}{2}(\hat{y} - y)^2$ 类似，都希望 $L$ 越小越好 上述的Loss function衡量了单个训练样本的表现，对于m个样本，我们定义Cost function (代价函数) ，它衡量了全体训练样本的表现 J(w,b) = \frac{1}{m} \sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)}) = -\frac{1}{m} \sum_{i=1}^{m}y^{(i)} \log\hat{y}^{(i)} + (1-y^{(i)}) \log(1-\hat{y}^{(i)})Loss function只适用于单个训练样本，Cost function是基于参数的总代价。所以，在训练对数几率回归模型时，我们要找到合适的参数 $w, b$ 使得Cost function尽可能的小 Gradient Descent (梯度下降)我们将使用梯度下降 (Gradient Descent) 算法来找出合适的参数 $w,b$，使得Cost function 即 $J(w,b)$ 最小 最上方的小红点为初始点，对于对数几率回归，一般使用0来初始化，随机初始化也有效，但通常不这么做 梯度下降过程： 从初始点开始，朝最陡的下坡方向走一步 重复上述过程，不断修正 $w, b​$ 使得 $J(w,b)​$ 接近全局最优值 (global opitmal) 代码表述为： Repeat {$w := w - \alpha \frac{\partial J(w,b)}{\partial w}​$ &nbsp; &nbsp;&nbsp;&nbsp;在代码中 $\frac{\partial J(w,b)}{\partial w}​$ 记作”dw”$b := b - \alpha \frac{\partial J(w,b)}{\partial b}​$ &nbsp; &nbsp;&nbsp;&nbsp;在代码中 $\frac{\partial J(w,b)}{\partial b}​$ 记作”db”} Computation Graph (计算图)神经网络的训练包含了两个过程： 正向传播 (Forward Propagation)，从输入经过一层层神经网络，最后得到 $\hat{y}$ ，从而计算代价函数 $J$ 反向传播 (Back Propagation)，根据损失函数 $L(\hat{y},y)$ 来反方向的计算每一层参数的偏导数，从而更新参数 下面我们用计算图 (Computation Graph) 来理解这个过程 从左向右，可以计算出 $J​$ 的值，对应着神经网络中输入经过计算得到代价函数 $J(w,b)​$ 值的过程 从右向左，根据求导的链式法则，可以得到： \frac{\partial J}{\partial a} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial a} = 3 \cdot 1 = 3 \frac{\partial J}{\partial b} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial u} \cdot \frac{\partial u}{\partial b} = 3 \cdot 1 \cdot c = 6 \frac{\partial J}{\partial c} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial u} \cdot \frac{\partial u}{\partial c} = 3 \cdot 1 \cdot b = 9在反向传播中，一般我们只关心最终输出值 (在这个例子中是 $J$ ) ，需要计算 $J$ 对于某个变量 (记作var) 的导数，即 $\frac {dJ}{dvar}​$，在Python代码中简写为dvar Logistic Regression Gradient Descent (对数几率回归梯度下降)现在，我们来实现对数几率回归梯度下降算法，只考虑单个样本的情况 : $z = w^{T} + b$ $\hat{y} = a = \sigma({z})​$ $L(\hat{y},y) = -(y \log\hat{y} + (1-y) \log(1-\hat{y}))​$ 假设样本只有两个，分别为 $x1, x2​$，则计算图如下 : 在对数几率回归中，我们需要做的是，改变参数 $w, b$ 的值，来最小化损失函数，即需要计算出 $dw, dz$ 向后传播计算损失函数 $L​$ 的偏导数步骤如下： $da = \frac {dL(a,y)}{da} = -\frac {y}{a} + \frac{1-y}{1-a}​$ $dz = \frac {dL}{dz} = \frac {dL}{da} \cdot \frac {da}{dz}= (-\frac {y}{a} + \frac{1-y}{1-a}) \cdot a \cdot (1-a) = a - y​$ $dw_1 = \frac {dL}{dw_1} = x_1 \cdot dz $ $dw_2 = \frac {dL}{dw_2} = x_2 \cdot dz $ $db = dz$ 所以，在对数几率回归梯度下降算法中你需要做的是 $ dz = a - y$ $dw_1 = x_1 \cdot dz ​$ $dw_2 = x_2 \cdot dz ​$ $db = dz​$ 更新$w_1​$, $w_1 = w_1 - \alpha dw_1​$ 更新$w_2$, $w_2 = w_2 - \alpha dw_2$ 更新$b​$, $b = b - \alpha db​$ Gradient descent on $m$ examples (将梯度下降算法应用到整个训练集)之前只实现了单个样本的梯度下降算法，现在我们将梯度下降算法应用到整个训练集 $J(w,b) = \frac{1}{m} \sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)}) $ $a^{(i)} = \hat{y}^{(i)} = \sigma(z^{(i)}) = \sigma(w^{T}x^{(i)}+b)$ $\frac {\partial}{\partial w_1}J(w,b) = \frac {1}{m} \sum_{i=1}^{m} \frac {\partial}{\partial w_1}L(a^{(i)},y^{(i)}) = \frac {1}{m} \sum_{i=1}^{m}dw_1^{(i)}$ $dw_1^{(i)}​$按照之前单个样本的情况计算 伪代码如下 : $J=0; dw_1=0; dw_2=0; db=0;$$for \quad i = 1 \quad to \quad m $$\quad z^{(i)} = w^{T}x^{(i)}+b$$\quad a^{(i)} = \sigma(z^{(i)})$$\quad J += -(y^{(i)} \log a^{(i)} + (1-y^{(i)}) \log(1-a^{(i)}))$$\quad dz^{(i)} = a^{(i)}-y^{(i)}$$\quad dw_1 += x_1^{(i)}dz^{(i)}$$\quad dw_2 += x_2^{(i)}dz^{(i)} \qquad$$\quad…\quad\quad\quad\quad\quad\quad$ //这里应该是一个循环，这里 $n_x = 2$$\quad db += dz^{(i)}$​$J /= m$$dw_1 /= m$$dw_2 /= m$$db /= m$ $w_1 = w_1 - \alpha dw_1​$$w_2 = w_2 - \alpha dw_2​$$b = b - \alpha db​$ 但这种方法，有两个循环，一个是最外层的循环，循环 $m$ 个训练样本，另一个是 $dw_1, dw_2$ (feature) 的循环，在这个例子中 $n_x = 2$。随着训练集越来越大，应该尽量避免使用for循环，而使用向量化技术 (vectorization)]]></content>
      <categories>
        <category>Deep Learning</category>
        <category>Neural Network</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java常量池]]></title>
    <url>%2F2019%2F01%2F17%2FJava-Constant-Pool%2F</url>
    <content type="text"><![CDATA[常量池 相同的值只存储一份，节省内存，共享访问，提高运行效率 基本类型的包装类 Boolean Byte Short Integer Long Character Float Double 八种基本类型的包装类 常量值范围 Boolean：true, false Byte Character : \u0000 - \u007f Short Integer Long : -128 - 127 Float Double : 无常量池 ==与equals() 对于基本数据类型，==比较他们的数值 对于对象，==比较两个对象在内存中的存放地址，可以通过重写equals()来比较两个对象的内容是否相等 字符串常量 Java为常量字符串建立了常量池缓存机制123456String s1 = "abc";String s2 = "ab" + "c";String s3 = "a" + "b" + "c"; //都是常量，是确定的，编译器将优化System.out.println(s1==s2); //trueSystem.out.println(s1==s3); //trueSystem.out.println(s2==s3); //true 基本类型的包装类和字符串的两种创建方式 字面值赋值，放在栈内存（将被常量化） Integer a = 1; String b = &quot;abc&quot;; new对象进行创建，放在堆内存（不会常量化） Integer c = new Integer(1); String d = new String(&quot;abc&quot;); 栈内存读取速度快，容量小 堆内存读取速度慢，容量大，可以通俗的理解为Java认为new出来的对象所占内存较大（不确定，而字面值是确定的），所以需要放在堆内存 Integer常量池的例子12345678910111213141516171819int i1 = 10;Integer i2 = 10; //自动装箱，10本来只是int，是基本类型，而我们需要把它变成一个对象，相当于包装了一层System.out.println(i1==i2) //true//自动拆箱 基本类型和包装类进行比较，包装类自动拆箱 Integer i3 = new Integer(10);System.out.println(i1==i3) //true 同理，包装类自动拆箱System.out.println(i2==i3) //false i2,i3都是对象，而i2是常量，在常量池，i3是new出来的对象，在堆内存中 Integer i4 = new Integer(5);Integer i5 = new Integer(5);System.out.println(i1 == (i4+i5)); //trueSystem.out.println(i1 == (i4+i5)); //trueSystem.out.println(i1 == (i4+i5)); //true//i4+i5的操作将会使i4,i5自动拆箱为基本类型并运算得到10，而根据之前所提到的，基本类型和包装类进行比较，包装类自动拆箱，所以都为trueInteger i6 = i4 + i5;System.out.println(i1==i6); //true，同理i4+i5的操作使i4,i5自动拆箱，得到10，相当于Integer i6 = 10;System.out.println(i3==i6); //false String常量池的例子字符串常量池存在于方法区，方法区包含的都是在整个程序中唯一的元素，如static变量 一个简单的例子 1234567String s1 = "abc";String s2 = "abc";String s3 = new String("abc");String s4 = new String("abc");System.out.println(s1==s2); //true 都是常量池System.out.println(s1==s3); //false 一个是栈内存，一个是堆内存System.out.println(s3==s4); //false 都是堆内存，但是不同对象 图解：(&quot;由&#39;代替) graph LR; subgraph 方法区 s['abc'] end subgraph 堆 A["s3 = new String('abc')"] B["s4 = new String('abc')"] end subgraph 栈 s1 s2 s3 s4 end s1-->s s2-->s A-->s B-->s s3-->A s4-->B 更为复杂的例子123456789101112String s5 = "abcdef";String s6 = s1 + "def"; //涉及到变量（不确定的），编译器不会优化String s7 = "abc" + "def"; //都是常量，编译器会优化成abcdefString s8 = "abc" + new String("def"); //涉及到new对象，编译器不优化System.out.println(s6==s7); //falseSystem.out.println(s6==s8); //falseSystem.out.println(s7==s8); //falseSystem.out.println(s5==s7); //trueString s9 = s3 + "def"; //由于s3是new的，涉及到new对象，编译器不优化System.out.println(s7==s9); //false//对于s5~s9，只有s5,s7是在常量池中，其余都在堆内存上，且地址互不相同]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim Tutorial]]></title>
    <url>%2F2019%2F01%2F14%2FVim-Tutorial%2F</url>
    <content type="text"><![CDATA[This tutorial includes some basic vim commands and I hope that it will be helpful. Moving the cursor h : left j : down k : up l : right It takes time to get used to it. Navigation w: move the cursor one word forward (to the first letter) b : one word backward (also to the first letter) e : one word forward (to the last letter) fx : forward to the letter x ( : to the start of the sentence ) : start of the sentence 0 : start of line $ : end of line { : start of paragraph } : end of paragraph G : end of file ctrl+G : to see the cursor location and file status gg : start of file xG : to the number x line of file typing a number before a motion repeats it that many times! Delete x: delete the character at the cursor dw: delete all the characters between the cursor and the first letter of the next word e.g. Please delete the word. (Assume the cursor is at l) After you press dw, the sentence becomes Please dethe word delete de: delete all the characters between the cursor and the next space e.g. Please delete the word. (Assume the cursor is at l) After you press de, the sentence becomes Please de the word delete d$ : delete to end of line dd : delete whole line p : After you delete something, press p to paste things you delete wherever you like. Insert a : insert after the cursor A : insert after the end of line i : insert before the cursor I : insert before the start of line o : insert in the next line O : insert in the previous line Search /yourSearchString + &lt;Enter&gt; : search for yourSearchString n : to search for the same string again (press &lt;Enter&gt; to exit) N : to search for the same string again, but in opposite direction ctrl+o : to go back to where you came from ctrl+i : to go forward set option :set ic : ignore case :set noic : disable ignore case :set hls : highlight the matches :set nohls : disable highlight matches :set is : increase search :set nois: disable increase search % : move the cursor to the other matching parenthesis Replace rx : replace the character at cursor with x ce : almost the same as de, but this time will place you in Insert Mode s/old/new : replace the first occurrence of ‘old’ with ‘new’ s/old/new/g : replace all occurrence of ‘old’ with ‘new’ in one line #,#/old/new/g : #,# are the line numbers of the range of lines where the replace should be done %s/old/new/g : replace all occurrence of ‘old’ with ‘new’ in the whole file %s/old/new/g : replace all occurrence of ‘old’ with ‘new’ in the whole file, with a prompt whether to replace or not Undo &amp; Redo u : undo the last command U : undo the command excuting on the while line ctrl+R : redo the command Copy &amp; Paste y : to copy p : to paste e.g. Start Visual Mode with v and move the cursor to chose whatever you want, type y to copy the highlighted text and type p to paste the text. Others . : repeat the last command &lt;start position&gt;&lt;command&gt;&lt;end position&gt; : many commands follow this pattern e.g. 0y$ means copy the whole line 0 move the cursor to the start of line y copy $ move the cursor to the end of line ctrl+n : auto complete]]></content>
      <categories>
        <category>vim</category>
      </categories>
      <tags>
        <tag>vim</tag>
      </tags>
  </entry>
</search>
